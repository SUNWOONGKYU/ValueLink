#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¯¸ìˆ˜ì§‘ 121ê°œ ê¸°ì—… ë‹¤ì¤‘ ì¿¼ë¦¬ ì „ëµìœ¼ë¡œ ì¬ê²€ìƒ‰
"""

import os
import sys
import csv
import requests
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")


def extract_first_investor(investor_str):
    """íˆ¬ìì ë¬¸ìì—´ì—ì„œ ì²« ë‹¨ì–´ ì¶”ì¶œ"""
    if not investor_str or investor_str == '-':
        return ""

    # ê³µë°±, í•˜ì´í”ˆ, ì‰¼í‘œë¡œ ë¶„ë¦¬í•˜ê³  ì²« ë‹¨ì–´ ì¶”ì¶œ
    words = investor_str.replace('-', ' ').replace(',', ' ').split()
    if words:
        return words[0]
    return ""


def search_naver_multi_query(company_name, investor_str):
    """ë„¤ì´ë²„ API - ì—¬ëŸ¬ ì¿¼ë¦¬ ì¡°í•©ìœ¼ë¡œ ê²€ìƒ‰"""

    url = "https://openapi.naver.com/v1/search/news.json"

    headers = {
        'X-Naver-Client-Id': NAVER_CLIENT_ID,
        'X-Naver-Client-Secret': NAVER_CLIENT_SECRET
    }

    # íˆ¬ìì ì²« ë‹¨ì–´ ì¶”ì¶œ
    first_investor = extract_first_investor(investor_str)

    # ì—¬ëŸ¬ ê²€ìƒ‰ ì¿¼ë¦¬ ì¡°í•© (ìš°ì„ ìˆœìœ„ ìˆœ)
    queries = []

    if first_investor:
        queries.append(f"{company_name} {first_investor} íˆ¬ì")
        queries.append(f"{company_name} {first_investor} ìœ ì¹˜")
        queries.append(f"{company_name} {first_investor}")

    queries.extend([
        f"{company_name} íˆ¬ììœ ì¹˜",
        f"{company_name} ì‹œë¦¬ì¦ˆ",
        f"{company_name} í€ë”©",
        f"{company_name} ë¼ìš´ë“œ",
        f"{company_name} VC",
    ])

    # ê° ì¿¼ë¦¬ ì‹œë„
    for query in queries:
        params = {
            'query': query,
            'display': 50,  # ìµœëŒ€í•œ ë§ì´
            'sort': 'date'
        }

        try:
            response = requests.get(url, headers=headers, params=params, timeout=10)

            if response.status_code == 200:
                items = response.json().get('items', [])

                for item in items:
                    title = item.get('title', '').replace('<b>', '').replace('</b>', '')
                    link = item.get('originallink') or item.get('link')
                    pub_date = item.get('pubDate', '')

                    # ê¸°ì—…ëª… ì •í™•íˆ í¬í•¨
                    if company_name not in title:
                        continue

                    # íˆ¬ì í‚¤ì›Œë“œ
                    investment_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ', 'VC', 'ë²¤ì²˜ìºí”¼í„¸']
                    if not any(kw in title for kw in investment_keywords):
                        continue

                    # ë‚ ì§œ íŒŒì‹±
                    try:
                        dt = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %z')
                        published_date = dt.strftime('%Y-%m-%d')
                    except:
                        published_date = datetime.now().strftime('%Y-%m-%d')

                    # ì‚¬ì´íŠ¸ëª…
                    site_mapping = {
                        'venturesquare.net': ('ë²¤ì²˜ìŠ¤í€˜ì–´', 9),
                        'wowtale.net': ('WOWTALE', 1),
                        'platum.kr': ('í”Œë˜í…€', 10),
                        'outstanding.kr': ('ì•„ì›ƒìŠ¤íƒ ë”©', 13),
                        'startuptoday.kr': ('ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´', 11),
                        'thebell.co.kr': ('ë”ë²¨', 16),
                    }

                    site_name = "ë„¤ì´ë²„ ë‰´ìŠ¤"
                    site_number = 99

                    for domain, (name, num) in site_mapping.items():
                        if domain in link:
                            site_name = name
                            site_number = num
                            break

                    return {
                        'site_number': site_number,
                        'site_name': site_name,
                        'site_url': "",
                        'article_title': title,
                        'article_url': link,
                        'published_date': published_date
                    }, query

            time.sleep(0.15)

        except Exception as e:
            continue

    return None, None


def main():
    print("=" * 80)
    print("ë¯¸ìˆ˜ì§‘ 121ê°œ ê¸°ì—… ë‹¤ì¤‘ ì¿¼ë¦¬ ì „ëµ ì¬ê²€ìƒ‰")
    print("=" * 80)

    # CSV ì½ê¸°
    csv_file = 'sensible_companies_2026_01_COMPLETE.csv'

    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        companies = {row['ê¸°ì—…ëª…']: row for row in reader}

    # ë¯¸ìˆ˜ì§‘ ê¸°ì—… ëª©ë¡
    with open('final_not_found_126.txt', 'r', encoding='utf-8') as f:
        not_found = [line.strip() for line in f if line.strip()]

    print(f"\në¯¸ìˆ˜ì§‘ ê¸°ì—…: {len(not_found)}ê°œ\n")

    found = 0
    duplicate = 0
    still_not_found = []

    for idx, company in enumerate(not_found, 1):
        if company not in companies:
            print(f"[{idx:3d}/{len(not_found)}] {company:20s}... âŒ CSVì— ì—†ìŒ")
            still_not_found.append(company)
            continue

        row = companies[company]
        investor = row['íˆ¬ìì']
        first_inv = extract_first_investor(investor)

        print(f"[{idx:3d}/{len(not_found)}] {company:20s} + {first_inv:15s}...", end=' ')

        # ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰
        article, matched_query = search_naver_multi_query(company, investor)

        if article and article['article_url']:
            # ì¤‘ë³µ í™•ì¸
            existing = supabase.table("investment_news_articles")\
                .select("id")\
                .eq("article_url", article['article_url'])\
                .execute()

            if not existing.data:
                try:
                    supabase.table("investment_news_articles").insert(article).execute()
                    print(f"âœ… [{article['site_name']}] Query: {matched_query}")
                    found += 1
                except:
                    print(f"âŒ DB ì˜¤ë¥˜")
            else:
                print(f"âš ï¸ ì¤‘ë³µ")
                duplicate += 1
        else:
            print("âŒ ëª» ì°¾ìŒ")
            still_not_found.append(company)

        time.sleep(0.2)

    print(f"\n{'='*80}")
    print("ì¬ê²€ìƒ‰ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"âœ… ìƒˆë¡œ ë°œê²¬: {found}ê°œ")
    print(f"âš ï¸ ì¤‘ë³µ: {duplicate}ê°œ")
    print(f"âŒ ì—¬ì „íˆ ëª» ì°¾ìŒ: {len(still_not_found)}ê°œ")
    print(f"{'='*80}")

    # ìµœì¢… í†µê³„
    result = supabase.table("investment_news_articles").select("article_title").execute()

    all_companies = list(companies.keys())
    final_collected = set()
    for article in result.data:
        for comp in all_companies:
            if comp in article['article_title']:
                final_collected.add(comp)

    print(f"\nğŸ“Š 126ê°œ ê¸°ì—… ìµœì¢…:")
    print(f"  âœ… ë‰´ìŠ¤ ìˆìŒ: {len(final_collected)}ê°œ ({len(final_collected)*100//126}%)")
    print(f"  âŒ ë‰´ìŠ¤ ì—†ìŒ: {126-len(final_collected)}ê°œ")

    if still_not_found:
        with open('still_not_found_after_multi_query.txt', 'w', encoding='utf-8') as f:
            for company in still_not_found:
                f.write(f"{company}\n")
        print(f"\nâš ï¸ ì—¬ì „íˆ ëª» ì°¾ì€ ê¸°ì—…: still_not_found_after_multi_query.txt ({len(still_not_found)}ê°œ)")

    print(f"\ninvestment_news_articles í…Œì´ë¸” ì´ ë ˆì½”ë“œ:")
    count_result = supabase.table("investment_news_articles").select("id", count="exact").execute()
    print(f"  {count_result.count}ê°œ")


if __name__ == '__main__':
    main()
