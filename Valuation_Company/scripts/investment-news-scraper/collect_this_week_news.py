#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì´ë²ˆ ì£¼ íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (2026-01-27 ~ 2026-01-28)

í”„ë¡œì„¸ìŠ¤:
1. 5ëŒ€ ì–¸ë¡ ê¸°ê´€ í¬ë¡¤ë§ (ë²¤ì²˜ìŠ¤í€˜ì–´, ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´, ì•„ì›ƒìŠ¤íƒ ë”©, ë”ë¸Œì´ì”¨, ìŠ¤íƒ€íŠ¸ì—…ì—”)
2. Geminië¡œ íˆ¬ì ë‰´ìŠ¤ ê²€ì¦
3. ë„¤ì´ë²„ APIë¡œ ë³´ì™„
"""

import os
import sys
from dotenv import load_dotenv
from supabase import create_client
import codecs
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
from google import genai
from google.genai import types

if sys.platform == 'win32':
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')

load_dotenv()

supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_SERVICE_KEY"))
gemini_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# 5ëŒ€ ì–¸ë¡ ê¸°ê´€
MEDIA_SITES = [
    {
        'id': 9,
        'name': 'ë²¤ì²˜ìŠ¤í€˜ì–´',
        'url': 'https://www.venturesquare.net/category/news/',
        'article_selector': 'article.post',
        'title_selector': 'h5 a',
        'link_selector': 'h5 a',
        'date_selector': 'time.entry-date',
    },
    {
        'id': 11,
        'name': 'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´',
        'url': 'https://www.startuptoday.kr/news/articleList.html',
        'article_selector': 'div.article-list-content',
        'title_selector': 'h4.titles',
        'link_selector': 'a',
        'date_selector': 'span.byline',
    },
    {
        'id': 13,
        'name': 'ì•„ì›ƒìŠ¤íƒ ë”©',
        'url': 'https://outstanding.kr/',
        'article_selector': 'article',
        'title_selector': 'h2 a',
        'link_selector': 'h2 a',
        'date_selector': 'time',
    },
    {
        'id': 8,
        'name': 'ë”ë¸Œì´ì”¨',
        'url': 'https://thevc.kr/news',
        'article_selector': 'div.news-item',
        'title_selector': 'h3 a',
        'link_selector': 'h3 a',
        'date_selector': 'span.date',
    },
    {
        'id': 12,
        'name': 'ìŠ¤íƒ€íŠ¸ì—…ì—”',
        'url': 'https://www.startupn.kr/news/articleList.html',
        'article_selector': 'div.article-list-content',
        'title_selector': 'h4.titles',
        'link_selector': 'a',
        'date_selector': 'span.byline',
    },
]

def extract_article_date(html_content, url):
    """ê¸°ì‚¬ HTMLì—ì„œ ë°œí–‰ì¼ ì¶”ì¶œ"""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')

        # ë‚ ì§œ ì¶”ì¶œ íŒ¨í„´
        date_patterns = [
            ('meta', {'property': 'article:published_time'}),
            ('meta', {'name': 'pubdate'}),
            ('time', {'datetime': True}),
        ]

        for tag_name, attrs in date_patterns:
            tag = soup.find(tag_name, attrs)
            if tag:
                date_str = tag.get('content') or tag.get('datetime')
                if date_str:
                    if 'T' in date_str:
                        return date_str.split('T')[0]
                    else:
                        return date_str[:10]

        return None
    except:
        return None

def verify_with_gemini(title, url):
    """Geminië¡œ íˆ¬ì ë‰´ìŠ¤ì¸ì§€ ê²€ì¦ ë° ì •ë³´ ì¶”ì¶œ"""
    prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ì´ ìŠ¤íƒ€íŠ¸ì—… íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”:

ì œëª©: {title}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
{{
    "is_investment": true,
    "company": "íšŒì‚¬ëª…",
    "stage": "ì‹œë“œ/í”„ë¦¬A/ì‹œë¦¬ì¦ˆA ë“±",
    "investors": "íˆ¬ììëª…",
    "amount": ìˆ«ìë§Œ
}}

íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ê°€ ì•„ë‹ˆë©´ {{"is_investment": false}}ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    try:
        import json

        response = gemini_client.models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0,
                max_output_tokens=256,
                response_mime_type='application/json'
            )
        )

        if response and hasattr(response, 'text'):
            text = response.text.strip()
            result = json.loads(text)
            return result

        return None
    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜ (íˆ¬ì í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ true)
        invest_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'ë¼ìš´ë“œ']
        if any(kw in title for kw in invest_keywords):
            return {'is_investment': True, 'company': None, 'stage': None, 'investors': None, 'amount': None}
        return {'is_investment': False}

def crawl_media_site(site):
    """ì–¸ë¡ ì‚¬ ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ë‰´ìŠ¤ í¬ë¡¤ë§ (2026-01-27 ~ 2026-01-28)"""
    print(f"\nğŸ“° {site['name']} í¬ë¡¤ë§ ì¤‘...")

    articles = []

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(site['url'], headers=headers, timeout=10)

        if response.status_code != 200:
            print(f"  âŒ HTTP {response.status_code}")
            return articles

        soup = BeautifulSoup(response.content, 'html.parser')
        article_elements = soup.select(site['article_selector'])[:20]  # ìµœì‹  20ê°œ

        print(f"  ê²€ìƒ‰ëœ ê¸°ì‚¬: {len(article_elements)}ê°œ")

        for article in article_elements:
            try:
                # ì œëª© & ë§í¬
                title_elem = article.select_one(site['title_selector'])
                link_elem = article.select_one(site['link_selector'])

                if not title_elem or not link_elem:
                    continue

                title = title_elem.get_text(strip=True)
                url = link_elem.get('href', '')

                # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                if url.startswith('/'):
                    base_url = site['url'].split('?')[0].rsplit('/', 1)[0]
                    url = base_url + url

                if not url.startswith('http'):
                    continue

                # íˆ¬ì í‚¤ì›Œë“œ í•„í„°
                invest_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ', 'Pre-A', 'ì‹œë“œ', 'ë¼ìš´ë“œ']
                if any(kw in title for kw in invest_keywords):
                    # ì¼ë‹¨ ìˆ˜ì§‘ (ë‚ ì§œëŠ” ë‚˜ì¤‘ì— ì¶”ì¶œ)
                    articles.append({
                        'site_id': site['id'],
                        'site_name': site['name'],
                        'title': title,
                        'url': url,
                        'published_date': None  # Gemini ê²€ì¦ ì‹œ ì¶”ì¶œ
                    })
                    print(f"    âœ… {title[:60]}...")

                time.sleep(0.3)

            except Exception as e:
                continue

    except Exception as e:
        print(f"  âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)[:50]}")

    return articles

def save_to_database(articles):
    """investment_news_articles í…Œì´ë¸”ì— ì €ì¥"""
    print(f"\nğŸ’¾ DB ì €ì¥ ì¤‘... ({len(articles)}ê°œ)")

    saved = 0

    for article in articles:
        try:
            # ì¤‘ë³µ ì²´í¬
            existing = supabase.table('investment_news_articles')\
                .select('id')\
                .eq('article_url', article['url'])\
                .execute()

            if existing.data:
                print(f"  âš ï¸  ì¤‘ë³µ: {article['title'][:40]}...")
                continue

            # ë‚ ì§œ ì¶”ì¶œ
            print(f"  ğŸ” {article['title'][:40]}... ", end='')

            article_response = requests.get(article['url'], headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            published_date = extract_article_date(article_response.content, article['url']) if article_response.status_code == 200 else None

            # 2026-01-27 ~ 2026-01-28ë§Œ ì²˜ë¦¬
            if published_date not in ['2026-01-27', '2026-01-28']:
                print(f"âŒ ë‚ ì§œ ë²”ìœ„ ë°– ({published_date})")
                continue

            # Gemini ê²€ì¦
            gemini_result = verify_with_gemini(article['title'], article['url'])

            if gemini_result and gemini_result.get('is_investment'):
                print(f"âœ… íˆ¬ì ë‰´ìŠ¤ ({published_date})")

                # ì €ì¥
                supabase.table('investment_news_articles').insert({
                    'site_number': article['site_id'],
                    'site_name': article['site_name'],
                    'site_url': article['url'].split('/')[2],  # domain
                    'article_title': article['title'],
                    'article_url': article['url'],
                    'published_date': published_date,
                    'has_amount': gemini_result.get('amount') is not None,
                    'has_investors': gemini_result.get('investors') is not None,
                    'has_stage': gemini_result.get('stage') is not None,
                }).execute()

                saved += 1
            else:
                print(f"âŒ íˆ¬ì ë‰´ìŠ¤ ì•„ë‹˜ ({published_date})")

            time.sleep(1)  # API ì œí•œ

        except Exception as e:
            print(f"  âŒ ì €ì¥ ì˜¤ë¥˜: {str(e)[:50]}")

    print(f"\nâœ… {saved}ê°œ ì €ì¥ ì™„ë£Œ")

# ë©”ì¸
print("=" * 80)
print("ì´ë²ˆ ì£¼ íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ ìˆ˜ì§‘ (2026-01-27 ~ 2026-01-28)")
print("=" * 80)

all_articles = []

# Step 1: 5ëŒ€ ì–¸ë¡ ê¸°ê´€ í¬ë¡¤ë§
for site in MEDIA_SITES:
    articles = crawl_media_site(site)
    all_articles.extend(articles)
    time.sleep(2)

print(f"\nğŸ“Š ì´ ìˆ˜ì§‘: {len(all_articles)}ê°œ")

# Step 2: Gemini ê²€ì¦ ë° DB ì €ì¥
if all_articles:
    save_to_database(all_articles)
else:
    print("\nâš ï¸  ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

print("\nì™„ë£Œ!")
