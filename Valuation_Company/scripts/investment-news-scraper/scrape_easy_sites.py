#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‰¬ìš´ ì‚¬ì´íŠ¸ íˆ¬ì ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘
ì •ì  HTML ì‚¬ì´íŠ¸ ìœ„ì£¼ë¡œ ë¹ ë¥´ê²Œ ìˆ˜ì§‘
"""

import os
import time
import json
import logging
from datetime import datetime, date
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraping_easy_sites_log.txt', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Supabase ì—°ê²°
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')

# ê¸°ê°„
START_DATE = date(2026, 1, 1)
END_DATE = date.today()

# í‚¤ì›Œë“œ
KEYWORDS = ['íˆ¬ì', 'íˆ¬ììœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'ë²¤ì²˜ìºí”¼íƒˆ', 'VC', 'ì—”ì ¤íˆ¬ì', 'M&A', 'ì¸ìˆ˜', 'ì–µì›', 'ì¡°ì›']

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
}


def contains_keyword(text: str) -> bool:
    """íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€"""
    if not text:
        return False
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in KEYWORDS)


def parse_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    date_formats = [
        '%Y-%m-%d',
        '%Y.%m.%d',
        '%Y/%m/%d',
        '%Yë…„ %mì›” %dì¼',
    ]

    for fmt in date_formats:
        try:
            parsed_date = datetime.strptime(date_str.strip(), fmt).date()
            return parsed_date.isoformat()
        except ValueError:
            continue

    return None


def scrape_daum_news():
    """ë‹¤ìŒë‰´ìŠ¤ ë²¤ì²˜/ìŠ¤íƒ€íŠ¸ì—… ì„¹ì…˜ ìŠ¤í¬ë˜í•‘"""
    articles = []
    site_number = 25
    site_name = "ë‹¤ìŒë‰´ìŠ¤ ë²¤ì²˜/ìŠ¤íƒ€íŠ¸ì—…"
    site_url = "https://news.daum.net"

    logger.info(f"ğŸ” [{site_name}] ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    try:
        url = "https://news.daum.net/breakingnews/digital/venture"
        logger.info(f"  ì ‘ì†: {url}")

        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # ë‹¤ìŒë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ ì°¾ê¸°
        # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ
        news_items = soup.select('ul.list_news2 li')
        logger.info(f"  ë°œê²¬: {len(news_items)}ê°œ ê¸°ì‚¬")

        for item in news_items:
            try:
                # ì œëª© ë° URL
                link = item.select_one('a.link_txt')
                if not link:
                    continue

                title = link.get_text(strip=True)
                article_url = link.get('href', '')

                # í‚¤ì›Œë“œ í•„í„°ë§
                if not contains_keyword(title):
                    continue

                # ë‚ ì§œ
                date_elem = item.select_one('span.info_time')
                if not date_elem:
                    continue

                date_text = date_elem.get_text(strip=True)
                # "2026.01.25" í˜•ì‹
                published_date = parse_date(date_text)

                if not published_date:
                    continue

                # ë‚ ì§œ ë²”ìœ„ í™•ì¸
                try:
                    date_obj = datetime.fromisoformat(published_date).date()
                    if not (START_DATE <= date_obj <= END_DATE):
                        continue
                except:
                    continue

                articles.append({
                    'site_number': site_number,
                    'site_name': site_name,
                    'site_url': site_url,
                    'article_title': title,
                    'article_url': article_url,
                    'published_date': published_date,
                    'content_snippet': None,
                })

                logger.info(f"  âœ… {title[:40]}... ({published_date})")

            except Exception as e:
                logger.error(f"  âŒ ê¸°ì‚¬ íŒŒì‹± ì—ëŸ¬: {e}")
                continue

    except Exception as e:
        logger.error(f"âŒ [{site_name}] ì—ëŸ¬: {e}")

    logger.info(f"âœ… [{site_name}] {len(articles)}ê±´ ìˆ˜ì§‘")
    return articles


def scrape_mk_news():
    """ë§¤ì¼ê²½ì œ IT ì„¹ì…˜ ìŠ¤í¬ë˜í•‘"""
    articles = []
    site_number = 24
    site_name = "ë§¤ì¼ê²½ì œ MKí…Œí¬ë¦¬ë·°"
    site_url = "https://www.mk.co.kr"

    logger.info(f"ğŸ” [{site_name}] ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    try:
        url = "https://www.mk.co.kr/news/it/"
        logger.info(f"  ì ‘ì†: {url}")

        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # ë§¤ì¼ê²½ì œ ê¸°ì‚¬ ëª©ë¡ ì°¾ê¸°
        # ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„
        news_items = soup.select('div.news_node')
        if not news_items:
            news_items = soup.select('li.news_node')

        logger.info(f"  ë°œê²¬: {len(news_items)}ê°œ ê¸°ì‚¬")

        for item in news_items:
            try:
                # ì œëª© ë° URL
                link = item.select_one('a')
                if not link:
                    continue

                title_elem = link.select_one('h3, h4, .news_ttl')
                if not title_elem:
                    continue

                title = title_elem.get_text(strip=True)
                article_url = link.get('href', '')

                # ì ˆëŒ€ URL ë³€í™˜
                if article_url and not article_url.startswith('http'):
                    article_url = site_url + article_url

                # í‚¤ì›Œë“œ í•„í„°ë§
                if not contains_keyword(title):
                    continue

                # ë‚ ì§œ
                date_elem = item.select_one('span.date, .news_date')
                if date_elem:
                    date_text = date_elem.get_text(strip=True)
                    published_date = parse_date(date_text)

                    if published_date:
                        date_obj = datetime.fromisoformat(published_date).date()
                        if not (START_DATE <= date_obj <= END_DATE):
                            continue

                        articles.append({
                            'site_number': site_number,
                            'site_name': site_name,
                            'site_url': site_url,
                            'article_title': title,
                            'article_url': article_url,
                            'published_date': published_date,
                            'content_snippet': None,
                        })

                        logger.info(f"  âœ… {title[:40]}... ({published_date})")

            except Exception as e:
                logger.error(f"  âŒ ê¸°ì‚¬ íŒŒì‹± ì—ëŸ¬: {e}")
                continue

    except Exception as e:
        logger.error(f"âŒ [{site_name}] ì—ëŸ¬: {e}")

    logger.info(f"âœ… [{site_name}] {len(articles)}ê±´ ìˆ˜ì§‘")
    return articles


def save_to_json(articles: List[Dict], filename: str = "inbox/investment_news_data.json"):
    """JSON íŒŒì¼ë¡œ ì €ì¥"""
    filepath = os.path.join(os.path.dirname(__file__), filename)

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    existing_data = []
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except:
            pass

    # ì¤‘ë³µ ì œê±°
    existing_urls = {article.get('article_url') for article in existing_data}
    new_articles = [a for a in articles if a.get('article_url') not in existing_urls]

    # ë³‘í•©
    all_articles = existing_data + new_articles

    # ì €ì¥
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=2)

    logger.info(f"ğŸ’¾ JSON ì €ì¥: {len(new_articles)}ê±´ ì¶”ê°€ (ì´ {len(all_articles)}ê±´)")
    return len(new_articles)


def save_to_supabase(articles: List[Dict]) -> int:
    """Supabaseì— ì €ì¥"""
    if not articles:
        return 0

    saved_count = 0
    api_url = f"{SUPABASE_URL}/rest/v1/investment_news_articles"
    headers = {
        'apikey': SUPABASE_KEY,
        'Authorization': f'Bearer {SUPABASE_KEY}',
        'Content-Type': 'application/json',
        'Prefer': 'return=representation'
    }

    for article in articles:
        try:
            response = requests.post(api_url, json=article, headers=headers, timeout=30)

            if response.status_code == 201:
                saved_count += 1
                logger.info(f"  ğŸ’¾ Supabase: {article['article_title'][:40]}...")
            elif response.status_code == 409:
                logger.info(f"  âš ï¸  ì¤‘ë³µ: {article['article_title'][:40]}...")

        except Exception as e:
            logger.error(f"  âŒ ì €ì¥ ì—ëŸ¬: {e}")

    return saved_count


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    logger.info("=" * 60)
    logger.info("ğŸ“° ì‰¬ìš´ ì‚¬ì´íŠ¸ íˆ¬ì ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘")
    logger.info(f"ğŸ“… ê¸°ê°„: {START_DATE} ~ {END_DATE}")
    logger.info("=" * 60)

    all_articles = []

    # ë‹¤ìŒë‰´ìŠ¤
    articles_daum = scrape_daum_news()
    all_articles.extend(articles_daum)
    time.sleep(2)

    # ë§¤ì¼ê²½ì œ
    articles_mk = scrape_mk_news()
    all_articles.extend(articles_mk)

    # JSON ì €ì¥
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ’¾ JSON íŒŒì¼ ì €ì¥ ì¤‘...")
    saved_json = save_to_json(all_articles)

    # Supabase ì €ì¥
    if all_articles:
        logger.info("ğŸ’¾ Supabase ì €ì¥ ì¤‘...")
        saved_db = save_to_supabase(all_articles)
        logger.info(f"âœ… Supabase: {saved_db}ê±´ ì €ì¥")

    logger.info("\n" + "=" * 60)
    logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(all_articles)}ê±´")
    logger.info(f"ğŸ’¾ JSON ì¶”ê°€: {saved_json}ê±´")
    logger.info("=" * 60)


if __name__ == '__main__':
    main()
