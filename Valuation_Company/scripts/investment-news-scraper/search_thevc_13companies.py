#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
theVC.krì—ì„œ ëª» ì°¾ì€ 13ê°œ ê¸°ì—… ê²€ìƒ‰
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# ëª» ì°¾ì€ 13ê°œ ê¸°ì—…
missing_companies = [
    "ì• í”Œì—ì´ì•„ì´",
    "ë””ì—”í‹°í…Œí¬ì†”ë£¨ì…˜",
    "ì—˜ë¦¬ì‚¬ì  ",
    "ì˜¤í”ˆì›¨ë”©",
    "ìŠ¤íŠœë””ì˜¤ì—í”¼ì†Œë“œ",
    "ë¶€ìŠ¤í‹°ìŠ¤",
    "íˆ¬ëª¨ë¡œìš°",
    "ë¹„ë°”íŠ¸ë¡œë¡œë³´í‹±ìŠ¤",
    "ë±ì‚¬ìŠ¤íŠœë””ì˜¤",
    "í•œì–‘ë¡œë³´í‹±ìŠ¤",
    "ì†Œì…œë¦­ìŠ¤ì½”ë¦¬ì•„",
    "ìŠ¤ì¹´ì´ì¸í…”ë¦¬ì „ìŠ¤",
    "í•˜ì´íŒŒì´ë¸Œë©"
]


def search_thevc(company_name):
    """theVC.krì—ì„œ ê¸°ì—… ê²€ìƒ‰"""

    # theVC ê²€ìƒ‰ URL
    search_url = f"https://thevc.kr/search?q={company_name}"

    try:
        response = requests.get(search_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê¸°ì—… ë§í¬ ì°¾ê¸°
        # theVC êµ¬ì¡°: ê¸°ì—…ëª… ë§í¬ê°€ ìˆìœ¼ë©´ í•´ë‹¹ í˜ì´ì§€ë¡œ
        links = soup.find_all('a', href=True)

        for link in links:
            href = link.get('href', '')
            text = link.get_text().strip()

            # ê¸°ì—… í˜ì´ì§€ ë§í¬ ì°¾ê¸°
            if '/organizations/' in href and company_name in text:
                org_url = f"https://thevc.kr{href}" if href.startswith('/') else href
                return org_url

        return None

    except Exception as e:
        print(f"    ì˜¤ë¥˜: {e}")
        return None


def get_investment_info_from_thevc(org_url):
    """theVC ê¸°ì—… í˜ì´ì§€ì—ì„œ íˆ¬ì ì •ë³´ ì¶”ì¶œ"""

    try:
        response = requests.get(org_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # theVC í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ í›„ íˆ¬ì ì •ë³´ ì¶”ì¶œ
        # (ì‹¤ì œ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)

        # ìµœê·¼ íˆ¬ì ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
        news_links = soup.find_all('a', href=True)

        for link in news_links:
            href = link.get('href', '')
            text = link.get_text().strip()

            # íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œ
            if any(kw in text for kw in ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series']):
                # ì™¸ë¶€ ë‰´ìŠ¤ ë§í¬ë©´ ë°˜í™˜
                if href.startswith('http') and 'thevc.kr' not in href:
                    return {
                        'url': href,
                        'title': text
                    }

        return None

    except Exception as e:
        return None


def main():
    print("=" * 80)
    print("theVC.krì—ì„œ ëª» ì°¾ì€ 13ê°œ ê¸°ì—… ê²€ìƒ‰")
    print("=" * 80)

    found = 0
    not_found = []

    for idx, company in enumerate(missing_companies, 1):
        print(f"\n[{idx:2d}/13] {company:25s}")

        # theVC ê²€ìƒ‰
        org_url = search_thevc(company)

        if org_url:
            print(f"  âœ… theVC í˜ì´ì§€ ë°œê²¬: {org_url}")

            # íˆ¬ì ì •ë³´ ì¶”ì¶œ
            investment_info = get_investment_info_from_thevc(org_url)

            if investment_info:
                print(f"  ğŸ“° ë‰´ìŠ¤ ë°œê²¬: {investment_info['title'][:50]}...")
                print(f"  ğŸ”— {investment_info['url']}")

                # DBì— ì €ì¥ ì‹œë„
                article = {
                    'site_number': 99,
                    'site_name': 'theVC ê²€ìƒ‰',
                    'site_url': "",
                    'article_title': investment_info['title'],
                    'article_url': investment_info['url'],
                    'published_date': datetime.now().strftime('%Y-%m-%d')
                }

                # ì¤‘ë³µ í™•ì¸
                existing = supabase.table("investment_news_articles")\
                    .select("id")\
                    .eq("article_url", article['article_url'])\
                    .execute()

                if not existing.data:
                    try:
                        supabase.table("investment_news_articles").insert(article).execute()
                        print(f"  ğŸ’¾ DB ì €ì¥ ì™„ë£Œ")
                        found += 1
                    except:
                        print(f"  âŒ DB ì €ì¥ ì‹¤íŒ¨")
                else:
                    print(f"  âš ï¸  ì´ë¯¸ DBì— ìˆìŒ")
            else:
                print(f"  âŒ íˆ¬ì ë‰´ìŠ¤ ëª» ì°¾ìŒ")
                not_found.append(company)
        else:
            print(f"  âŒ theVCì—ì„œ ëª» ì°¾ìŒ")
            not_found.append(company)

        time.sleep(1)

    print(f"\n{'='*80}")
    print("theVC ê²€ìƒ‰ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"âœ… ë°œê²¬: {found}ê°œ")
    print(f"âŒ ëª» ì°¾ìŒ: {len(not_found)}ê°œ")

    if not_found:
        print(f"\nâŒ theVCì—ì„œë„ ëª» ì°¾ì€ ê¸°ì—…:")
        for company in not_found:
            print(f"  - {company}")


if __name__ == '__main__':
    main()
