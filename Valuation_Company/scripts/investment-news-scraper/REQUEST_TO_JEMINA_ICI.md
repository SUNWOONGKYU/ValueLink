# ì¬ë¯¸ë‚˜ ICI ì‘ì—… ìš”ì²­ì„œ

**ì‘ì„±ì¼**: 2026-01-25
**ìš”ì²­ì**: ì‚¬ìš©ì
**ì‘ì„±ì**: Claude Code

---

## ğŸ“‹ í˜„ì¬ ìƒí™©

### âœ… ì™„ë£Œëœ ì‘ì—… (Claude Code)
1. Supabase í…Œì´ë¸” 2ê°œ ìƒì„± ì™„ë£Œ
   - `investment_news_articles` (ê¸°ì‚¬ ì €ì¥)
   - `investment_news_ranking` (ë­í‚¹ ì§‘ê³„)

2. Python ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± ì™„ë£Œ
   - íŒŒì¼: `scrape_investment_news.py`
   - í™˜ê²½ ì„¤ì •: `.env` (Supabase ì—°ê²° ì™„ë£Œ)
   - íŒ¨í‚¤ì§€ ì„¤ì¹˜: ì™„ë£Œ

3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê²°ê³¼
   - âŒ **ìˆ˜ì§‘ ê±´ìˆ˜: 0ê±´**
   - âŒ ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨

---

## âŒ ë¬¸ì œì 

**ì›ì¸**: `scrape_generic_site()` í•¨ìˆ˜ê°€ **ë²”ìš© í…œí”Œë¦¿**ì´ë¼ì„œ ê° ì‚¬ì´íŠ¸ì˜ ì‹¤ì œ HTML êµ¬ì¡°ì™€ ë§ì§€ ì•ŠìŒ

**ë¡œê·¸ í™•ì¸**:
```
[8] ë”ë¸Œì´ì”¨: 0ê±´ ìˆ˜ì§‘
[9] ë²¤ì²˜ìŠ¤í€˜ì–´: 0ê±´ ìˆ˜ì§‘
[10] í”Œë˜í…€: 0ê±´ ìˆ˜ì§‘
... (ëª¨ë“  ì‚¬ì´íŠ¸ 0ê±´)
```

---

## ğŸ¯ ì¬ë¯¸ë‚˜ ICIê°€ í•´ì•¼ í•  ì‘ì—…

### ì‘ì—… ë‚´ìš©
**ê° ì‚¬ì´íŠ¸ì˜ HTML êµ¬ì¡°ì— ë§ê²Œ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•**

### ëŒ€ìƒ ì‚¬ì´íŠ¸ (19ê°œ)
| ë²ˆí˜¸ | ì‚¬ì´íŠ¸ëª… | URL |
|------|---------|-----|
| 8 | ë”ë¸Œì´ì”¨ | https://thevc.kr |
| 9 | ë²¤ì²˜ìŠ¤í€˜ì–´ | https://www.venturesquare.net |
| 10 | í”Œë˜í…€ | https://platum.kr |
| 11 | ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´ | https://startuptoday.kr |
| 12 | ìŠ¤íƒ€íŠ¸ì—…ì—” | https://startupn.kr |
| 13 | ì•„ì›ƒìŠ¤íƒ ë”© | https://outstanding.kr |
| 14 | ëª¨ë¹„ì¸ì‚¬ì´ë“œ | https://mobiinside.co.kr |
| 15 | ì§€ë””ë„·ì½”ë¦¬ì•„ | https://www.zdnet.co.kr |
| 16 | ë”ë²¨ | https://www.thebell.co.kr |
| 17 | ë„¥ìŠ¤íŠ¸ìœ ë‹ˆì½˜ | https://nextunicorn.kr |
| 18 | í…Œí¬ì›”ë“œë‰´ìŠ¤ | https://www.epnc.co.kr |
| 19 | AIíƒ€ì„ìŠ¤ | https://www.aitimes.com |
| 20 | ë²¤ì²˜ê²½ì˜ì‹ ë¬¸ | https://www.vmnews.co.kr |
| 21 | ë‰´ìŠ¤í†± | https://www.newstopkorea.com |
| 22 | ë¸”ë¡œí„° | https://www.bloter.net |
| 23 | ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸ | https://www.economist.co.kr |
| 24 | ë§¤ì¼ê²½ì œ MKí…Œí¬ë¦¬ë·° | https://www.mk.co.kr/news/it |
| 25 | ë‹¤ìŒë‰´ìŠ¤ ë²¤ì²˜/ìŠ¤íƒ€íŠ¸ì—… | https://news.daum.net/section/2/venture |
| 26 | ëŒ€í•œë¯¼êµ­ ì •ì±…ë¸Œë¦¬í•‘ | https://www.korea.kr |

---

## ğŸ“ íŒŒì¼ ìœ„ì¹˜

**í”„ë¡œì íŠ¸ ê²½ë¡œ**:
```
C:\ValueLink\Valuation_Company\scripts\investment-news-scraper\
```

**ìˆ˜ì •í•  íŒŒì¼**:
```
scrape_investment_news.py
```

**ìˆ˜ì •í•  í•¨ìˆ˜**:
- `scrape_generic_site()` (137ë²ˆì§¸ ì¤„ë¶€í„°)

---

## ğŸ”§ ìˆ˜ì • ë°©ë²•

### í˜„ì¬ ì½”ë“œ (ë²”ìš© í…œí”Œë¦¿ - ì‘ë™ ì•ˆ í•¨)

```python
def scrape_generic_site(site: Dict) -> List[Dict]:
    """ë²”ìš© ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ (ì‘ë™ ì•ˆ í•¨)"""

    # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ (ì‹¤ì œë¡œëŠ” ì‚¬ì´íŠ¸ë³„ë¡œ ë‹¤ë¦„)
    article_elements = soup.select('article') or soup.select('.article-item')

    for element in article_elements:
        # ì œëª© ì¶”ì¶œ
        title_elem = element.select_one('h2, h3, .title')
        # URL ì¶”ì¶œ
        link_elem = element.select_one('a[href]')
        # ë‚ ì§œ ì¶”ì¶œ
        date_elem = element.select_one('.date, .publish-date')
```

### ìˆ˜ì • ë°©ë²• (ì‚¬ì´íŠ¸ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§•)

#### ë‹¨ê³„ 1: ì‚¬ì´íŠ¸ HTML êµ¬ì¡° ë¶„ì„

ê° ì‚¬ì´íŠ¸ë¥¼ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê³ :
1. F12 (ê°œë°œì ë„êµ¬)
2. Elements íƒ­
3. ê¸°ì‚¬ ì œëª© ìš”ì†Œ ì°¾ê¸° â†’ í´ë˜ìŠ¤ëª…, íƒœê·¸ í™•ì¸

**ì˜ˆì‹œ (ë²¤ì²˜ìŠ¤í€˜ì–´)**:
- ê¸°ì‚¬ ëª©ë¡: `<div class="post-list">`
- ê°œë³„ ê¸°ì‚¬: `<article class="post-item">`
- ì œëª©: `<h3 class="post-title">`
- ë§í¬: `<a class="post-link" href="...">`
- ë‚ ì§œ: `<time class="post-date">2026-01-15</time>`

#### ë‹¨ê³„ 2: ì‚¬ì´íŠ¸ë³„ í•¨ìˆ˜ ì‘ì„±

**ê¶Œì¥ ë°©ì‹**: ì‚¬ì´íŠ¸ë³„ë¡œ ë³„ë„ í•¨ìˆ˜ ì‘ì„±

```python
def scrape_venturesquare(site: Dict) -> List[Dict]:
    """ë²¤ì²˜ìŠ¤í€˜ì–´ ì „ìš© ìŠ¤í¬ë˜í•‘"""
    articles = []
    site_number = site['number']
    site_name = site['name']

    # íˆ¬ì ì„¹ì…˜ URL
    url = 'https://www.venturesquare.net/category/investment'

    response = requests.get(url, headers=HEADERS, timeout=10)
    soup = BeautifulSoup(response.text, 'lxml')

    # ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
    for article_elem in soup.select('article.post-item'):  # ì‹¤ì œ ì…€ë ‰í„°
        try:
            # ì œëª©
            title = article_elem.select_one('h3.post-title').get_text(strip=True)

            # í‚¤ì›Œë“œ í•„í„°ë§
            if not contains_keyword(title):
                continue

            # URL
            article_url = article_elem.select_one('a.post-link')['href']
            if not article_url.startswith('http'):
                article_url = 'https://www.venturesquare.net' + article_url

            # ë‚ ì§œ
            date_text = article_elem.select_one('time.post-date').get_text(strip=True)
            published_date = parse_date(date_text)

            # ë‚ ì§œ í•„í„°ë§
            if not published_date or not is_valid_date(published_date):
                continue

            # ë‚´ìš© ë°œì·Œ (ì„ íƒ ì‚¬í•­)
            snippet_elem = article_elem.select_one('.post-excerpt')
            snippet = snippet_elem.get_text(strip=True)[:200] if snippet_elem else None

            articles.append({
                'site_number': site_number,
                'site_name': site_name,
                'site_url': 'www.venturesquare.net',
                'article_title': title,
                'article_url': article_url,
                'published_date': published_date.isoformat(),
                'content_snippet': snippet,
            })

        except Exception as e:
            logger.error(f"ê¸°ì‚¬ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            continue

    logger.info(f"âœ… [{site_number}] {site_name}: {len(articles)}ê±´ ìˆ˜ì§‘")
    return articles


def scrape_platum(site: Dict) -> List[Dict]:
    """í”Œë˜í…€ ì „ìš© ìŠ¤í¬ë˜í•‘"""
    # í”Œë˜í…€ HTML êµ¬ì¡°ì— ë§ê²Œ ì‘ì„±
    ...


# ... 19ê°œ ì‚¬ì´íŠ¸ ê°ê° í•¨ìˆ˜ ì‘ì„±
```

#### ë‹¨ê³„ 3: main() í•¨ìˆ˜ ìˆ˜ì •

```python
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # ì‚¬ì´íŠ¸ë³„ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ ë§¤í•‘
    scraping_functions = {
        8: scrape_venturesquare,  # ë”ë¸Œì´ì”¨ (ì˜ëª» ë§¤í•‘ë¨, ìˆ˜ì • í•„ìš”)
        9: scrape_venturesquare,
        10: scrape_platum,
        11: scrape_startuptoday,
        # ... ë‚˜ë¨¸ì§€ ì‚¬ì´íŠ¸
    }

    for idx, site in enumerate(SITES, 1):
        site_number = site['number']

        # í•´ë‹¹ ì‚¬ì´íŠ¸ ì „ìš© í•¨ìˆ˜ í˜¸ì¶œ
        if site_number in scraping_functions:
            articles = scraping_functions[site_number](site)
        else:
            # ê¸°ë³¸ í•¨ìˆ˜ (ì‘ë™ ì•ˆ í•¨)
            articles = scrape_generic_site(site)

        total_articles.extend(articles)
        time.sleep(REQUEST_DELAY)
```

---

## ğŸ” HTML êµ¬ì¡° ë¶„ì„ íŒ

### ë°©ë²• 1: ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬
1. ì‚¬ì´íŠ¸ ì—´ê¸°
2. F12 (ê°œë°œì ë„êµ¬)
3. ê¸°ì‚¬ ì œëª© ìš°í´ë¦­ â†’ "ê²€ì‚¬"
4. HTML êµ¬ì¡° í™•ì¸

### ë°©ë²• 2: Pythonìœ¼ë¡œ í™•ì¸
```python
import requests
from bs4 import BeautifulSoup

url = 'https://www.venturesquare.net'
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')

# HTML ì¶œë ¥ (íŒŒì¼ë¡œ ì €ì¥)
with open('venturesquare.html', 'w', encoding='utf-8') as f:
    f.write(soup.prettify())

# ë¸Œë¼ìš°ì €ì—ì„œ ì—´ì–´ì„œ êµ¬ì¡° í™•ì¸
```

### ë°©ë²• 3: ì¼ë°˜ì ì¸ íŒ¨í„´

**ê¸°ì‚¬ ëª©ë¡ ì»¨í…Œì´ë„ˆ**:
- `<div class="news-list">`, `<section class="articles">`
- `<ul class="post-list">`, `<div id="content">`

**ê°œë³„ ê¸°ì‚¬**:
- `<article>`, `<li class="post">`, `<div class="item">`

**ì œëª©**:
- `<h1>`, `<h2>`, `<h3>`
- `.title`, `.headline`, `.post-title`

**ë§í¬**:
- `<a href="...">`

**ë‚ ì§œ**:
- `<time>`, `<span class="date">`, `.publish-date`

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### 1. ê°œë³„ ì‚¬ì´íŠ¸ í…ŒìŠ¤íŠ¸

í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•  ë•Œë§ˆë‹¤:
```python
# scrape_investment_news.py ë§¨ ì•„ë˜ ì¶”ê°€
if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸: ë²¤ì²˜ìŠ¤í€˜ì–´ë§Œ
    test_site = {'number': 9, 'name': 'ë²¤ì²˜ìŠ¤í€˜ì–´', 'url': 'https://www.venturesquare.net'}
    articles = scrape_venturesquare(test_site)
    print(f"ìˆ˜ì§‘ ê±´ìˆ˜: {len(articles)}")
    if articles:
        print("ì²« ë²ˆì§¸ ê¸°ì‚¬:", articles[0])
```

ì‹¤í–‰:
```powershell
python scrape_investment_news.py
```

### 2. ì „ì²´ ì‹¤í–‰

ëª¨ë“  ì‚¬ì´íŠ¸ í•¨ìˆ˜ ì‘ì„± ì™„ë£Œ í›„:
```powershell
python scrape_investment_news.py
```

### 3. ë°ì´í„° í™•ì¸

Supabase SQL Editorì—ì„œ:
```sql
-- ì „ì²´ ê±´ìˆ˜
SELECT COUNT(*) FROM investment_news_articles;

-- ì‚¬ì´íŠ¸ë³„ ê±´ìˆ˜
SELECT site_name, COUNT(*) as cnt
FROM investment_news_articles
GROUP BY site_name
ORDER BY cnt DESC;

-- ìµœê·¼ ê¸°ì‚¬ 10ê°œ
SELECT site_name, article_title, published_date
FROM investment_news_articles
ORDER BY collected_at DESC
LIMIT 10;
```

---

## âš ï¸ ì£¼ì˜ì‚¬í•­

### 1. ë™ì  ì‚¬ì´íŠ¸ (JavaScript ë Œë”ë§)
ì¼ë¶€ ì‚¬ì´íŠ¸ëŠ” JavaScriptë¡œ ì½˜í…ì¸ ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

**í™•ì¸ ë°©ë²•**:
```python
response = requests.get(url)
print(response.text)  # HTMLì— ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ JavaScript ì‚¬ì´íŠ¸
```

**í•´ê²°ì±…**: Selenium ì‚¬ìš©
```python
from selenium import webdriver
from selenium.webdriver.common.by import By

driver = webdriver.Chrome()
driver.get(url)
time.sleep(3)  # JavaScript ë¡œë”© ëŒ€ê¸°

# Seleniumìœ¼ë¡œ ìš”ì†Œ ì°¾ê¸°
articles = driver.find_elements(By.CSS_SELECTOR, 'article.post')
```

**Selenium ì„¤ì¹˜**:
```powershell
pip install selenium webdriver-manager
```

### 2. robots.txt í™•ì¸
ê° ì‚¬ì´íŠ¸ì˜ í¬ë¡¤ë§ ì •ì±… í™•ì¸:
```
https://www.venturesquare.net/robots.txt
```

### 3. ìš”ì²­ ì œí•œ
- ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ì€ IP ì°¨ë‹¨ ê°€ëŠ¥
- í˜„ì¬ ì„¤ì •: 2ì´ˆ ëŒ€ê¸° (ì ì ˆí•¨)

### 4. SSL ì—ëŸ¬
ë¡œê·¸ì— SSL ì—ëŸ¬ê°€ ìˆì—ˆìŒ (ë²¤ì²˜ê²½ì˜ì‹ ë¬¸):
```python
# SSL ê²€ì¦ ë¹„í™œì„±í™” (ì„ì‹œ)
response = requests.get(url, headers=HEADERS, verify=False)
```

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼

ê° ì‚¬ì´íŠ¸ë³„ í‰ê·  10-25ê±´ ìˆ˜ì§‘ ì˜ˆìƒ:
- ì´ 200-500ê±´

ì„±ê³µ ì‹œ:
```
âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!
ğŸ“Š ìˆ˜ì§‘ ê±´ìˆ˜: 287ê±´
ğŸ’¾ ì €ì¥ ê±´ìˆ˜: 287ê±´

ğŸ“ˆ ì‚¬ì´íŠ¸ë³„ ìˆ˜ì§‘ ê±´ìˆ˜:
  - ë²¤ì²˜ìŠ¤í€˜ì–´: 45ê±´
  - í”Œë˜í…€: 38ê±´
  - ë”ë¸Œì´ì”¨: 32ê±´
  ...
```

---

## ğŸš€ ì™„ë£Œ í›„ ì‘ì—…

### 1. ë­í‚¹ ì—…ë°ì´íŠ¸
Supabase SQL Editorì—ì„œ:
```sql
SELECT update_news_ranking();
```

### 2. ë­í‚¹ ì¡°íšŒ
```sql
SELECT * FROM v_latest_ranking;
```

### 3. ê²°ê³¼ ë³´ê³ 
ì‚¬ìš©ìì—ê²Œ ìµœì¢… ë­í‚¹ ì œê³µ

---

## ğŸ“ ë¬¸ì˜

**íŒŒì¼ ìœ„ì¹˜**: `C:\ValueLink\Valuation_Company\scripts\investment-news-scraper\`
**ë¬¸ì œ ë°œìƒ ì‹œ**: ë¡œê·¸ í™•ì¸ (`scraping_log.txt`)

---

**ì‘ì„± ì™„ë£Œ**: 2026-01-25
**ìš”ì²­ì**: ì‚¬ìš©ì
**ë‹´ë‹¹ì**: ì¬ë¯¸ë‚˜ ICI
