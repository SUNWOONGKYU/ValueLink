#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ê¸°ì—… ì •ë³´ ì¶”ì¶œ
- CEO, ì„¤ë¦½ì¼, ì§€ì—­, íˆ¬ìê¸ˆì•¡, ë‚ ì§œ
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)


def extract_ceo_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ CEO ì´ë¦„ ì¶”ì¶œ"""

    # ì œì™¸í•  í‚¤ì›Œë“œë“¤
    exclude_keywords = [
        'ì£¼ì‹íšŒì‚¬', 'ìŠ¤íƒ€íŠ¸ì—…', 'ê¸°ì—…', 'íšŒì‚¬',
        'ì™€ìš°í…Œì¼', 'ìš°í…Œì¼', 'ë²¤ì²˜ìŠ¤í€˜ì–´', 'ë”ë¸Œì´ì”¨',
        'ì¸í„°ë·°', 'ê¸°ì', 'í¸ì§‘ì¥', 'ì‘ì„±ì'
    ]

    # íŒ¨í„´ë“¤
    patterns = [
        r'ëŒ€í‘œ(?:ì´ì‚¬)?\s+([ê°€-í£]{2,4})(?:\s|,|\.)',
        r'([ê°€-í£]{2,4})\s+ëŒ€í‘œ(?:ì´ì‚¬)?(?:\s|,|\.)',
        r'CEO\s+([ê°€-í£]{2,4})(?:\s|,|\.)',
        r'([ê°€-í£]{2,4})\s+CEO(?:\s|,|\.)',
        r'ê³µë™ëŒ€í‘œ\s+([ê°€-í£]{2,4})(?:\s|,|\.)',
        r'\(ëŒ€í‘œ\s*([ê°€-í£]{2,4})\)',
    ]

    for pattern in patterns:
        matches = re.finditer(pattern, text)
        for match in matches:
            ceo_name = match.group(1)
            # ì œì™¸ í‚¤ì›Œë“œ í™•ì¸
            if len(ceo_name) >= 2:
                excluded = False
                for keyword in exclude_keywords:
                    if keyword in ceo_name:
                        excluded = True
                        break
                if not excluded:
                    return ceo_name

    return None


def extract_founded_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì„¤ë¦½ì¼ ì¶”ì¶œ"""

    # íŒ¨í„´ë“¤ (ë” ë‹¤ì–‘í•˜ê²Œ)
    patterns = [
        r'(\d{4})ë…„\s*(?:ì„¤ë¦½|ì°½ì—…|ì°½ë¦½|ì¶œë²”)',
        r'(?:ì„¤ë¦½|ì°½ì—…|ì°½ë¦½|ì¶œë²”)\s*(\d{4})ë…„',
        r'(\d{4})ë…„\s*(?:ì„¤ë¦½|ì°½ì—…|ì°½ë¦½|ì¶œë²”)(?:ëœ|í•œ|ëœ)',
        r'(\d{4})ë…„ì—\s*(?:ì„¤ë¦½|ì°½ì—…|ì°½ë¦½|ì¶œë²”)',
        r'(\d{4})ë…„\s*(?:ë¶€í„°|ì—)\s*(?:ì„¤ë¦½|ì°½ì—…|ì°½ë¦½|ì¶œë²”)',
        r'(\d{4})\.\d{1,2}\s*(?:ì„¤ë¦½|ì°½ì—…|ì°½ë¦½)',  # 2019.03 ì„¤ë¦½
        r'(\d{4})-\d{1,2}\s*(?:ì„¤ë¦½|ì°½ì—…|ì°½ë¦½)',   # 2019-03 ì„¤ë¦½
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            year = match.group(1)
            # 1990~2026 ì‚¬ì´ë§Œ ìœ íš¨
            if 1990 <= int(year) <= 2026:
                return f"{year}-01-01"  # YYYY-MM-DD í˜•ì‹

    return None


def extract_location_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì§€ì—­ ì¶”ì¶œ"""

    # íŒ¨í„´ë“¤ - ì£¼ìš” ì§€ì—­ëª…
    locations = [
        'íŒêµ', 'ê°•ë‚¨', 'ì„œì´ˆ', 'ì—­ì‚¼', 'ì‚¼ì„±', 'í…Œí—¤ë€',
        'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°',
        'ì„±ë‚¨', 'ìš©ì¸', 'ìˆ˜ì›', 'ì•ˆì–‘', 'ë¶€ì²œ',
        'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼'
    ]

    patterns = [
        r'([ê°€-í£]+)(?:ì—\s*ë³¸ì‚¬|ì—\s*ìœ„ì¹˜|ì†Œì¬)',
        r'ë³¸ì‚¬[ë¥¼]?\s*ë‘”\s*([ê°€-í£]+)',
        r'([ê°€-í£]+)\s*(?:ë³¸ì‚¬|ì‚¬ë¬´ì‹¤)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            location = match.group(1)
            # ì£¼ìš” ì§€ì—­ëª…ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
            for loc in locations:
                if loc in location:
                    return loc

    # ì§ì ‘ ì§€ì—­ëª… ê²€ìƒ‰
    for loc in locations:
        if loc in text:
            return loc

    return None


def extract_amount_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ íˆ¬ìê¸ˆì•¡ ì¶”ì¶œ"""

    patterns = [
        r'(\d+(?:\.\d+)?)\s*ì–µ\s*ì›',
        r'(\d+(?:\.\d+)?)\s*ì–µ',
        r'(\d+(?:,\d+)?)\s*ì–µ',
        r'(\d+)\s*ì¡°\s*(\d+)\s*ì–µ',
        r'(\d+)\s*ì¡°',
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            if 'ì¡°' in pattern:
                if len(match.groups()) == 2:  # Xì¡° Yì–µ
                    jo = int(match.group(1))
                    eok = int(match.group(2))
                    return jo * 10000 + eok
                else:  # Xì¡°
                    return int(match.group(1)) * 10000
            else:
                return int(float(match.group(1)))

    return None


def extract_date_from_html(soup, url):
    """HTMLì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""

    # ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
    meta_tags = [
        'article:published_time',
        'publishedDate',
        'datePublished',
        'pubdate',
    ]

    for tag in meta_tags:
        meta = soup.find('meta', property=tag) or soup.find('meta', attrs={'name': tag})
        if meta and meta.get('content'):
            try:
                date_str = meta.get('content')
                date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                return date_obj.strftime('%Y-%m-%d')
            except:
                pass

    # time íƒœê·¸ì—ì„œ ì¶”ì¶œ
    time_tag = soup.find('time')
    if time_tag:
        datetime_attr = time_tag.get('datetime')
        if datetime_attr:
            try:
                date_obj = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                return date_obj.strftime('%Y-%m-%d')
            except:
                pass

    # URLì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
    url_date_patterns = [
        r'/(\d{4})/(\d{2})/(\d{2})/',
        r'/(\d{8})/',
    ]

    for pattern in url_date_patterns:
        match = re.search(pattern, url)
        if match:
            if len(match.groups()) == 3:
                return f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
            else:
                date_str = match.group(1)
                return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"

    return None


def extract_company_info():
    """ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ê¸°ì—… ì •ë³´ ì¶”ì¶œ"""

    print("=" * 70)
    print("ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ê¸°ì—… ì •ë³´ ì¶”ì¶œ")
    print("=" * 70)

    # ì •ë³´ê°€ ì—†ëŠ” ë ˆì½”ë“œ ê°€ì ¸ì˜¤ê¸° (50ê°œì”© ì²˜ë¦¬)
    result = supabase.table("deals")\
        .select("id, company_name, news_url, news_title, ceo, founded, location, amount, news_date")\
        .limit(50)\
        .execute()

    deals = result.data
    print(f"\nì²˜ë¦¬í•  ë ˆì½”ë“œ: {len(deals)}ê°œ\n")

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    }

    success_count = 0
    fail_count = 0

    for idx, deal in enumerate(deals, 1):
        company_name = deal['company_name']
        news_url = deal.get('news_url')

        print(f"[{idx}/{len(deals)}] {company_name}...", end=" ")

        if not news_url:
            print("âŒ URL ì—†ìŒ")
            fail_count += 1
            continue

        try:
            # ë‰´ìŠ¤ í˜ì´ì§€ í¬ë¡¤ë§
            response = requests.get(news_url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            article_text = soup.get_text()

            # ì •ë³´ ì¶”ì¶œ
            update_data = {}

            # CEO ì¶”ì¶œ (ì—†ì„ ë•Œë§Œ)
            if not deal.get('ceo'):
                ceo = extract_ceo_from_text(article_text)
                if ceo:
                    update_data['ceo'] = ceo

            # ì„¤ë¦½ì¼ ì¶”ì¶œ (ì—†ì„ ë•Œë§Œ)
            if not deal.get('founded'):
                founded = extract_founded_from_text(article_text)
                if founded:
                    update_data['founded'] = founded

            # ì§€ì—­ ì¶”ì¶œ (ì—†ì„ ë•Œë§Œ)
            if not deal.get('location'):
                location = extract_location_from_text(article_text)
                if location:
                    update_data['location'] = location

            # íˆ¬ìê¸ˆì•¡ ì¶”ì¶œ (ì—†ì„ ë•Œë§Œ)
            if not deal.get('amount'):
                amount = extract_amount_from_text(article_text)
                if amount:
                    update_data['amount'] = amount

            # ë‚ ì§œ ì¶”ì¶œ (ì—†ì„ ë•Œë§Œ)
            if not deal.get('news_date'):
                news_date = extract_date_from_html(soup, news_url)
                if news_date:
                    update_data['news_date'] = news_date

            # DB ì—…ë°ì´íŠ¸
            if update_data:
                supabase.table("deals")\
                    .update(update_data)\
                    .eq("id", deal['id'])\
                    .execute()

                result_str = []
                if 'ceo' in update_data:
                    result_str.append(f"CEO: {update_data['ceo']}")
                if 'founded' in update_data:
                    result_str.append(f"ì„¤ë¦½: {update_data['founded']}")
                if 'location' in update_data:
                    result_str.append(f"ì§€ì—­: {update_data['location']}")
                if 'amount' in update_data:
                    result_str.append(f"ğŸ’° {update_data['amount']}ì–µ")
                if 'news_date' in update_data:
                    result_str.append(f"ğŸ“… {update_data['news_date']}")

                print(f"âœ… {', '.join(result_str)}")
                success_count += 1
            else:
                print("âš ï¸ ì •ë³´ ì—†ìŒ")
                fail_count += 1

            time.sleep(0.3)  # í¬ë¡¤ë§ ê°„ê²©

        except Exception as e:
            print(f"âŒ {str(e)[:30]}")
            fail_count += 1

    print("\n" + "=" * 70)
    print("ì¶”ì¶œ ì™„ë£Œ")
    print("=" * 70)
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
    print("=" * 70)


if __name__ == '__main__':
    extract_company_info()
