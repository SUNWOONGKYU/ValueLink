#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì‹¤ì œ ë‰´ìŠ¤ í˜ì´ì§€ í¬ë¡¤ë§í•´ì„œ ë‚ ì§œ ì¶”ì¶œ
"""

import os
import sys
import re
import requests
from datetime import datetime
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from supabase import create_client, Client

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

def extract_date_from_html(html_content, url):
    """HTMLì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
    soup = BeautifulSoup(html_content, 'html.parser')

    # 1. ë©”íƒ€ íƒœê·¸ì—ì„œ ì°¾ê¸°
    meta_patterns = [
        ('meta', {'property': 'article:published_time'}),
        ('meta', {'name': 'pubdate'}),
        ('meta', {'name': 'published_time'}),
        ('meta', {'property': 'og:published_time'}),
        ('meta', {'name': 'date'}),
        ('meta', {'itemprop': 'datePublished'}),
    ]

    for tag_name, attrs in meta_patterns:
        tag = soup.find(tag_name, attrs)
        if tag and tag.get('content'):
            content = tag.get('content')
            # ISO 8601 í˜•ì‹ (2026-01-15T12:00:00)
            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', content)
            if date_match:
                return date_match.group(1), f"ë©”íƒ€ íƒœê·¸ ({attrs})"

    # 2. time íƒœê·¸ì—ì„œ ì°¾ê¸°
    time_tags = soup.find_all('time')
    for time_tag in time_tags:
        datetime_attr = time_tag.get('datetime')
        if datetime_attr:
            date_match = re.search(r'(\d{4}-\d{2}-\d{2})', datetime_attr)
            if date_match:
                return date_match.group(1), "time íƒœê·¸ datetime"

        text = time_tag.get_text()
        date_match = re.search(r'(\d{4})[.-](\d{2})[.-](\d{2})', text)
        if date_match:
            return f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}", "time íƒœê·¸ í…ìŠ¤íŠ¸"

    # 3. class/idê°€ 'date', 'time', 'published' í¬í•¨í•˜ëŠ” ìš”ì†Œ
    date_elements = soup.find_all(class_=re.compile(r'(date|time|publish|byline)', re.I))
    date_elements += soup.find_all(id=re.compile(r'(date|time|publish)', re.I))

    for elem in date_elements:
        text = elem.get_text()
        # YYYY-MM-DD, YYYY.MM.DD, YYYY/MM/DD
        date_match = re.search(r'(\d{4})[.-/](\d{2})[.-/](\d{2})', text)
        if date_match:
            return f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}", f"ìš”ì†Œ class/id ({elem.get('class')})"

        # 2026ë…„ 1ì›” 15ì¼
        date_match = re.search(r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼', text)
        if date_match:
            year = date_match.group(1)
            month = date_match.group(2).zfill(2)
            day = date_match.group(3).zfill(2)
            return f"{year}-{month}-{day}", "í•œê¸€ ë‚ ì§œ í˜•ì‹"

    # 4. ë³¸ë¬¸ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (ìµœí›„ì˜ ìˆ˜ë‹¨)
    text = soup.get_text()

    # YYYY-MM-DD íŒ¨í„´
    date_matches = re.findall(r'(202[0-9]-\d{2}-\d{2})', text)
    if date_matches:
        # ê°€ì¥ ìµœê·¼ ë‚ ì§œ (2020~2029)
        valid_dates = [d for d in date_matches if 2020 <= int(d[:4]) <= 2029]
        if valid_dates:
            return valid_dates[0], "ë³¸ë¬¸ í…ìŠ¤íŠ¸"

    return None, "ë‚ ì§œ ì—†ìŒ"

def crawl_and_extract_date(url):
    """URL í¬ë¡¤ë§í•˜ì—¬ ë‚ ì§œ ì¶”ì¶œ"""

    # DuckDuckGo ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬
    if 'duckduckgo.com' in url:
        # uddg íŒŒë¼ë¯¸í„°ì—ì„œ ì‹¤ì œ URL ì¶”ì¶œ
        match = re.search(r'uddg=([^&]+)', url)
        if match:
            import urllib.parse
            url = urllib.parse.unquote(match.group(1))
            print(f"    ë¦¬ë‹¤ì´ë ‰íŠ¸: {url[:70]}...")

    # URLì´ //ë¡œ ì‹œì‘í•˜ë©´ https: ì¶”ê°€
    if url.startswith('//'):
        url = 'https:' + url

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
        response.raise_for_status()

        # ì¸ì½”ë”© ì„¤ì •
        response.encoding = response.apparent_encoding

        date, source = extract_date_from_html(response.content, url)
        return date, source

    except Exception as e:
        return None, f"í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)[:50]}"

print("=" * 80)
print("ë‰´ìŠ¤ í˜ì´ì§€ í¬ë¡¤ë§í•˜ì—¬ ë‚ ì§œ ì¶”ì¶œ")
print("=" * 80)

# 2026-01-28 ë‰´ìŠ¤ ì¡°íšŒ
deals = supabase.table("deals")\
    .select("*")\
    .eq("news_date", "2026-01-28")\
    .execute()

print(f"\nì²˜ë¦¬í•  Deal: {len(deals.data)}ê°œ\n")

update_count = 0

for idx, deal in enumerate(deals.data, 1):
    company = deal['company_name']
    url = deal.get('news_url', '')

    print(f"[{idx}/{len(deals.data)}] {company}")
    print(f"  URL: {url[:70]}...")

    date, source = crawl_and_extract_date(url)

    if date:
        print(f"  âœ… {date} (ì¶œì²˜: {source})")

        # ì—…ë°ì´íŠ¸
        supabase.table("deals")\
            .update({'news_date': date})\
            .eq("id", deal['id'])\
            .execute()

        supabase.table("investment_news_articles")\
            .update({'published_date': date})\
            .eq("article_url", deal['news_url'])\
            .execute()

        update_count += 1
    else:
        print(f"  âš ï¸  {source}")

print("\n" + "=" * 80)
print(f"âœ… {update_count}/{len(deals.data)}ê°œ ìˆ˜ì • ì™„ë£Œ")
print("=" * 80)

# ìµœì¢… í†µê³„
deals_updated = supabase.table("deals").select("news_date").execute()
from collections import Counter
date_counter = Counter([d['news_date'] for d in deals_updated.data if d.get('news_date')])

print(f"\nğŸ“Š ë‰´ìŠ¤ ê²Œì¬ì¼ ë¶„í¬ (ìƒìœ„ 15ê°œ):")
for date, count in sorted(date_counter.items(), reverse=True)[:15]:
    print(f"  {date}: {count}ê°œ")

print(f"\n2026-01-28 ë‚¨ì€ ê°œìˆ˜: {date_counter.get('2026-01-28', 0)}ê°œ")
