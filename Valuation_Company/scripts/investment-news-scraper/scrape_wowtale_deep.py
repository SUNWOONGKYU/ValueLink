#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WOWTALE ê¹Šì´ íƒìƒ‰ - ê²€ìƒ‰ + ì•„ì¹´ì´ë¸Œ + ì¹´í…Œê³ ë¦¬
"""

import os
import sys
import csv
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}


def get_wowtale_archive_urls():
    """WOWTALE ìµœê·¼ ì—¬ëŸ¬ ë‹¬ ì•„ì¹´ì´ë¸Œ URL ìˆ˜ì§‘"""

    # 2026ë…„ 1ì›”, 2025ë…„ 12ì›”, 11ì›”, 10ì›” íƒìƒ‰
    months = [
        '2026/01/',
        '2025/12/',
        '2025/11/',
        '2025/10/',
    ]

    all_urls = set()

    for month in months:
        print(f"\nğŸ“… {month} íƒìƒ‰ ì¤‘...")

        # ë©”ì¸ í˜ì´ì§€ íƒìƒ‰
        url = 'https://wowtale.net'
        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            links = soup.find_all('a', href=True)
            for link in links:
                href = link.get('href', '')
                if month in href and 'wowtale.net' in href:
                    all_urls.add(href)
                    print(f"  âœ… {href}")

        except Exception as e:
            print(f"  âŒ {month} íƒìƒ‰ ì‹¤íŒ¨: {e}")

        time.sleep(1)

    # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ íƒìƒ‰ (íˆ¬ì, ìŠ¤íƒ€íŠ¸ì—… ë“±)
    category_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'ìŠ¤íƒ€íŠ¸ì—…']

    print(f"\nğŸ“‚ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ íƒìƒ‰ ì¤‘...")
    url = 'https://wowtale.net'
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        links = soup.find_all('a', href=True)
        for link in links:
            href = link.get('href', '')
            text = link.get_text().strip()

            # íˆ¬ì ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ë§í¬
            if any(kw in text for kw in category_keywords):
                if 'wowtale.net' in href and '/202' in href:
                    all_urls.add(href)
                    print(f"  âœ… {href} ({text})")

    except Exception as e:
        print(f"  âŒ ì¹´í…Œê³ ë¦¬ íƒìƒ‰ ì‹¤íŒ¨: {e}")

    return list(all_urls)


def extract_article_data(url):
    """WOWTALE ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ"""

    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # ì œëª© ì¶”ì¶œ
        title_tag = soup.find('h1', class_='entry-title')
        if not title_tag:
            title_tag = soup.find('h1')
        if not title_tag:
            title_tag = soup.find('title')

        title = title_tag.get_text().strip() if title_tag else ""

        # íˆ¬ì í‚¤ì›Œë“œ í™•ì¸
        investment_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ']
        if not any(kw in title for kw in investment_keywords):
            return None

        # ë‚ ì§œ ì¶”ì¶œ
        date_tag = soup.find('time', class_='entry-date')
        if not date_tag:
            date_tag = soup.find('time')

        published_date = datetime.now().strftime('%Y-%m-%d')
        if date_tag:
            try:
                datetime_attr = date_tag.get('datetime')
                if datetime_attr:
                    dt = datetime.strptime(datetime_attr.split('T')[0], '%Y-%m-%d')
                    published_date = dt.strftime('%Y-%m-%d')
            except:
                pass

        return {
            'site_number': 1,
            'site_name': 'WOWTALE',
            'site_url': "",
            'article_title': title,
            'article_url': url,
            'published_date': published_date
        }

    except Exception as e:
        return None


def main():
    print("=" * 80)
    print("WOWTALE ê¹Šì´ íƒìƒ‰ (ê²€ìƒ‰ + ì•„ì¹´ì´ë¸Œ + ì¹´í…Œê³ ë¦¬)")
    print("=" * 80)

    # WOWTALE ì•„ì¹´ì´ë¸Œ URL ìˆ˜ì§‘
    urls = get_wowtale_archive_urls()

    print(f"\nì´ ë°œê²¬ URL: {len(urls)}ê°œ\n")

    found = 0
    duplicate = 0
    no_investment = 0

    for idx, url in enumerate(urls, 1):
        print(f"[{idx:3d}/{len(urls)}] {url[:60]}...", end=' ')

        # ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ
        article = extract_article_data(url)

        if not article:
            print("âŒ íˆ¬ì ë‰´ìŠ¤ ì•„ë‹˜")
            no_investment += 1
            continue

        # ì¤‘ë³µ í™•ì¸
        existing = supabase.table("investment_news_articles")\
            .select("id")\
            .eq("article_url", article['article_url'])\
            .execute()

        if not existing.data:
            try:
                supabase.table("investment_news_articles").insert(article).execute()
                print(f"âœ… {article['article_title'][:40]}...")
                found += 1
            except Exception as e:
                print(f"âŒ DB ì˜¤ë¥˜: {e}")
        else:
            print(f"âš ï¸ ì¤‘ë³µ")
            duplicate += 1

        time.sleep(0.5)

    print(f"\n{'='*80}")
    print("WOWTALE íƒìƒ‰ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"âœ… ìƒˆë¡œ ë°œê²¬: {found}ê°œ")
    print(f"âš ï¸ ì¤‘ë³µ: {duplicate}ê°œ")
    print(f"âŒ íˆ¬ì ë‰´ìŠ¤ ì•„ë‹˜: {no_investment}ê°œ")
    print(f"{'='*80}")

    # ìµœì¢… í†µê³„
    print(f"\ninvestment_news_articles í…Œì´ë¸” ì´ ë ˆì½”ë“œ:")
    count_result = supabase.table("investment_news_articles").select("id", count="exact").execute()
    print(f"  {count_result.count}ê°œ")


if __name__ == '__main__':
    main()
