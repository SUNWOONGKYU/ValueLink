#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íˆ¬ì ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ - ìµœì í™”ëœ 5ê°œ ë¯¸ë””ì–´
"""

import csv
import sys
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import quote


def safe_print(text, end='\n'):
    """ì¸ì½”ë”© ì—ëŸ¬ ë°©ì§€ ì¶œë ¥"""
    try:
        print(text, end=end)
    except UnicodeEncodeError:
        print(text.encode('cp949', errors='replace').decode('cp949'), end=end)


# ìµœì í™”ëœ 5ê°œ ë‰´ìŠ¤ ë¯¸ë””ì–´ (11ê°œ â†’ 5ê°œ)
MEDIA_SITES = [
    {
        'name': 'WOWTALE',
        'search_url': 'https://www.wowtale.net/?s={keyword}',
        'link_selector': 'h2 a, h3 a',
        'priority': 1,  # ë©”ì¸ ì†ŒìŠ¤ (76% ì»¤ë²„)
    },
    {
        'name': 'ë²¤ì²˜ìŠ¤í€˜ì–´',
        'search_url': 'https://www.venturesquare.net/?s={keyword}',
        'link_selector': 'h2.entry-title a, h3.entry-title a, a.post-title',
        'priority': 2,  # ì„œë¸Œ ì†ŒìŠ¤ (21% ì»¤ë²„)
    },
    {
        'name': 'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´',
        'search_url': 'https://www.startuptoday.kr/news/articleList.html?sc_area=A&view_type=sm&sc_word={keyword}',
        'link_selector': 'div.list-titles a, h4.titles a',
        'priority': 3,  # í‹ˆìƒˆ ì†ŒìŠ¤ (2% ì»¤ë²„)
    },
    {
        'name': 'ì•„ì›ƒìŠ¤íƒ ë”©',
        'search_url': 'https://outstanding.kr/?s={keyword}',
        'link_selector': 'h2 a, h3 a, a.article-link',
        'priority': 4,  # ì‹¬ì¸µ ë¶„ì„, í™•ì¥ìš©
    },
    {
        'name': 'í”Œë˜í…€',
        'search_url': 'https://platum.kr/?s={keyword}',
        'link_selector': 'h2.entry-title a, div.post-title a',
        'priority': 5,  # í•´ì™¸ ë‰´ìŠ¤, í™•ì¥ìš©
    },
]


def search_in_media(company_name):
    """
    5ê°œ ë¯¸ë””ì–´ì—ì„œ íˆ¬ì ë‰´ìŠ¤ ê²€ìƒ‰

    ìš°ì„ ìˆœìœ„:
    1. WOWTALE (ë©”ì¸, 76% ì»¤ë²„)
    2. ë²¤ì²˜ìŠ¤í€˜ì–´ (ì„œë¸Œ, 21% ì»¤ë²„)
    3. ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´ (í‹ˆìƒˆ, 2% ì»¤ë²„)
    4. ì•„ì›ƒìŠ¤íƒ ë”© (í™•ì¥)
    5. í”Œë˜í…€ (í™•ì¥)
    """

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Referer': 'https://www.google.com/',
    }

    keyword = quote(company_name)

    # ìš°ì„ ìˆœìœ„ ìˆœì„œë¡œ ê²€ìƒ‰
    for site in sorted(MEDIA_SITES, key=lambda x: x['priority']):
        try:
            search_url = site['search_url'].format(keyword=keyword)

            response = requests.get(search_url, headers=headers, timeout=8)

            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')

                # ê²€ìƒ‰ ê²°ê³¼ ë§í¬ ì°¾ê¸°
                for selector in site['link_selector'].split(','):
                    links = soup.select(selector.strip())

                    for link in links[:5]:  # ìƒìœ„ 5ê°œë§Œ
                        title = link.get_text(strip=True)
                        url = link.get('href', '')

                        # íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
                        investment_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ']
                        if any(kw in title for kw in investment_keywords):
                            # ìƒëŒ€ ê²½ë¡œë©´ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                            if url.startswith('/'):
                                base_url = site['search_url'].split('?')[0].rsplit('/', 1)[0]
                                url = base_url + url

                            if url.startswith('http'):
                                return url, site['name']

            time.sleep(0.2)  # ì‚¬ì´íŠ¸ ë¶€í•˜ ë°©ì§€

        except Exception as e:
            continue

    return None, None


def scrape_companies(input_file, output_file):
    """ê¸°ì—… ëª©ë¡ì—ì„œ ë‰´ìŠ¤ URL ìˆ˜ì§‘"""

    print("=" * 70)
    print("íˆ¬ì ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ (5ê°œ ë¯¸ë””ì–´)")
    print("=" * 70)

    # ì†ŒìŠ¤ ëª©ë¡ ì¶œë ¥
    print("\nğŸ“° ê²€ìƒ‰ ì†ŒìŠ¤:")
    for idx, site in enumerate(sorted(MEDIA_SITES, key=lambda x: x['priority']), 1):
        print(f"  {idx}. {site['name']}")

    print("\n" + "=" * 70)

    # CSV ì½ê¸°
    with open(input_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        companies = list(reader)

    total = len(companies)
    safe_print(f"\nì´ ê¸°ì—… ìˆ˜: {total}ê°œ\n")

    found_count = 0
    results = []

    for idx, company in enumerate(companies, 1):
        company_name = company['ê¸°ì—…ëª…']

        safe_print(f"[{idx}/{total}] {company_name}...", end=" ")

        news_url, source_name = search_in_media(company_name)

        if news_url:
            company['ë‰´ìŠ¤URL'] = news_url
            company['ë‰´ìŠ¤ì†ŒìŠ¤'] = source_name
            found_count += 1
            safe_print(f"âœ… [{source_name}]")
        else:
            company['ë‰´ìŠ¤URL'] = ''
            company['ë‰´ìŠ¤ì†ŒìŠ¤'] = ''
            safe_print("âŒ")

        results.append(company)

    # ê²°ê³¼ ì €ì¥
    with open(output_file, 'w', encoding='utf-8', newline='') as f:
        if results:
            writer = csv.DictWriter(f, fieldnames=results[0].keys())
            writer.writeheader()
            writer.writerows(results)

    # í†µê³„
    print("\n" + "=" * 70)
    print("ìˆ˜ì§‘ ì™„ë£Œ")
    print("=" * 70)
    print(f"âœ… ë°œê²¬: {found_count}ê°œ")
    print(f"âŒ ë¯¸ë°œê²¬: {total - found_count}ê°œ")
    print(f"ğŸ“Š ì„±ê³µë¥ : {found_count * 100 / total:.1f}%")

    # ì†ŒìŠ¤ë³„ í†µê³„
    source_stats = {}
    for company in results:
        source = company.get('ë‰´ìŠ¤ì†ŒìŠ¤', '')
        if source:
            source_stats[source] = source_stats.get(source, 0) + 1

    if source_stats:
        print("\nğŸ“° ì†ŒìŠ¤ë³„ ë°œê²¬ ìˆ˜:")
        for source, count in sorted(source_stats.items(), key=lambda x: x[1], reverse=True):
            print(f"  - {source}: {count}ê°œ ({count * 100 / found_count:.1f}%)")

    print("=" * 70)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    if len(sys.argv) < 3:
        print("ì‚¬ìš©ë²•: python news_scraper_5sources.py <ì…ë ¥íŒŒì¼> <ì¶œë ¥íŒŒì¼>")
        print("\nì˜ˆì‹œ:")
        print("  python news_scraper_5sources.py companies.csv results.csv")
        sys.exit(1)

    input_file = sys.argv[1]
    output_file = sys.argv[2]

    try:
        scrape_companies(input_file, output_file)
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
