#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íˆ¬ì ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ (Selenium ë²„ì „)
Seleniumì„ ì‚¬ìš©í•˜ì—¬ JavaScript ë Œë”ë§ ì‚¬ì´íŠ¸ë„ ìŠ¤í¬ë˜í•‘
"""

import os
import time
import json
import logging
from datetime import datetime, date
from typing import List, Dict
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from dotenv import load_dotenv
import requests

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraping_selenium_log.txt', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Supabase ì—°ê²°
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')

# ê¸°ê°„ ì„¤ì •
START_DATE = date(2026, 1, 1)
END_DATE = date.today()

# í‚¤ì›Œë“œ
KEYWORDS = ['íˆ¬ì', 'íˆ¬ììœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'ë²¤ì²˜ìºí”¼í„¸', 'VC', 'ì—”ì ¤íˆ¬ì', 'M&A', 'ì¸ìˆ˜']


def setup_driver():
    """Chrome WebDriver ì„¤ì •"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver


def contains_keyword(text: str) -> bool:
    """íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€"""
    if not text:
        return False
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in KEYWORDS)


def parse_date(date_str: str) -> str:
    """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    date_formats = [
        '%Y-%m-%d',
        '%Y.%m.%d',
        '%Y/%m/%d',
        '%Yë…„ %mì›” %dì¼',
    ]

    for fmt in date_formats:
        try:
            parsed_date = datetime.strptime(date_str.strip(), fmt).date()
            return parsed_date.isoformat()
        except ValueError:
            continue

    return None


def scrape_thevc(driver):
    """THE VC ìŠ¤í¬ë˜í•‘"""
    articles = []
    site_number = 8
    site_name = "ë”ë¸Œì´ì”¨"
    site_url = "https://thevc.kr"

    logger.info(f"ğŸ” [{site_name}] ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    try:
        # íˆ¬ì í˜ì´ì§€ ì ‘ì†
        url = "https://thevc.kr/browse/investments"
        driver.get(url)

        # JavaScript ë Œë”ë§ ëŒ€ê¸°
        time.sleep(3)

        # í˜ì´ì§€ ìŠ¤í¬ë¡¤ (lazy loading ëŒ€ì‘)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

        # íˆ¬ì í•­ëª© ì°¾ê¸°
        # THE VCëŠ” íˆ¬ì DB í˜•íƒœì´ë¯€ë¡œ ë‹¤ë¥¸ ë°©ì‹ í•„ìš”
        logger.info(f"  í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {driver.title}")

        # í˜ì´ì§€ ì†ŒìŠ¤ í™•ì¸
        page_source = driver.page_source

        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸: í˜ì´ì§€ì— "íˆ¬ì" í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        if "íˆ¬ì" in page_source:
            logger.info(f"  âœ… í˜ì´ì§€ì—ì„œ 'íˆ¬ì' í‚¤ì›Œë“œ ë°œê²¬")

        # TODO: THE VCì˜ ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ê²Œ ìŠ¤í¬ë˜í•‘ ë¡œì§ êµ¬í˜„
        # í˜„ì¬ëŠ” í…ŒìŠ¤íŠ¸ ë‹¨ê³„

    except Exception as e:
        logger.error(f"  âŒ ì—ëŸ¬: {e}")

    logger.info(f"âœ… [{site_name}] {len(articles)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    return articles


def scrape_platum(driver):
    """í”Œë˜í…€ ìŠ¤í¬ë˜í•‘"""
    articles = []
    site_number = 10
    site_name = "í”Œë˜í…€"
    site_url = "https://platum.kr"

    logger.info(f"ğŸ” [{site_name}] ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    try:
        # ë‰´ìŠ¤ í˜ì´ì§€
        url = "https://platum.kr/news/"
        driver.get(url)
        time.sleep(3)

        logger.info(f"  í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {driver.title}")

        # TODO: í”Œë˜í…€ HTML êµ¬ì¡°ì— ë§ê²Œ êµ¬í˜„

    except Exception as e:
        logger.error(f"  âŒ ì—ëŸ¬: {e}")

    logger.info(f"âœ… [{site_name}] {len(articles)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    return articles


def save_to_json(articles: List[Dict], filename: str = "inbox/investment_news_data.json"):
    """JSON íŒŒì¼ë¡œ ì €ì¥"""
    filepath = os.path.join(os.path.dirname(__file__), filename)

    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ìˆìœ¼ë©´)
    existing_data = []
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        except:
            pass

    # ì¤‘ë³µ ì œê±° (article_url ê¸°ì¤€)
    existing_urls = {article.get('article_url') for article in existing_data}
    new_articles = [a for a in articles if a.get('article_url') not in existing_urls]

    # ë³‘í•©
    all_articles = existing_data + new_articles

    # ì €ì¥
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(all_articles, f, ensure_ascii=False, indent=2)

    logger.info(f"ğŸ’¾ JSON ì €ì¥: {len(new_articles)}ê±´ ì¶”ê°€ (ì´ {len(all_articles)}ê±´)")
    return len(new_articles)


def save_to_supabase(articles: List[Dict]) -> int:
    """Supabaseì— ì €ì¥ (REST API)"""
    if not articles:
        return 0

    saved_count = 0
    api_url = f"{SUPABASE_URL}/rest/v1/investment_news_articles"
    headers = {
        'apikey': SUPABASE_KEY,
        'Authorization': f'Bearer {SUPABASE_KEY}',
        'Content-Type': 'application/json',
        'Prefer': 'return=representation'
    }

    for article in articles:
        try:
            response = requests.post(api_url, json=article, headers=headers, timeout=30)

            if response.status_code == 201:
                saved_count += 1
                logger.info(f"  ğŸ’¾ Supabase ì €ì¥: {article['article_title'][:50]}...")
            elif response.status_code == 409:
                logger.info(f"  âš ï¸  ì¤‘ë³µ URL: {article['article_title'][:50]}...")
            else:
                logger.error(f"  âŒ ì €ì¥ ì‹¤íŒ¨ (HTTP {response.status_code})")
        except Exception as e:
            logger.error(f"  âŒ ì €ì¥ ì—ëŸ¬: {e}")

    return saved_count


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    logger.info("=" * 60)
    logger.info("ğŸ“° íˆ¬ì ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹œì‘ (Selenium)")
    logger.info(f"ğŸ“… ê¸°ê°„: {START_DATE} ~ {END_DATE}")
    logger.info("=" * 60)

    driver = None
    all_articles = []

    try:
        # WebDriver ì„¤ì •
        logger.info("ğŸ”§ Chrome WebDriver ì„¤ì • ì¤‘...")
        driver = setup_driver()
        logger.info("âœ… WebDriver ì¤€ë¹„ ì™„ë£Œ")

        # í…ŒìŠ¤íŠ¸: THE VC ìŠ¤í¬ë˜í•‘
        articles_thevc = scrape_thevc(driver)
        all_articles.extend(articles_thevc)

        # í…ŒìŠ¤íŠ¸: í”Œë˜í…€ ìŠ¤í¬ë˜í•‘
        articles_platum = scrape_platum(driver)
        all_articles.extend(articles_platum)

        # JSON ì €ì¥
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ’¾ JSON íŒŒì¼ ì €ì¥ ì¤‘...")
        saved_json = save_to_json(all_articles)

        # Supabase ì €ì¥
        if all_articles:
            logger.info("ğŸ’¾ Supabase ì €ì¥ ì¤‘...")
            saved_db = save_to_supabase(all_articles)
            logger.info(f"âœ… Supabase ì €ì¥ ì™„ë£Œ: {saved_db}ê±´")

        logger.info("\n" + "=" * 60)
        logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(all_articles)}ê±´")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ ì¹˜ëª…ì  ì—ëŸ¬: {e}", exc_info=True)

    finally:
        if driver:
            driver.quit()
            logger.info("ğŸ”§ WebDriver ì¢…ë£Œ")


if __name__ == '__main__':
    main()
