#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì¶”ê°€ ì–¸ë¡ ì‚¬ì—ì„œ ëª» ì°¾ì€ 3ê°œ ê¸°ì—… ê²€ìƒ‰
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time
from urllib.parse import quote

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# ëª» ì°¾ì€ 3ê°œ ê¸°ì—…
target_companies = {
    "ë¶€ìŠ¤í‹°ìŠ¤": "SBIì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸",
    "ì• í”Œì—ì´ì•„ì´": "ê´‘ë¦¼ë²¤ì²˜ìŠ¤",
    "ì†Œì…œë¦­ìŠ¤ì½”ë¦¬ì•„": "ë„¤ì´ë²„ D2SF"
}

# ì¶”ê°€ ì–¸ë¡ ì‚¬
ADDITIONAL_MEDIA = {
    "ë”ë²¨": {
        "number": 16,
        "search_url": "https://www.thebell.co.kr/search/result.asp?search_key={}",
        "domain": "thebell.co.kr"
    },
    "ë¸”ë¡œí„°": {
        "number": 22,
        "search_url": "https://www.bloter.net/?s={}",
        "domain": "bloter.net"
    },
    "ì§€ë””ë„·ì½”ë¦¬ì•„": {
        "number": 99,
        "search_url": "https://zdnet.co.kr/search/?query={}",
        "domain": "zdnet.co.kr"
    },
    "ì „ìì‹ ë¬¸": {
        "number": 99,
        "search_url": "https://www.etnews.com/search?kw={}",
        "domain": "etnews.com"
    },
    "ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸": {
        "number": 23,
        "search_url": "https://economist.co.kr/?s={}",
        "domain": "economist.co.kr"
    },
    "AIíƒ€ì„ìŠ¤": {
        "number": 19,
        "search_url": "https://www.aitimes.com/search/search.html?kwd={}",
        "domain": "aitimes.com"
    }
}


def search_media_site(media_name, media_info, company_name, investor):
    """ê° ì–¸ë¡ ì‚¬ ì‚¬ì´íŠ¸ ë‚´ ê²€ìƒ‰"""

    # ê²€ìƒ‰ ì¿¼ë¦¬
    queries = [
        f"{company_name} {investor} íˆ¬ì",
        f"{company_name} íˆ¬ììœ ì¹˜",
        f"{company_name} í€ë”©"
    ]

    for query in queries:
        search_url = media_info['search_url'].format(quote(query))

        try:
            response = requests.get(search_url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            # ëª¨ë“  ë§í¬ ê²€ìƒ‰
            links = soup.find_all('a', href=True)

            for link in links:
                href = link.get('href', '')
                text = link.get_text().strip()

                # ê¸°ì—…ëª… í™•ì¸
                if company_name not in text:
                    continue

                # íˆ¬ì í‚¤ì›Œë“œ í™•ì¸
                investment_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ']
                if not any(kw in text for kw in investment_keywords):
                    continue

                # í•´ë‹¹ ì–¸ë¡ ì‚¬ ë„ë©”ì¸ í™•ì¸
                if media_info['domain'] in href or href.startswith('/'):
                    # ì ˆëŒ€ URL ë³€í™˜
                    if href.startswith('/'):
                        base_url = f"https://{media_info['domain']}"
                        href = base_url + href

                    return {
                        'title': text,
                        'url': href,
                        'query': query
                    }

            time.sleep(0.5)

        except Exception as e:
            continue

    return None


def main():
    print("=" * 80)
    print("ì¶”ê°€ ì–¸ë¡ ì‚¬ì—ì„œ ëª» ì°¾ì€ 3ê°œ ê¸°ì—… ê²€ìƒ‰")
    print("=" * 80)
    print(f"\nğŸ¯ íƒ€ê²Ÿ: {', '.join(target_companies.keys())}\n")

    total_found = 0
    total_duplicate = 0
    found_companies = set()

    for company, investor in target_companies.items():
        print(f"\n{'='*80}")
        print(f"ğŸ” {company} (íˆ¬ì: {investor})")
        print(f"{'='*80}")

        for media_name, media_info in ADDITIONAL_MEDIA.items():
            print(f"\n  [{media_name}] ê²€ìƒ‰ ì¤‘...", end=' ')

            result = search_media_site(media_name, media_info, company, investor)

            if result:
                print(f"\n  âœ… ë°œê²¬!")
                print(f"     ì œëª©: {result['title'][:60]}...")
                print(f"     URL: {result['url']}")
                print(f"     ê²€ìƒ‰ì–´: {result['query']}")

                # DB ì €ì¥
                article = {
                    'site_number': media_info['number'],
                    'site_name': media_name,
                    'site_url': "",
                    'article_title': result['title'],
                    'article_url': result['url'],
                    'published_date': datetime.now().strftime('%Y-%m-%d')
                }

                # ì¤‘ë³µ í™•ì¸
                existing = supabase.table("investment_news_articles")\
                    .select("id")\
                    .eq("article_url", article['article_url'])\
                    .execute()

                if not existing.data:
                    try:
                        supabase.table("investment_news_articles").insert(article).execute()
                        print(f"     ğŸ’¾ DB ì €ì¥ ì™„ë£Œ")
                        total_found += 1
                        found_companies.add(company)
                        break  # í•˜ë‚˜ ì°¾ìœ¼ë©´ ë‹¤ìŒ ê¸°ì—…ìœ¼ë¡œ
                    except:
                        print(f"     âŒ DB ì˜¤ë¥˜")
                else:
                    print(f"     âš ï¸  ì¤‘ë³µ")
                    total_duplicate += 1
                    found_companies.add(company)
                    break
            else:
                print("ëª» ì°¾ìŒ")

        time.sleep(1)

    print(f"\n{'='*80}")
    print("ì¶”ê°€ ì–¸ë¡ ì‚¬ ê²€ìƒ‰ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"âœ… ìƒˆë¡œ ë°œê²¬: {total_found}ê°œ")
    print(f"âš ï¸  ì¤‘ë³µ: {total_duplicate}ê°œ")
    print(f"{'='*80}")

    if found_companies:
        print(f"\nğŸ‰ ë°œê²¬ëœ ê¸°ì—…:")
        for company in found_companies:
            print(f"  âœ… {company}")

    not_found = set(target_companies.keys()) - found_companies
    if not_found:
        print(f"\nâŒ ìµœì¢… ë¯¸ë°œê²¬ ê¸°ì—… ({len(not_found)}ê°œ):")
        for company in not_found:
            print(f"  - {company}")
    else:
        print(f"\nğŸ‰ğŸ‰ğŸ‰ ëª¨ë“  ê¸°ì—… ë°œê²¬ ì™„ë£Œ!")

    # ìµœì¢… í†µê³„
    count_result = supabase.table("investment_news_articles").select("id", count="exact").execute()
    print(f"\ninvestment_news_articles í…Œì´ë¸” ì´ ë ˆì½”ë“œ: {count_result.count}ê°œ")


if __name__ == '__main__':
    main()
