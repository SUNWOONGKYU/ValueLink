#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ íˆ¬ìê¸ˆì•¡ + ë‚ ì§œ ì¶”ì¶œ
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)


def extract_amount_from_text(text):
    """í…ìŠ¤íŠ¸ì—ì„œ íˆ¬ìê¸ˆì•¡ ì¶”ì¶œ"""

    # íŒ¨í„´ë“¤
    patterns = [
        r'(\d+(?:\.\d+)?)\s*ì–µ\s*ì›',
        r'(\d+(?:\.\d+)?)\s*ì–µ',
        r'(\d+(?:,\d+)?)\s*ì–µ',
        r'(\d+)\s*ì¡°\s*(\d+)\s*ì–µ',
        r'(\d+)\s*ì¡°',
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            if 'ì¡°' in pattern:
                if len(match.groups()) == 2:  # Xì¡° Yì–µ
                    jo = int(match.group(1))
                    eok = int(match.group(2))
                    return jo * 10000 + eok
                else:  # Xì¡°
                    return int(match.group(1)) * 10000
            else:
                amount_str = match.group(1).replace(',', '').replace('.', '')
                return int(float(match.group(1)))

    return None


def extract_date_from_html(soup, url):
    """HTMLì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""

    # ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
    meta_tags = [
        'article:published_time',
        'publishedDate',
        'datePublished',
        'pubdate',
    ]

    for tag in meta_tags:
        meta = soup.find('meta', property=tag) or soup.find('meta', attrs={'name': tag})
        if meta and meta.get('content'):
            try:
                date_str = meta.get('content')
                # ISO í˜•ì‹ íŒŒì‹±
                date_obj = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                return date_obj.strftime('%Y-%m-%d')
            except:
                pass

    # time íƒœê·¸ì—ì„œ ì¶”ì¶œ
    time_tag = soup.find('time')
    if time_tag:
        datetime_attr = time_tag.get('datetime')
        if datetime_attr:
            try:
                date_obj = datetime.fromisoformat(datetime_attr.replace('Z', '+00:00'))
                return date_obj.strftime('%Y-%m-%d')
            except:
                pass

    # URLì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸° (YYYY/MM/DD ë˜ëŠ” YYYYMMDD)
    url_date_patterns = [
        r'/(\d{4})/(\d{2})/(\d{2})/',
        r'/(\d{8})/',
    ]

    for pattern in url_date_patterns:
        match = re.search(pattern, url)
        if match:
            if len(match.groups()) == 3:
                return f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
            else:
                date_str = match.group(1)
                return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"

    return None


def extract_from_news():
    """ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""

    print("=" * 70)
    print("ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ íˆ¬ìê¸ˆì•¡ + ë‚ ì§œ ì¶”ì¶œ")
    print("=" * 70)

    # amountê°€ ì—†ëŠ” ë ˆì½”ë“œ ê°€ì ¸ì˜¤ê¸°
    result = supabase.table("deals")\
        .select("id, company_name, news_url, news_title")\
        .is_("amount", "null")\
        .limit(50)\
        .execute()

    deals = result.data
    print(f"\nì²˜ë¦¬í•  ë ˆì½”ë“œ: {len(deals)}ê°œ\n")

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    }

    success_count = 0
    fail_count = 0

    for idx, deal in enumerate(deals, 1):
        company_name = deal['company_name']
        news_url = deal.get('news_url')
        news_title = deal.get('news_title', '')

        print(f"[{idx}/{len(deals)}] {company_name}...", end=" ")

        if not news_url:
            print("âŒ URL ì—†ìŒ")
            fail_count += 1
            continue

        try:
            # ë‰´ìŠ¤ í˜ì´ì§€ í¬ë¡¤ë§
            response = requests.get(news_url, headers=headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            article_text = soup.get_text()

            # íˆ¬ìê¸ˆì•¡ ì¶”ì¶œ (ì œëª©ì—ì„œ ë¨¼ì € ì‹œë„)
            amount = extract_amount_from_text(news_title)
            if not amount:
                amount = extract_amount_from_text(article_text)

            # ë‚ ì§œ ì¶”ì¶œ
            news_date = extract_date_from_html(soup, news_url)

            # DB ì—…ë°ì´íŠ¸
            update_data = {}
            if amount:
                update_data['amount'] = amount
            if news_date:
                update_data['news_date'] = news_date

            if update_data:
                supabase.table("deals")\
                    .update(update_data)\
                    .eq("id", deal['id'])\
                    .execute()

                result_str = []
                if amount:
                    result_str.append(f"ğŸ’° {amount}ì–µ")
                if news_date:
                    result_str.append(f"ğŸ“… {news_date}")

                print(f"âœ… {' '.join(result_str)}")
                success_count += 1
            else:
                print("âš ï¸ ì •ë³´ ì—†ìŒ")
                fail_count += 1

            time.sleep(0.3)  # í¬ë¡¤ë§ ê°„ê²©

        except Exception as e:
            print(f"âŒ {str(e)[:30]}")
            fail_count += 1

    print("\n" + "=" * 70)
    print("ì¶”ì¶œ ì™„ë£Œ")
    print("=" * 70)
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
    print("=" * 70)


if __name__ == '__main__':
    extract_from_news()
