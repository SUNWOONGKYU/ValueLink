#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
92ê°œ ê¸°ì—…ì˜ ì‹¤ì œ íˆ¬ì ë‰´ìŠ¤ URLì„ investment_news_articles í…Œì´ë¸”ì— ì €ì¥
"""

import os
import sys
import csv
from datetime import datetime, timedelta
from dotenv import load_dotenv
from supabase import create_client, Client

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

# ì‚¬ì´íŠ¸ëª… â†’ site_number ë§¤í•‘
SITE_MAPPING = {
    'WOWTALE': 1,
    'ë²¤ì²˜ìŠ¤í€˜ì–´': 9,
    'í”Œë˜í…€': 10,
    'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´': 11,
    'ì•„ì›ƒìŠ¤íƒ ë”©': 13,
    'ë”ë¸Œì´ì”¨': 8,
    'ë”ë²¨': 16,
    'ì¡°ì„ ë¹„ì¦ˆ': 24,
    'ë§¤ì¼ê²½ì œ': 24,
    'MKí…Œí¬ë¦¬ë·°': 24,
    'ë¸”ë¡œí„°': 22,
    'ë¡œë´‡ì‹ ë¬¸': 99,
    'ì•„ì‹œì•„ê²½ì œ': 99,
    'í•œêµ­ê²½ì œ': 23,
    'ì´ë°ì¼ë¦¬': 99,
    'í—¬ë¡œí‹°': 99,
    'ZDNet Korea': 15,
    'ì›í‹°ë“œ': 99,
    'ì½”ë¦¬ì•„ë°ì¼ë¦¬': 99,
    'ì„œìš¸ê²½ì œ': 99,
    'ìŠ¤íƒ€íŠ¸ì—… íˆ¬ë°ì´': 11
}

# ì‚¬ì´íŠ¸ëª… â†’ site_url ë§¤í•‘
SITE_URL_MAPPING = {
    'WOWTALE': 'https://wowtale.net',
    'ë²¤ì²˜ìŠ¤í€˜ì–´': 'https://www.venturesquare.net',
    'í”Œë˜í…€': 'https://platum.kr',
    'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´': 'https://www.startuptoday.kr',
    'ì•„ì›ƒìŠ¤íƒ ë”©': 'https://outstanding.kr',
    'ë”ë¸Œì´ì”¨': 'https://thevc.kr',
    'ë”ë²¨': 'https://www.thebell.co.kr',
    'ì¡°ì„ ë¹„ì¦ˆ': 'https://biz.chosun.com',
    'ë§¤ì¼ê²½ì œ': 'https://www.mk.co.kr',
    'MKí…Œí¬ë¦¬ë·°': 'https://www.mk.co.kr',
    'ë¸”ë¡œí„°': 'https://www.bloter.net',
    'ë¡œë´‡ì‹ ë¬¸': 'https://www.irobotnews.com',
    'ì•„ì‹œì•„ê²½ì œ': 'https://www.asiae.co.kr',
    'í•œêµ­ê²½ì œ': 'https://www.hankyung.com',
    'ì´ë°ì¼ë¦¬': 'https://www.edaily.co.kr',
    'í—¬ë¡œí‹°': 'https://www.hellot.net',
    'ZDNet Korea': 'https://www.zdnet.co.kr',
    'ì›í‹°ë“œ': 'https://www.wanted.co.kr',
    'ì½”ë¦¬ì•„ë°ì¼ë¦¬': 'https://www.koreadaily.com',
    'ì„œìš¸ê²½ì œ': 'https://www.sedaily.com',
    'ìŠ¤íƒ€íŠ¸ì—… íˆ¬ë°ì´': 'https://www.startuptoday.kr'
}


def get_published_date(week_str):
    """ì£¼ì°¨ ì •ë³´ë¡œ ë°œí–‰ì¼ ì¶”ì •"""
    # 2026ë…„ 1ì›” 27ì¼ ê¸°ì¤€ ì—­ì‚°
    base_date = datetime(2026, 1, 27)

    # ì£¼ì°¨ íŒŒì‹±
    if 'ì£¼ì°¨' in week_str:
        week_num = int(week_str.replace('ì£¼ì°¨', ''))
        # 5ì£¼ì°¨ = ìµœì‹  (ì˜¤ëŠ˜)
        # 4ì£¼ì°¨ = 1ì£¼ ì „
        # 3ì£¼ì°¨ = 2ì£¼ ì „
        weeks_ago = 5 - week_num
        target_date = base_date - timedelta(weeks=weeks_ago)
        return target_date.strftime('%Y-%m-%d')

    return base_date.strftime('%Y-%m-%d')


def main():
    print("=" * 60)
    print("92ê°œ ê¸°ì—… ë‰´ìŠ¤ URL â†’ investment_news_articles í…Œì´ë¸” ì €ì¥")
    print("=" * 60)

    csv_file = 'final_found_urls.csv'

    # CSV ì½ê¸°
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        companies = list(reader)

    print(f"\nì´ {len(companies)}ê°œ ë ˆì½”ë“œ\n")

    saved_count = 0
    duplicate_count = 0
    error_count = 0

    for idx, row in enumerate(companies, 1):
        company_name = row['ê¸°ì—…ëª…']
        stage = row['ë‹¨ê³„']
        amount = row['ì‹ ê·œ']
        news_url = row['ë‰´ìŠ¤URL']
        site_name = row['ë‰´ìŠ¤ì†ŒìŠ¤']
        week = row['ì£¼ì°¨']

        # ì‚¬ì´íŠ¸ ì •ë³´
        site_number = SITE_MAPPING.get(site_name, 99)
        site_url = SITE_URL_MAPPING.get(site_name, '')

        # ë°œí–‰ì¼
        published_date = get_published_date(week)

        # ì œëª© ìƒì„±
        if amount and amount != 'ë¹„ê³µê°œ' and amount != '-':
            article_title = f"{company_name} {stage} {amount} íˆ¬ì ìœ ì¹˜"
        else:
            article_title = f"{company_name} {stage} íˆ¬ì ìœ ì¹˜"

        print(f"[{idx}/{len(companies)}] {company_name}...", end=' ')

        try:
            # ì¤‘ë³µ í™•ì¸
            existing = supabase.table("investment_news_articles")\
                .select("id")\
                .eq("article_url", news_url)\
                .execute()

            if existing.data:
                print("âš ï¸ ì¤‘ë³µ")
                duplicate_count += 1
                continue

            # ì €ì¥
            supabase.table("investment_news_articles").insert({
                "site_number": site_number,
                "site_name": site_name,
                "site_url": site_url,
                "article_title": article_title,
                "article_url": news_url,
                "published_date": published_date,
                "content_snippet": f"{company_name} | {stage} | {amount}"
            }).execute()

            print(f"âœ… ì €ì¥ [{site_name}]")
            saved_count += 1

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {str(e)[:50]}")
            error_count += 1

    print(f"\n{'='*60}")
    print("ì €ì¥ ì™„ë£Œ")
    print(f"{'='*60}")
    print(f"âœ… ì €ì¥: {saved_count}ê°œ")
    print(f"âš ï¸ ì¤‘ë³µ: {duplicate_count}ê°œ")
    print(f"âŒ ì˜¤ë¥˜: {error_count}ê°œ")
    print(f"{'='*60}")

    # ì‚¬ì´íŠ¸ë³„ í†µê³„
    site_stats = {}
    for row in companies:
        site_name = row['ë‰´ìŠ¤ì†ŒìŠ¤']
        site_stats[site_name] = site_stats.get(site_name, 0) + 1

    print("\nğŸ“° ì‚¬ì´íŠ¸ë³„ í†µê³„:")
    for site, count in sorted(site_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {site}: {count}ê°œ")


if __name__ == '__main__':
    main()
