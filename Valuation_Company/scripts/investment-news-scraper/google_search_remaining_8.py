#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ë‚¨ì€ 8ê°œ ê¸°ì—… ë‰´ìŠ¤ ì°¾ê¸°
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time
from urllib.parse import quote

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# ë‚¨ì€ 8ê°œ ê¸°ì—…
remaining_companies = {
    "ì• í”Œì—ì´ì•„ì´": "ê´‘ë¦¼ë²¤ì²˜ìŠ¤",
    "ìŠ¤íŠœë””ì˜¤ì—í”¼ì†Œë“œ": "ì¼€ë¦¬ì†Œí”„íŠ¸",
    "ë¶€ìŠ¤í‹°ìŠ¤": "SBIì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸",
    "ë¹„ë°”íŠ¸ë¡œë¡œë³´í‹±ìŠ¤": "ì¹´ì´ìŠ¤íŠ¸í™€ë”©ìŠ¤",
    "ë±ì‚¬ìŠ¤íŠœë””ì˜¤": "NCì†Œí”„íŠ¸",
    "í•œì–‘ë¡œë³´í‹±ìŠ¤": "ë‚˜ìš°ë¡œë³´í‹±ìŠ¤",
    "ì†Œì…œë¦­ìŠ¤ì½”ë¦¬ì•„": "ë„¤ì´ë²„",
    "ìŠ¤ì¹´ì´ì¸í…”ë¦¬ì „ìŠ¤": "SKAIì›”ë“œì™€ì´ë“œ"
}


def google_search_via_duckduckgo(company_name, investor):
    """DuckDuckGo HTML ê²€ìƒ‰ìœ¼ë¡œ êµ¬ê¸€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""

    # ì—¬ëŸ¬ ê²€ìƒ‰ ì¿¼ë¦¬ ì‹œë„
    queries = [
        f"{company_name} {investor} íˆ¬ììœ ì¹˜",
        f"{company_name} {investor} íˆ¬ì",
        f"{company_name} íˆ¬ì ë‰´ìŠ¤",
        f"{company_name} ì‹œë¦¬ì¦ˆ íˆ¬ì",
        f"{company_name} í€ë”©",
    ]

    for query in queries:
        url = f"https://html.duckduckgo.com/html/?q={quote(query)}"

        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            # DuckDuckGo ê²€ìƒ‰ ê²°ê³¼
            results = soup.find_all('a', class_='result__a')

            for result in results[:10]:  # ìƒìœ„ 10ê°œ
                href = result.get('href', '')
                title = result.get_text().strip()

                # ê¸°ì—…ëª… í™•ì¸
                if company_name not in title:
                    continue

                # íˆ¬ì í‚¤ì›Œë“œ í™•ì¸
                if not any(kw in title for kw in ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ']):
                    continue

                # ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë§Œ
                news_domains = [
                    'venturesquare.net', 'wowtale.net', 'platum.kr',
                    'outstanding.kr', 'startuptoday.kr', 'thebell.co.kr',
                    'zdnet.co.kr', 'etnews.com', 'bloter.net',
                    'techcrunch.com', 'venturebeat.com'
                ]

                if any(domain in href for domain in news_domains):
                    # DuckDuckGo redirect URL ì²˜ë¦¬
                    if 'duckduckgo.com/l/' in href:
                        # ì‹¤ì œ URL ì¶”ì¶œ ì‹œë„
                        try:
                            redirect_response = requests.get(f"https:{href}", allow_redirects=True, timeout=5)
                            actual_url = redirect_response.url
                        except:
                            actual_url = href
                    else:
                        actual_url = href

                    return {
                        'title': title,
                        'url': actual_url,
                        'query': query
                    }

            time.sleep(1)

        except Exception as e:
            continue

    return None


def google_search_direct(company_name, investor):
    """êµ¬ê¸€ ì§ì ‘ ê²€ìƒ‰ (ê°„ë‹¨í•œ scraping)"""

    query = f"{company_name} {investor} íˆ¬ììœ ì¹˜ ë‰´ìŠ¤"
    url = f"https://www.google.com/search?q={quote(query)}&num=20"

    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ë§í¬
        links = soup.find_all('a')

        for link in links:
            href = link.get('href', '')
            text = link.get_text().strip()

            # /url?q= íŒ¨í„´
            if '/url?q=' in href:
                # ì‹¤ì œ URL ì¶”ì¶œ
                start = href.find('/url?q=') + 7
                end = href.find('&', start)
                if end == -1:
                    actual_url = href[start:]
                else:
                    actual_url = href[start:end]

                # ê¸°ì—…ëª… í™•ì¸
                if company_name not in text:
                    continue

                # íˆ¬ì í‚¤ì›Œë“œ
                if not any(kw in text for kw in ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ']):
                    continue

                # ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë§Œ
                news_domains = [
                    'venturesquare.net', 'wowtale.net', 'platum.kr',
                    'outstanding.kr', 'startuptoday.kr', 'thebell.co.kr',
                    'zdnet.co.kr', 'etnews.com', 'bloter.net'
                ]

                if any(domain in actual_url for domain in news_domains):
                    return {
                        'title': text,
                        'url': actual_url,
                        'query': query
                    }

    except Exception as e:
        pass

    return None


def main():
    print("=" * 80)
    print("êµ¬ê¸€ ê²€ìƒ‰ìœ¼ë¡œ ë‚¨ì€ 8ê°œ ê¸°ì—… ì¬íƒìƒ‰")
    print("=" * 80)

    found = 0
    duplicate = 0
    not_found = []

    for idx, (company, investor) in enumerate(remaining_companies.items(), 1):
        print(f"\n[{idx}/8] {company:25s} + {investor}")

        # DuckDuckGo ê²€ìƒ‰ ì‹œë„
        print(f"  ğŸ” DuckDuckGo ê²€ìƒ‰ ì¤‘...", end=' ')
        result = google_search_via_duckduckgo(company, investor)

        # DuckDuckGo ì‹¤íŒ¨ ì‹œ êµ¬ê¸€ ì§ì ‘ ê²€ìƒ‰
        if not result:
            print("ì‹¤íŒ¨")
            print(f"  ğŸ” êµ¬ê¸€ ì§ì ‘ ê²€ìƒ‰ ì¤‘...", end=' ')
            result = google_search_direct(company, investor)

        if result:
            print(f"\n  âœ… ë°œê²¬: {result['title'][:60]}...")
            print(f"  ğŸ”— {result['url']}")
            print(f"  ğŸ” ê²€ìƒ‰ì–´: {result['query']}")

            # ì‚¬ì´íŠ¸ëª… ì¶”ì¶œ
            site_mapping = {
                'venturesquare.net': ('ë²¤ì²˜ìŠ¤í€˜ì–´', 9),
                'wowtale.net': ('WOWTALE', 1),
                'platum.kr': ('í”Œë˜í…€', 10),
                'outstanding.kr': ('ì•„ì›ƒìŠ¤íƒ ë”©', 13),
                'startuptoday.kr': ('ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´', 11),
                'thebell.co.kr': ('ë”ë²¨', 16),
                'zdnet.co.kr': ('ì§€ë””ë„·', 99),
                'etnews.com': ('ì „ìì‹ ë¬¸', 99),
                'bloter.net': ('ë¸”ë¡œí„°', 22),
            }

            site_name = "êµ¬ê¸€ ê²€ìƒ‰"
            site_number = 99

            for domain, (name, num) in site_mapping.items():
                if domain in result['url']:
                    site_name = name
                    site_number = num
                    break

            # DB ì €ì¥
            article = {
                'site_number': site_number,
                'site_name': site_name,
                'site_url': "",
                'article_title': result['title'],
                'article_url': result['url'],
                'published_date': datetime.now().strftime('%Y-%m-%d')
            }

            # ì¤‘ë³µ í™•ì¸
            existing = supabase.table("investment_news_articles")\
                .select("id")\
                .eq("article_url", article['article_url'])\
                .execute()

            if not existing.data:
                try:
                    supabase.table("investment_news_articles").insert(article).execute()
                    print(f"  ğŸ’¾ DB ì €ì¥ ì™„ë£Œ")
                    found += 1
                except Exception as e:
                    print(f"  âŒ DB ì˜¤ë¥˜: {e}")
            else:
                print(f"  âš ï¸  ì¤‘ë³µ")
                duplicate += 1
        else:
            print("ì‹¤íŒ¨")
            print(f"  âŒ ëª» ì°¾ìŒ")
            not_found.append(company)

        time.sleep(2)  # ê²€ìƒ‰ ê°„ê²©

    print(f"\n{'='*80}")
    print("êµ¬ê¸€ ê²€ìƒ‰ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"âœ… ìƒˆë¡œ ë°œê²¬: {found}ê°œ")
    print(f"âš ï¸ ì¤‘ë³µ: {duplicate}ê°œ")
    print(f"âŒ ì—¬ì „íˆ ëª» ì°¾ìŒ: {len(not_found)}ê°œ")
    print(f"{'='*80}")

    # ìµœì¢… í†µê³„
    count_result = supabase.table("investment_news_articles").select("id", count="exact").execute()
    print(f"\ninvestment_news_articles í…Œì´ë¸” ì´ ë ˆì½”ë“œ: {count_result.count}ê°œ")

    if not_found:
        print(f"\nâŒ ìµœì¢… ë¯¸ë°œê²¬ ê¸°ì—… ({len(not_found)}ê°œ):")
        for company in not_found:
            print(f"  - {company}")


if __name__ == '__main__':
    main()
