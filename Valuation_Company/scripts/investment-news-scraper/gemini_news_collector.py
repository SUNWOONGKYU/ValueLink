#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Geminië¥¼ í™œìš©í•œ íˆ¬ì ë‰´ìŠ¤ ìˆ˜ì§‘
ëª°íŠ¸ë´‡ì´ ì‚¬ìš©í•œ ë°©ë²•ì„ ì¬í˜„

Gemini 1.5ì˜ ì›¹ ê²€ìƒ‰ ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬
ê¸°ì¡´ ë°©ë²•ì´ ë†“ì¹œ íˆ¬ì ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°œê²¬
"""

import os
import sys
import csv
import json
import time
from datetime import datetime
from dotenv import load_dotenv
import google.generativeai as genai
from supabase import create_client, Client

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

# API ì„¤ì •
genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

# Gemini ëª¨ë¸ ì„¤ì • (2.5 Flash - ìµœì‹ , ë¹ ë¥´ê³  íš¨ìœ¨ì , 1M tokens)
model = genai.GenerativeModel('gemini-2.5-flash')

print("=" * 80)
print("Gemini íˆ¬ì ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°")
print("=" * 80)

def search_investment_news_with_gemini(company_name, year=2026, month=1):
    """
    Geminië¥¼ í™œìš©í•˜ì—¬ íŠ¹ì • íšŒì‚¬ì˜ íˆ¬ì ë‰´ìŠ¤ ê²€ìƒ‰

    Args:
        company_name: íšŒì‚¬ëª…
        year: ì—°ë„ (ê¸°ë³¸: 2026)
        month: ì›” (ê¸°ë³¸: 1)

    Returns:
        list: ë°œê²¬í•œ ê¸°ì‚¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    prompt = f"""
ë‹¤ìŒ íšŒì‚¬ì˜ {year}ë…„ {month}ì›” íˆ¬ì ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì£¼ì„¸ìš”:

**íšŒì‚¬ëª…**: {company_name}

**ê²€ìƒ‰ ì¡°ê±´**:
- íˆ¬ì ìœ ì¹˜ ê´€ë ¨ ê¸°ì‚¬ë§Œ
- í•œêµ­ ì–¸ë¡ ì‚¬ ê¸°ì‚¬
- {year}ë…„ {month}ì›”ì— ë°œí–‰ëœ ê¸°ì‚¬
- ì‹œë¦¬ì¦ˆA, ì‹œë“œ, ë¸Œë¦¿ì§€ ë“± íˆ¬ì ë‹¨ê³„ ì–¸ê¸‰

**í•„ìš”í•œ ì •ë³´**:
ê° ê¸°ì‚¬ë§ˆë‹¤ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì œê³µ:
1. article_title: ê¸°ì‚¬ ì œëª©
2. article_url: ê¸°ì‚¬ URL
3. site_name: ì–¸ë¡ ì‚¬ëª…
4. published_date: ë°œí–‰ì¼ (YYYY-MM-DD)
5. summary: íˆ¬ì ê´€ë ¨ í•µì‹¬ ë‚´ìš© (íˆ¬ìì, ê¸ˆì•¡, ë‹¨ê³„)

**ì¶œë ¥ í˜•ì‹**:
```json
[
  {{
    "article_title": "ì œëª©",
    "article_url": "https://...",
    "site_name": "ì–¸ë¡ ì‚¬",
    "published_date": "2026-01-15",
    "summary": "íˆ¬ììëª…, íˆ¬ìê¸ˆì•¡, íˆ¬ìë‹¨ê³„ ë“±"
  }}
]
```

ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ë¹ˆ ë°°ì—´ []ì„ ë°˜í™˜í•´ì£¼ì„¸ìš”.
"""

    try:
        print(f"\nğŸ” {company_name} ê²€ìƒ‰ ì¤‘...")

        # Geminiì—ê²Œ ê²€ìƒ‰ ìš”ì²­ (ì›¹ ê²€ìƒ‰ í¬í•¨)
        response = model.generate_content(
            prompt,
            generation_config={
                'temperature': 0.1,  # ì •í™•ì„± ìš°ì„ 
                'top_p': 0.8,
                'top_k': 40,
                'max_output_tokens': 2048,
            }
        )

        # ì‘ë‹µ íŒŒì‹±
        text = response.text.strip()

        # JSON ì¶”ì¶œ (```json ... ``` ì œê±°)
        if "```json" in text:
            json_start = text.find("```json") + 7
            json_end = text.find("```", json_start)
            text = text[json_start:json_end].strip()
        elif "```" in text:
            json_start = text.find("```") + 3
            json_end = text.find("```", json_start)
            text = text[json_start:json_end].strip()

        articles = json.loads(text)

        if articles:
            print(f"  âœ… {len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            for idx, article in enumerate(articles, 1):
                print(f"     {idx}. {article.get('article_title', 'N/A')[:50]}...")
        else:
            print(f"  âš ï¸  ê¸°ì‚¬ ì—†ìŒ")

        return articles

    except json.JSONDecodeError as e:
        print(f"  âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        print(f"     ì‘ë‹µ: {text[:200]}...")
        return []
    except Exception as e:
        print(f"  âŒ ì˜¤ë¥˜: {e}")
        return []

def save_to_news_table(articles, company_name):
    """
    ë°œê²¬í•œ ê¸°ì‚¬ë¥¼ ë‰´ìŠ¤ í…Œì´ë¸”ì— ì €ì¥

    Args:
        articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        company_name: íšŒì‚¬ëª…

    Returns:
        int: ì €ì¥ëœ ê¸°ì‚¬ ìˆ˜
    """
    saved_count = 0

    for article in articles:
        # ì¤‘ë³µ í™•ì¸
        existing = supabase.table("investment_news_articles")\
            .select("id")\
            .eq("article_url", article['article_url'])\
            .execute()

        if existing.data:
            print(f"  âš ï¸  ì¤‘ë³µ: {article['article_title'][:40]}...")
            continue

        # ì €ì¥ ë°ì´í„° ì¤€ë¹„
        data = {
            'site_number': 999,  # Gemini ìˆ˜ì§‘ í‘œì‹œ
            'site_name': article.get('site_name', 'Unknown'),
            'site_url': '',
            'article_title': article['article_title'],
            'article_url': article['article_url'],
            'published_date': article.get('published_date', datetime.now().strftime('%Y-%m-%d')),
            'company_keywords': company_name,
            'gemini_summary': article.get('summary', '')
        }

        try:
            supabase.table("investment_news_articles").insert(data).execute()
            print(f"  âœ… ì €ì¥: {article['article_title'][:40]}...")
            saved_count += 1
        except Exception as e:
            print(f"  âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

    return saved_count

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # ì„¼ì„œë¸”ë°•ìŠ¤ ê¸°ì—… ëª©ë¡ ë¡œë“œ
    csv_path = "data/sensiblebox_companies_gemini_extracted.csv"

    if not os.path.exists(csv_path):
        print(f"\nâŒ íŒŒì¼ ì—†ìŒ: {csv_path}")
        return

    companies = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            company_name = row.get('íšŒì‚¬ëª…', '').strip()
            if company_name and company_name not in ['```', '```csv']:
                companies.append(company_name)

    print(f"\nğŸ“‹ ì´ {len(companies)}ê°œ ê¸°ì—… ê²€ìƒ‰")
    print("=" * 80)

    # í†µê³„
    total_found = 0
    total_saved = 0
    companies_with_news = []
    companies_without_news = []

    # ê° íšŒì‚¬ë³„ ê²€ìƒ‰
    for idx, company_name in enumerate(companies, 1):
        print(f"\n[{idx}/{len(companies)}] {company_name}")

        articles = search_investment_news_with_gemini(company_name)

        if articles:
            saved = save_to_news_table(articles, company_name)
            total_found += len(articles)
            total_saved += saved
            companies_with_news.append({
                'company': company_name,
                'found': len(articles),
                'saved': saved
            })
        else:
            companies_without_news.append(company_name)

        # API ì œí•œ ë°©ì§€ (GeminiëŠ” ë¶„ë‹¹ 60íšŒ)
        if idx < len(companies):
            time.sleep(1)

    # ìµœì¢… ê²°ê³¼
    print("\n" + "=" * 80)
    print("ìµœì¢… ê²°ê³¼")
    print("=" * 80)

    print(f"\nâœ… ê¸°ì‚¬ ë°œê²¬: {total_found}ê°œ")
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {total_saved}ê°œ")
    print(f"âœ… ë‰´ìŠ¤ ìˆëŠ” ê¸°ì—…: {len(companies_with_news)}ê°œ")
    print(f"âš ï¸  ë‰´ìŠ¤ ì—†ëŠ” ê¸°ì—…: {len(companies_without_news)}ê°œ")

    if companies_with_news:
        print(f"\nğŸ“Š ë‰´ìŠ¤ ë°œê²¬ ê¸°ì—… Top 10:")
        sorted_companies = sorted(companies_with_news, key=lambda x: x['found'], reverse=True)
        for idx, item in enumerate(sorted_companies[:10], 1):
            print(f"  {idx:2d}. {item['company']:20s} - {item['found']}ê°œ ë°œê²¬, {item['saved']}ê°œ ì €ì¥")

    if companies_without_news:
        print(f"\nâš ï¸  ë‰´ìŠ¤ ì—†ëŠ” ê¸°ì—… ({len(companies_without_news)}ê°œ):")
        for company in companies_without_news[:10]:
            print(f"  - {company}")
        if len(companies_without_news) > 10:
            print(f"  ... ì™¸ {len(companies_without_news) - 10}ê°œ")

    # ê²°ê³¼ ì €ì¥
    result_file = f"data/gemini_collection_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump({
            'total_companies': len(companies),
            'total_found': total_found,
            'total_saved': total_saved,
            'companies_with_news': companies_with_news,
            'companies_without_news': companies_without_news,
            'timestamp': datetime.now().isoformat()
        }, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {result_file}")

if __name__ == "__main__":
    main()
