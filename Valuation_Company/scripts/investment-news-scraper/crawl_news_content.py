#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§í•˜ì—¬ íˆ¬ìê¸ˆì•¡, íˆ¬ìë‹¨ê³„, íˆ¬ìì ì¶”ì¶œ
"""

import os
import sys
import re
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from supabase import create_client, Client

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

def extract_from_content(html_content):
    """ë³¸ë¬¸ì—ì„œ íˆ¬ìê¸ˆì•¡, íˆ¬ìë‹¨ê³„, íˆ¬ìì ì¶”ì¶œ"""

    soup = BeautifulSoup(html_content, 'html.parser')
    text = soup.get_text()

    # 1. íˆ¬ìê¸ˆì•¡ ì¶”ì¶œ
    amount = None
    amount_patterns = [
        r'(\d+(?:\.\d+)?)\s*ì¡°\s*ì›?',      # 1ì¡°ì›
        r'(\d+)\s*ì¡°',                        # 1ì¡°
        r'(\d+(?:\.\d+)?)\s*ì–µ\s*ì›?',      # 100ì–µì›
        r'(\d+)\s*ì–µ',                        # 100ì–µ
        r'\$\s*(\d+(?:\.\d+)?)\s*[Mm]',     # $10M
    ]

    for pattern in amount_patterns:
        matches = re.findall(pattern, text)
        if matches:
            value = float(matches[0])
            if 'ì¡°' in pattern:
                amount = value * 10000  # ì¡° -> ì–µ
            elif '$' in pattern or 'M' in pattern or 'm' in pattern:
                amount = value * 13  # M$ -> ì–µ
            else:
                amount = value
            break

    # 2. íˆ¬ìë‹¨ê³„ ì¶”ì¶œ
    stage = None
    stage_patterns = [
        ('ì‹œë¦¬ì¦ˆC', ['ì‹œë¦¬ì¦ˆC', 'Series C', 'ì‹œë¦¬ì¦ˆ C']),
        ('ì‹œë¦¬ì¦ˆB', ['ì‹œë¦¬ì¦ˆB', 'Series B', 'ì‹œë¦¬ì¦ˆ B']),
        ('ì‹œë¦¬ì¦ˆA', ['ì‹œë¦¬ì¦ˆA', 'Series A', 'ì‹œë¦¬ì¦ˆ A']),
        ('í”„ë¦¬A', ['í”„ë¦¬A', 'Pre-A', 'PreA', 'Pre A']),
        ('ì‹œë“œ', ['ì‹œë“œ', 'Seed', 'ì‹œë“œë¼ìš´ë“œ']),
        ('ë¸Œë¦¿ì§€', ['ë¸Œë¦¿ì§€', 'Bridge']),
    ]

    for stage_name, keywords in stage_patterns:
        for keyword in keywords:
            if keyword in text:
                stage = stage_name
                break
        if stage:
            break

    # 3. íˆ¬ìì ì¶”ì¶œ (ì£¼ìš” í‚¤ì›Œë“œ)
    investors = []
    investor_keywords = [
        'ë²¤ì²˜íˆ¬ì', 'ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸', 'íŒŒíŠ¸ë„ˆìŠ¤', 'ìºí”¼íƒˆ',
        'VC', 'Partners', 'Investment', 'Ventures'
    ]

    # ê°„ë‹¨í•œ íˆ¬ìì ì¶”ì¶œ (íšŒì‚¬ëª… + í‚¤ì›Œë“œ)
    for keyword in investor_keywords:
        pattern = r'([ê°€-í£A-Za-z]+(?:ë²¤ì²˜íˆ¬ì|ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸|íŒŒíŠ¸ë„ˆìŠ¤|ìºí”¼íƒˆ|ë²¤ì²˜ìŠ¤))'
        matches = re.findall(pattern, text)
        investors.extend(matches[:5])  # ìµœëŒ€ 5ê°œ

    investors_str = ', '.join(set(investors[:5])) if investors else None

    return amount, stage, investors_str

def crawl_news_content(url):
    """ë‰´ìŠ¤ í˜ì´ì§€ í¬ë¡¤ë§"""

    # DuckDuckGo ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬
    if 'duckduckgo.com' in url:
        match = re.search(r'uddg=([^&]+)', url)
        if match:
            import urllib.parse
            url = urllib.parse.unquote(match.group(1))

    # URLì´ //ë¡œ ì‹œì‘í•˜ë©´ https: ì¶”ê°€
    if url.startswith('//'):
        url = 'https:' + url

    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True)
        response.raise_for_status()
        response.encoding = response.apparent_encoding

        return extract_from_content(response.content)

    except Exception as e:
        return None, None, None

print("=" * 80)
print("ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§í•˜ì—¬ íˆ¬ìê¸ˆì•¡, íˆ¬ìë‹¨ê³„ ì¶”ì¶œ")
print("=" * 80)

# íˆ¬ìê¸ˆì•¡ ì—†ëŠ” Deal ì¡°íšŒ
deals = supabase.table("deals")\
    .select("*")\
    .is_("amount", "null")\
    .order("number")\
    .execute()

print(f"\nì²˜ë¦¬í•  Deal: {len(deals.data)}ê°œ")

amount_updated = 0
stage_updated = 0
investors_updated = 0

for idx, deal in enumerate(deals.data, 1):
    company = deal['company_name']
    url = deal.get('news_url')

    if not url:
        continue

    print(f"\n[{idx}/{len(deals.data)}] {deal['number']:3d}. {company}")
    print(f"  URL: {url[:70]}...")

    amount, stage, investors = crawl_news_content(url)

    updates = {}

    if amount:
        updates['amount'] = round(amount, 1)
        print(f"  âœ… ê¸ˆì•¡: {round(amount, 1)}ì–µì›")
        amount_updated += 1

    if stage and (not deal.get('stage') or deal.get('stage') in ['-', 'None']):
        updates['stage'] = stage
        print(f"  âœ… ë‹¨ê³„: {stage}")
        stage_updated += 1

    if investors and (not deal.get('investors') or deal.get('investors') == '-'):
        updates['investors'] = investors
        print(f"  âœ… íˆ¬ìì: {investors[:50]}...")
        investors_updated += 1

    if not updates:
        print(f"  âš ï¸  ì¶”ì¶œ ì‹¤íŒ¨")

    # ì—…ë°ì´íŠ¸ ì‹¤í–‰
    if updates:
        supabase.table("deals")\
            .update(updates)\
            .eq("id", deal['id'])\
            .execute()

print("\n" + "=" * 80)
print("ìµœì¢… ê²°ê³¼")
print("=" * 80)

print(f"\nâœ… íˆ¬ìê¸ˆì•¡ ì—…ë°ì´íŠ¸: {amount_updated}ê°œ")
print(f"âœ… íˆ¬ìë‹¨ê³„ ì—…ë°ì´íŠ¸: {stage_updated}ê°œ")
print(f"âœ… íˆ¬ìì ì—…ë°ì´íŠ¸: {investors_updated}ê°œ")

# ìµœì¢… í†µê³„
deals_final = supabase.table("deals").select("*").execute()

empty_amount = len([d for d in deals_final.data if not d.get('amount') or d.get('amount') == 0])
empty_stage = len([d for d in deals_final.data if not d.get('stage') or d.get('stage') in ['-', 'None']])

print(f"\nğŸ“Š ìµœì¢… í†µê³„:")
print(f"  íˆ¬ìê¸ˆì•¡ ì—†ìŒ: {empty_amount}ê°œ (ì´ì „: 73ê°œ)")
print(f"  íˆ¬ìë‹¨ê³„ ì—†ìŒ: {empty_stage}ê°œ (ì´ì „: 3ê°œ)")
