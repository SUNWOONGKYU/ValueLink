#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë§ˆì§€ë§‰ 2ê°œ ê¸°ì—… ê³µê²©ì  ê²€ìƒ‰
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time
from urllib.parse import quote

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET")

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# ë§ˆì§€ë§‰ 2ê°œ ê¸°ì—…
last_2_companies = {
    'ë””ì•¤í‹°í…Œí¬ì†”ë£¨ì…˜': {
        'íˆ¬ìì': 'ë¦¬ì¸ì¸ë² ìŠ¤íŠ¸ë¨¼íŠ¸',
        'íˆ¬ìì2': 'L&Së²¤ì²˜ìºí”¼íƒˆ',
        'íˆ¬ìì3': 'í‚¹ê³ íˆ¬ìíŒŒíŠ¸ë„ˆìŠ¤',
        'ì£¼ìš”ì‚¬ì—…': 'ì‚°ì—… ê³µì • ìë™í™” ì†”ë£¨ì…˜',
        'ë³€í˜•': ['ë””ì•¤í‹°í…Œí¬', 'ë””ì•¤í‹°', 'DNTí…Œí¬ì†”ë£¨ì…˜', 'DNT', 'D&Tí…Œí¬ì†”ë£¨ì…˜', 'D&T']
    },
    'ì—˜ë¦¬ì‹œì „': {
        'íˆ¬ìì': 'ë°ì¼ë¦¬íŒŒíŠ¸ë„ˆìŠ¤-NHíˆ¬ìì¦ê¶Œ',
        'íˆ¬ìì2': 'ë°ì¼ë¦¬íŒŒíŠ¸ë„ˆìŠ¤',
        'íˆ¬ìì3': 'NHíˆ¬ìì¦ê¶Œ',
        'ë‹¨ê³„': 'ì‹œë¦¬ì¦ˆC',
        'ê¸ˆì•¡': '50ì–µ',
        'ì£¼ìš”ì‚¬ì—…': 'ìœ ì „ì ì¹˜ë£Œì œ',
        'ë³€í˜•': ['ì—˜ë¦¬ì‹œì „', 'Ellision', 'ellision', 'ì—˜ë¦¬ì…˜', 'Elision']
    }
}

def search_naver_aggressive(company_name, company_info):
    """ë„¤ì´ë²„ API - ê³µê²©ì  ê²€ìƒ‰"""

    url = "https://openapi.naver.com/v1/search/news.json"

    headers = {
        'X-Naver-Client-Id': NAVER_CLIENT_ID,
        'X-Naver-Client-Secret': NAVER_CLIENT_SECRET
    }

    # ë‹¤ì–‘í•œ ê²€ìƒ‰ ì¿¼ë¦¬
    all_variants = [company_name] + company_info.get('ë³€í˜•', [])
    investors = [company_info.get('íˆ¬ìì', ''), company_info.get('íˆ¬ìì2', ''), company_info.get('íˆ¬ìì3', '')]

    queries = []

    # ê¸°ì—…ëª… + íˆ¬ìì
    for variant in all_variants:
        for investor in investors:
            if investor:
                queries.append(f"{variant} {investor}")
                queries.append(f"{variant} {investor} íˆ¬ì")

    # ê¸°ì—…ëª… + í‚¤ì›Œë“œ
    for variant in all_variants:
        queries.extend([
            f"{variant} íˆ¬ììœ ì¹˜",
            f"{variant} íˆ¬ì",
            f"{variant} í€ë”©",
            f"{variant} ì‹œë¦¬ì¦ˆ",
            variant
        ])

    # íˆ¬ìì + ì£¼ìš”ì‚¬ì—…
    business = company_info.get('ì£¼ìš”ì‚¬ì—…', '')
    if business:
        for investor in investors:
            if investor:
                queries.append(f"{investor} {business}")

    print(f"\n  ğŸ” {len(queries)}ê°œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì¤‘...")

    for query in queries[:50]:  # ìµœëŒ€ 50ê°œ ì¿¼ë¦¬
        params = {
            'query': query,
            'display': 100,
            'sort': 'date'
        }

        try:
            response = requests.get(url, headers=headers, params=params, timeout=10)

            if response.status_code == 200:
                items = response.json().get('items', [])

                for item in items:
                    title = item.get('title', '').replace('<b>', '').replace('</b>', '')
                    link = item.get('originallink') or item.get('link')
                    pub_date = item.get('pubDate', '')

                    # ê¸°ì—…ëª… ë³€í˜• ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨
                    found = False
                    for variant in all_variants:
                        if variant in title:
                            found = True
                            break

                    if not found:
                        continue

                    # íˆ¬ì í‚¤ì›Œë“œ í™•ì¸
                    investment_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ', 'VC', 'ìºí”¼íƒˆ']
                    if not any(kw in title for kw in investment_keywords):
                        continue

                    # ë‚ ì§œ íŒŒì‹±
                    try:
                        dt = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %z')
                        published_date = dt.strftime('%Y-%m-%d')
                    except:
                        published_date = datetime.now().strftime('%Y-%m-%d')

                    # ì‚¬ì´íŠ¸ëª…
                    site_mapping = {
                        'wowtale.net': ('WOWTALE', 1),
                        'venturesquare.net': ('ë²¤ì²˜ìŠ¤í€˜ì–´', 9),
                        'thebell.co.kr': ('ë”ë²¨', 16),
                        'platum.kr': ('í”Œë˜í…€', 10),
                        'startuptoday.kr': ('ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´', 11),
                    }

                    site_name = "ë„¤ì´ë²„ ë‰´ìŠ¤"
                    site_number = 99

                    for domain, (name, num) in site_mapping.items():
                        if domain in link:
                            site_name = name
                            site_number = num
                            break

                    return {
                        'site_number': site_number,
                        'site_name': site_name,
                        'site_url': '',
                        'article_title': title,
                        'article_url': link,
                        'published_date': published_date
                    }, query

            time.sleep(0.1)

        except Exception as e:
            continue

    return None, None


def search_google_duckduckgo(company_name, company_info):
    """DuckDuckGoë¡œ êµ¬ê¸€ ê²€ìƒ‰"""

    all_variants = [company_name] + company_info.get('ë³€í˜•', [])
    investors = [company_info.get('íˆ¬ìì', ''), company_info.get('íˆ¬ìì2', ''), company_info.get('íˆ¬ìì3', '')]

    queries = []
    for variant in all_variants[:3]:
        for investor in investors:
            if investor:
                queries.append(f"{variant} {investor} íˆ¬ì")

    for query in queries[:10]:
        url = f"https://html.duckduckgo.com/html/?q={quote(query)}"

        try:
            response = requests.get(url, headers=HEADERS, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')

            results = soup.find_all('a', class_='result__a')

            for result in results[:20]:
                href = result.get('href', '')
                text = result.get_text().strip()

                # ê¸°ì—…ëª… í™•ì¸
                found = False
                for variant in all_variants:
                    if variant in text:
                        found = True
                        break

                if not found:
                    continue

                # íˆ¬ì í‚¤ì›Œë“œ
                if not any(kw in text for kw in ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ']):
                    continue

                # ë‰´ìŠ¤ ì‚¬ì´íŠ¸
                news_domains = [
                    'wowtale.net', 'venturesquare.net', 'thebell.co.kr',
                    'platum.kr', 'startuptoday.kr', 'etnews.com',
                    'zdnet.co.kr', 'bloter.net', 'moneys.co.kr',
                    'etoday.co.kr', 'newstomato.com'
                ]

                if any(domain in href for domain in news_domains):
                    return {
                        'title': text,
                        'url': href,
                        'query': query
                    }

            time.sleep(1)

        except Exception as e:
            continue

    return None


def main():
    print("=" * 80)
    print("ë§ˆì§€ë§‰ 2ê°œ ê¸°ì—… ê³µê²©ì  ê²€ìƒ‰")
    print("=" * 80)

    found = 0
    not_found = []

    for idx, (company, info) in enumerate(last_2_companies.items(), 1):
        print(f"\n[{idx}/2] {company}")
        print(f"  íˆ¬ìì: {info['íˆ¬ìì']}")
        print(f"  ì£¼ìš”ì‚¬ì—…: {info['ì£¼ìš”ì‚¬ì—…']}")
        print(f"  ê²€ìƒ‰ ë³€í˜•: {', '.join(info['ë³€í˜•'][:3])}...")

        # ë„¤ì´ë²„ ê²€ìƒ‰
        print("\n  ğŸ” ë„¤ì´ë²„ API ê²€ìƒ‰...")
        article, query = search_naver_aggressive(company, info)

        if not article:
            print("  âŒ ë„¤ì´ë²„ì—ì„œ ëª» ì°¾ìŒ")
            print("\n  ğŸ” DuckDuckGo ê²€ìƒ‰...")
            result = search_google_duckduckgo(company, info)

            if result:
                print(f"  âœ… ë°œê²¬: {result['title'][:60]}...")
                print(f"  ğŸ”— {result['url']}")

                # ì‚¬ì´íŠ¸ëª… ì¶”ì¶œ
                site_mapping = {
                    'wowtale.net': ('WOWTALE', 1),
                    'venturesquare.net': ('ë²¤ì²˜ìŠ¤í€˜ì–´', 9),
                    'thebell.co.kr': ('ë”ë²¨', 16),
                    'platum.kr': ('í”Œë˜í…€', 10),
                    'startuptoday.kr': ('ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´', 11),
                    'etnews.com': ('ì „ìì‹ ë¬¸', 99),
                    'zdnet.co.kr': ('ì§€ë””ë„·', 99),
                    'bloter.net': ('ë¸”ë¡œí„°', 22),
                    'moneys.co.kr': ('ë¨¸ë‹ˆS', 99),
                    'etoday.co.kr': ('ì´íˆ¬ë°ì´', 99),
                }

                site_name = "ê¸°íƒ€ ë‰´ìŠ¤"
                site_number = 99

                for domain, (name, num) in site_mapping.items():
                    if domain in result['url']:
                        site_name = name
                        site_number = num
                        break

                article = {
                    'site_number': site_number,
                    'site_name': site_name,
                    'site_url': '',
                    'article_title': result['title'],
                    'article_url': result['url'],
                    'published_date': datetime.now().strftime('%Y-%m-%d')
                }

        if article:
            print(f"  âœ… ë°œê²¬: {article['article_title'][:60]}...")
            print(f"  ğŸ” ê²€ìƒ‰ì–´: {query if query else 'êµ¬ê¸€ ê²€ìƒ‰'}")
            print(f"  ğŸ“° [{article['site_name']}]")

            # ì¤‘ë³µ í™•ì¸
            existing = supabase.table("investment_news_articles")\
                .select("id")\
                .eq("article_url", article['article_url'])\
                .execute()

            if not existing.data:
                try:
                    supabase.table("investment_news_articles").insert(article).execute()
                    print(f"  ğŸ’¾ DB ì €ì¥ ì™„ë£Œ")
                    found += 1
                except Exception as e:
                    print(f"  âŒ DB ì˜¤ë¥˜: {e}")
            else:
                print(f"  âš ï¸  ì¤‘ë³µ")
                found += 1
        else:
            print(f"  âŒ ìµœì¢… ë¯¸ë°œê²¬")
            not_found.append(company)

        time.sleep(1)

    print(f"\n{'='*80}")
    print("ê²€ìƒ‰ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"âœ… ë°œê²¬: {found}ê°œ")
    print(f"âŒ ìµœì¢… ë¯¸ë°œê²¬: {len(not_found)}ê°œ")

    if not_found:
        print(f"\nâŒ ìµœì¢… ë¯¸ë°œê²¬ ê¸°ì—…:")
        for company in not_found:
            print(f"  - {company}")
    else:
        print(f"\nğŸ‰ ëª¨ë“  ê¸°ì—… ë°œê²¬ ì™„ë£Œ!")

    # ìµœì¢… í†µê³„
    count_result = supabase.table("investment_news_articles").select("id", count="exact").execute()
    print(f"\ninvestment_news_articles í…Œì´ë¸” ì´ ë ˆì½”ë“œ: {count_result.count}ê°œ")


if __name__ == '__main__':
    main()
