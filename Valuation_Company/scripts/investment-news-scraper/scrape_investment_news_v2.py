#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íˆ¬ì ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸ v2
ì‘ì„±ì¼: 2026-01-26
ì‘ì„±ì: Claude Code
ìš©ë„: êµ­ë‚´ íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ë° Supabase ì €ì¥ (REST API ë°©ì‹)
"""

import os
import time
import logging
from datetime import datetime, date
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraping_log.txt', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# ================================================================
# ì„¤ì •
# ================================================================

# Supabase ì—°ê²°
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError("í™˜ê²½ë³€ìˆ˜ SUPABASE_URLê³¼ SUPABASE_KEYë¥¼ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")

# ëŒ€ìƒ ì‚¬ì´íŠ¸ ëª©ë¡
SITES = [
    {
        'number': 9,
        'name': 'ë²¤ì²˜ìŠ¤í€˜ì–´',
        'url': 'https://www.venturesquare.net/category/news-contents/news-trends/news/',
        'max_pages': 10  # ìµœê·¼ 10í˜ì´ì§€ ìˆ˜ì§‘ (ì•½ 80ê±´)
    },
    # ë‹¤ë¥¸ ì‚¬ì´íŠ¸ë“¤ì€ ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€ (Selenium ë˜ëŠ” API í•„ìš”)
]

# ê²€ìƒ‰ í‚¤ì›Œë“œ (íˆ¬ììœ ì¹˜ ê´€ë ¨)
KEYWORDS = ['íˆ¬ì', 'íˆ¬ììœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'ë²¤ì²˜ìºí”¼í„¸', 'VC', 'ì—”ì ¤íˆ¬ì', 'í”„ë¦¬ì‹œë¦¬ì¦ˆ', 'ë¸Œë¦¿ì§€', 'M&A', 'ì¸ìˆ˜']

# ê¸°ê°„ ì„¤ì •
START_DATE = date(2026, 1, 1)
END_DATE = date.today()

# ìš”ì²­ ì„¤ì •
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}

REQUEST_DELAY = 2  # ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)


# ================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ================================================================

def contains_keyword(text: str) -> bool:
    """í…ìŠ¤íŠ¸ì— íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    if not text:
        return False
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in KEYWORDS)


def parse_date(date_str: str) -> Optional[date]:
    """ë‚ ì§œ ë¬¸ìì—´ì„ date ê°ì²´ë¡œ ë³€í™˜ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)"""
    date_formats = [
        '%Y-%m-%d',
        '%Y.%m.%d',
        '%Y/%m/%d',
        '%Yë…„ %mì›” %dì¼',
    ]

    for fmt in date_formats:
        try:
            return datetime.strptime(date_str.strip(), fmt).date()
        except ValueError:
            continue

    logger.warning(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}")
    return None


def is_valid_date(article_date: date) -> bool:
    """ê¸°ì‚¬ ë‚ ì§œê°€ ìˆ˜ì§‘ ê¸°ê°„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
    return START_DATE <= article_date <= END_DATE


# ================================================================
# ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
# ================================================================

def scrape_venturesquare(site_info: Dict) -> List[Dict]:
    """
    ë²¤ì²˜ìŠ¤í€˜ì–´ ìŠ¤í¬ë˜í•‘
    URL íŒ¨í„´: https://www.venturesquare.net/category/news-contents/news-trends/news/page/{N}/
    """
    articles = []
    site_number = site_info['number']
    site_name = site_info['name']
    base_url = site_info['url']
    max_pages = site_info.get('max_pages', 3)

    logger.info(f"ğŸ” [{site_name}] ìŠ¤í¬ë˜í•‘ ì‹œì‘ (ìµœëŒ€ {max_pages}í˜ì´ì§€)")

    for page in range(1, max_pages + 1):
        if page == 1:
            url = base_url
        else:
            url = f"{base_url}page/{page}/"

        try:
            logger.info(f"  ğŸ“„ í˜ì´ì§€ {page} ìš”ì²­: {url}")
            response = requests.get(url, headers=HEADERS, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'lxml')

            # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ: h4.bold ì•ˆì˜ a íƒœê·¸
            article_elements = soup.select('h4.bold a.black')
            logger.info(f"  âœ… {len(article_elements)}ê°œ ê¸°ì‚¬ ë°œê²¬")

            for elem in article_elements:
                try:
                    title = elem.get_text(strip=True)

                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if not contains_keyword(title):
                        continue

                    # URL
                    article_url = elem.get('href', '')
                    if not article_url.startswith('http'):
                        article_url = 'https://www.venturesquare.net' + article_url

                    # ë‚ ì§œ: ë™ì¼í•œ li íƒœê·¸ ë‚´ì˜ time ìš”ì†Œ
                    li_parent = elem.find_parent('li')
                    if not li_parent:
                        continue

                    date_elem = li_parent.select_one('time[datetime]')
                    if not date_elem:
                        continue

                    date_text = date_elem.get('datetime', '').split('T')[0]
                    published_date = parse_date(date_text)

                    if not published_date or not is_valid_date(published_date):
                        continue

                    # ê¸°ì‚¬ ë°ì´í„° ì €ì¥
                    articles.append({
                        'site_number': site_number,
                        'site_name': site_name,
                        'site_url': 'https://www.venturesquare.net',
                        'article_title': title,
                        'article_url': article_url,
                        'published_date': published_date.isoformat(),
                        'content_snippet': None,
                    })

                    logger.info(f"  âœ… ìˆ˜ì§‘: {title[:50]}... ({published_date})")

                except Exception as e:
                    logger.error(f"  âŒ ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue

            # í˜ì´ì§€ ê°„ ëŒ€ê¸°
            if page < max_pages:
                time.sleep(1)

        except requests.RequestException as e:
            logger.error(f"  âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
            break
        except Exception as e:
            logger.error(f"  âŒ í˜ì´ì§€ {page} ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            break

    logger.info(f"âœ… [{site_name}] ì´ {len(articles)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    return articles


# ì‚¬ì´íŠ¸ë³„ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ ë§¤í•‘
SITE_SCRAPERS = {
    9: scrape_venturesquare,
    # ë‹¤ë¥¸ ì‚¬ì´íŠ¸ëŠ” ìˆœì°¨ì ìœ¼ë¡œ ì¶”ê°€
}


def scrape_site(site: Dict) -> List[Dict]:
    """ì‚¬ì´íŠ¸ë³„ ìŠ¤í¬ë˜í•‘ ë””ìŠ¤íŒ¨ì²˜"""
    site_number = site['number']
    scraper_func = SITE_SCRAPERS.get(site_number)

    if not scraper_func:
        logger.warning(f"âš ï¸  {site['name']} (#{site_number}) ìŠ¤í¬ë˜í¼ ë¯¸êµ¬í˜„")
        return []

    return scraper_func(site)


# ================================================================
# Supabase ì €ì¥
# ================================================================

def save_to_supabase(articles: List[Dict]) -> int:
    """
    ìˆ˜ì§‘ëœ ê¸°ì‚¬ë¥¼ Supabaseì— ì €ì¥ (REST API ì§ì ‘ í˜¸ì¶œ)
    """
    if not articles:
        return 0

    saved_count = 0
    failed_count = 0

    # REST API ì—”ë“œí¬ì¸íŠ¸
    api_url = f"{SUPABASE_URL}/rest/v1/investment_news_articles"
    headers = {
        'apikey': SUPABASE_KEY,
        'Authorization': f'Bearer {SUPABASE_KEY}',
        'Content-Type': 'application/json',
        'Prefer': 'return=representation'
    }

    # ê°œë³„ ì €ì¥ (ì¤‘ë³µ ì²˜ë¦¬ë¥¼ ìœ„í•´)
    for article in articles:
        try:
            response = requests.post(api_url, json=article, headers=headers, timeout=30)

            if response.status_code == 201:
                saved_count += 1
                logger.info(f"  ğŸ’¾ ì €ì¥ ì„±ê³µ: {article['article_title'][:50]}...")
            elif response.status_code == 409:
                # ì¤‘ë³µ URL
                logger.info(f"  âš ï¸  ì¤‘ë³µ URL ìŠ¤í‚µ: {article['article_title'][:50]}...")
            else:
                failed_count += 1
                logger.error(f"  âŒ ì €ì¥ ì‹¤íŒ¨ (HTTP {response.status_code}): {article['article_title'][:50]}...")
                logger.error(f"     ì‘ë‹µ: {response.text[:200]}")

        except requests.RequestException as e:
            failed_count += 1
            logger.error(f"  âŒ ì €ì¥ ìš”ì²­ ì‹¤íŒ¨: {e}")
        except Exception as e:
            failed_count += 1
            logger.error(f"  âŒ ì €ì¥ ì˜¤ë¥˜: {e}")

    logger.info(f"ğŸ’¾ Supabase ì €ì¥ ì™„ë£Œ: ì„±ê³µ {saved_count}ê±´ / ì‹¤íŒ¨ {failed_count}ê±´")
    return saved_count


# ================================================================
# ë©”ì¸ í•¨ìˆ˜
# ================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=" * 60)
    logger.info("ğŸ“° íˆ¬ì ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    logger.info(f"ğŸ“… ê¸°ê°„: {START_DATE} ~ {END_DATE}")
    logger.info(f"ğŸŒ ëŒ€ìƒ ì‚¬ì´íŠ¸: {len(SITES)}ê°œ")
    logger.info("=" * 60)

    start_time = time.time()
    total_articles = []

    # ì‚¬ì´íŠ¸ë³„ ìŠ¤í¬ë˜í•‘
    for idx, site in enumerate(SITES, 1):
        logger.info(f"\n[{idx}/{len(SITES)}] {site['name']} ì²˜ë¦¬ ì¤‘...")

        articles = scrape_site(site)
        total_articles.extend(articles)

        # ìš”ì²­ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        if idx < len(SITES):
            logger.info(f"â³ {REQUEST_DELAY}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(REQUEST_DELAY)

    # Supabase ì €ì¥
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ’¾ Supabase ì €ì¥ ì‹œì‘...")
    saved_count = save_to_supabase(total_articles)

    # ê²°ê³¼ ìš”ì•½
    elapsed_time = time.time() - start_time
    logger.info("\n" + "=" * 60)
    logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê±´ìˆ˜: {len(total_articles)}ê±´")
    logger.info(f"ğŸ’¾ ì €ì¥ ê±´ìˆ˜: {saved_count}ê±´")
    logger.info(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    logger.info("=" * 60)

    # ë­í‚¹ ì—…ë°ì´íŠ¸ ì•ˆë‚´
    if saved_count > 0:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Œ ë‹¤ìŒ ë‹¨ê³„:")
        logger.info("1. Supabaseì—ì„œ ë‹¤ìŒ SQL ì‹¤í–‰:")
        logger.info("   SELECT update_news_ranking();")
        logger.info("2. ë­í‚¹ í™•ì¸:")
        logger.info("   SELECT * FROM v_latest_ranking;")
        logger.info("=" * 60)


# ================================================================
# ì‹¤í–‰
# ================================================================

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)
