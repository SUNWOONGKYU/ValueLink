#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
STEP 1-1: 5ê°œ ì–¸ë¡ ì‚¬ ì§ì ‘ ìŠ¤í¬ë˜í•‘
- WOWTALE
- ë²¤ì²˜ìŠ¤í€˜ì–´
- ì•„ì›ƒìŠ¤íƒ ë”©
- ë”VC
- ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´

â†’ investment_news_articles í…Œì´ë¸”ì— ì €ì¥
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re
from dotenv import load_dotenv
from supabase import create_client, Client

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

# 5ê°œ ì–¸ë¡ ì‚¬ ì„¤ì • (ë”VC ì œì™¸, í”Œë˜í…€ í¬í•¨)
SITES = [
    {
        'number': 1,
        'name': 'WOWTALE',
        'url': 'https://www.wowtale.net',
        'search_url': 'https://www.wowtale.net/?s=íˆ¬ììœ ì¹˜',
        'selectors': {
            'article': 'article.post, div.post',
            'title': 'h2.entry-title a, h3.entry-title a',
            'link': 'h2.entry-title a, h3.entry-title a',
            'date': 'time.entry-date, .published'
        }
    },
    {
        'number': 9,
        'name': 'ë²¤ì²˜ìŠ¤í€˜ì–´',
        'url': 'https://www.venturesquare.net',
        'search_url': 'https://www.venturesquare.net/category/news-contents/news-trends/news/',
        'selectors': {
            'article': 'li',
            'title': 'h4.bold a.black',
            'link': 'h4.bold a.black',
            'date': 'time'
        }
    },
    {
        'number': 13,
        'name': 'ì•„ì›ƒìŠ¤íƒ ë”©',
        'url': 'https://outstanding.kr',
        'search_url': 'https://outstanding.kr/?s=íˆ¬ììœ ì¹˜',
        'selectors': {
            'article': 'article.post, div.post-item',
            'title': 'h2 a, h3 a',
            'link': 'h2 a, h3 a',
            'date': 'time, .date'
        }
    },
    {
        'number': 10,
        'name': 'í”Œë˜í…€',
        'url': 'https://platum.kr',
        'search_url': 'https://platum.kr/category/investment',
        'selectors': {
            'article': 'article.archive-post, div.post_content',
            'title': 'h2.entry-title a, .title a',
            'link': 'h2.entry-title a, .title a',
            'date': 'time.entry-date, .date'
        }
    },
    {
        'number': 11,
        'name': 'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´',
        'url': 'https://www.startuptoday.kr',
        'search_url': 'https://www.startuptoday.kr/news/articleList.html?sc_area=A&view_type=sm&sc_word=íˆ¬ììœ ì¹˜',
        'selectors': {
            'article': 'div.list-block, article',
            'title': 'div.list-titles a, h4.titles a',
            'link': 'div.list-titles a, h4.titles a',
            'date': '.list-dated, time'
        }
    }
]

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}

INVESTMENT_KEYWORDS = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ', 'VC', 'ë²¤ì²˜ìºí”¼í„¸']


def parse_date(date_str):
    """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
    # ê¸°ë³¸ê°’: ì˜¤ëŠ˜ ë‚ ì§œ
    default_date = datetime.now().strftime('%Y-%m-%d')

    if not date_str:
        return default_date

    # ìˆ«ìë§Œ ì¶”ì¶œ
    numbers = re.findall(r'\d+', date_str)
    if len(numbers) >= 3:
        try:
            y, m, d = map(int, numbers[:3])
            if y < 100:
                y += 2000
            return f"{y:04d}-{m:02d}-{d:02d}"
        except:
            return default_date

    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
    return default_date


def scrape_site(site_info):
    """ì‚¬ì´íŠ¸ë³„ ìŠ¤í¬ë˜í•‘"""
    articles = []

    print(f"\n{'='*60}")
    print(f"ğŸ“° {site_info['name']} ìŠ¤í¬ë˜í•‘ ì¤‘...")
    print(f"{'='*60}")

    try:
        response = requests.get(site_info['search_url'], headers=HEADERS, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.content, 'html.parser')

        # ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ
        items = soup.select(site_info['selectors']['article'])

        print(f"ë°œê²¬ëœ í•­ëª©: {len(items)}ê°œ")

        for idx, item in enumerate(items[:30], 1):  # ìµœëŒ€ 30ê°œ
            try:
                # ì œëª©ê³¼ URL ì¶”ì¶œ (ê°™ì€ ì…€ë ‰í„°)
                link_elem = item.select_one(site_info['selectors']['link'])
                if not link_elem:
                    # ëŒ€ì²´ ì…€ë ‰í„° ì‹œë„
                    link_elem = item.find('a')
                    if not link_elem:
                        continue

                title = link_elem.get_text(strip=True)
                url = link_elem.get('href', '')

                # ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if len(title) < 10:
                    continue

                # ê³µì§€ì‚¬í•­ ì œì™¸
                if '[ê³µì§€]' in title or 'ê³µì§€' in title[:5]:
                    continue

                # íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸ (ì™„í™”: ìˆìœ¼ë©´ ìš°ì„ , ì—†ì–´ë„ ì¼ë¶€ í¬í•¨)
                has_keyword = any(keyword in title for keyword in INVESTMENT_KEYWORDS)

                # íˆ¬ì ì„¹ì…˜ì—ì„œ ê°€ì ¸ì˜¤ë¯€ë¡œ í‚¤ì›Œë“œ ì—†ì–´ë„ ì¼ë¶€ í¬í•¨
                if not has_keyword and idx > 10:  # ìƒìœ„ 10ê°œëŠ” í‚¤ì›Œë“œ ì—†ì–´ë„ OK
                    continue

                # ìƒëŒ€ ê²½ë¡œ â†’ ì ˆëŒ€ ê²½ë¡œ
                if url.startswith('/'):
                    url = site_info['url'] + url
                elif not url.startswith('http'):
                    url = site_info['url'] + '/' + url

                # ë‚ ì§œ ì¶”ì¶œ
                date_elem = item.select_one(site_info['selectors']['date'])
                date_str = None
                if date_elem:
                    date_str = date_elem.get('datetime') or date_elem.get_text(strip=True)

                published_date = parse_date(date_str)

                articles.append({
                    'site_number': site_info['number'],
                    'site_name': site_info['name'],
                    'site_url': site_info['url'],
                    'article_title': title,
                    'article_url': url,
                    'published_date': published_date,
                    'content_snippet': None
                })

                print(f"  [{idx}] {title[:50]}...")

            except Exception as e:
                continue

        print(f"âœ… {site_info['name']}: {len(articles)}ê°œ ìˆ˜ì§‘")

    except Exception as e:
        print(f"âŒ {site_info['name']} ì˜¤ë¥˜: {str(e)[:100]}")

    return articles


def save_to_db(articles):
    """DBì— ì €ì¥"""
    print(f"\n{'='*60}")
    print(f"ğŸ’¾ DB ì €ì¥ ì¤‘...")
    print(f"{'='*60}")

    saved_count = 0
    duplicate_count = 0

    for article in articles:
        try:
            # ì¤‘ë³µ í™•ì¸
            existing = supabase.table("investment_news_articles")\
                .select("id")\
                .eq("article_url", article['article_url'])\
                .execute()

            if existing.data:
                duplicate_count += 1
                continue

            # ì €ì¥
            supabase.table("investment_news_articles").insert({
                "site_number": article['site_number'],
                "site_name": article['site_name'],
                "site_url": article['site_url'],
                "article_title": article['article_title'],
                "article_url": article['article_url'],
                "published_date": article['published_date'],
                "content_snippet": article['content_snippet']
            }).execute()

            saved_count += 1

        except Exception as e:
            print(f"  âŒ ì €ì¥ ì‹¤íŒ¨: {article['article_title'][:30]}... - {str(e)[:50]}")

    print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")
    print(f"âš ï¸ ì¤‘ë³µ ê±´ë„ˆëœ€: {duplicate_count}ê°œ")


def main():
    print("=" * 60)
    print("STEP 1-1: 5ê°œ ì–¸ë¡ ì‚¬ ì§ì ‘ ìŠ¤í¬ë˜í•‘")
    print("=" * 60)

    all_articles = []

    for site in SITES:
        articles = scrape_site(site)
        all_articles.extend(articles)

    print(f"\n{'='*60}")
    print(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(all_articles)}ê°œ")
    print(f"{'='*60}")

    # ì‚¬ì´íŠ¸ë³„ í†µê³„
    site_stats = {}
    for article in all_articles:
        site_name = article['site_name']
        site_stats[site_name] = site_stats.get(site_name, 0) + 1

    for site_name, count in site_stats.items():
        print(f"  - {site_name}: {count}ê°œ")

    # DB ì €ì¥
    if all_articles:
        save_to_db(all_articles)

    print(f"\n{'='*60}")
    print("STEP 1-1 ì™„ë£Œ!")
    print(f"{'='*60}")


if __name__ == '__main__':
    main()
