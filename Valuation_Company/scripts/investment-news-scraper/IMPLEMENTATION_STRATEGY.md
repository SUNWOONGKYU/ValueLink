# 10ê°œ ì‚¬ì´íŠ¸ ë‰´ìŠ¤ ìˆ˜ì§‘ êµ¬í˜„ ì „ëµ

> ì‹¤ì œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ êµ¬í˜„ ë°©ë²•

---

## ì—°êµ¬ ê²°ê³¼ ìš”ì•½

### âœ… ìˆ˜ì§‘ ê°€ëŠ¥: 8ê°œ ì‚¬ì´íŠ¸

| # | ì‚¬ì´íŠ¸ | ë°©ë²• | ìƒíƒœ | ìƒì„¸ |
|---|--------|------|------|------|
| 1 | **ë²¤ì²˜ìŠ¤í€˜ì–´** | RSS | âœ… | 30ê°œ ê¸°ì‚¬, ìµœì‹  ì—…ë°ì´íŠ¸ |
| 2 | **ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´** | ì›¹ ìŠ¤í¬ë˜í•‘ | âœ… | selector: `article`, 8ê°œ ìš”ì†Œ |
| 3 | **ì•„ì›ƒìŠ¤íƒ ë”©** | RSS | âœ… | 10ê°œ ê¸°ì‚¬ |
| 4 | ë”ë²¨ | - | âŒ | ìœ ë£Œ ì‚¬ì´íŠ¸, ì„ íƒì ì—†ìŒ |
| 5 | ë”ë¸Œì´ì”¨ | - | âŒ | íˆ¬ìì‚¬ ëª©ë¡, ì„ íƒì ì—†ìŒ |
| 6 | **ìŠ¤íƒ€íŠ¸ì—…ì—”** | ì›¹ ìŠ¤í¬ë˜í•‘ | âœ… | selector: `article`, 15ê°œ ìš”ì†Œ |
| 7 | **ë¸”ë¡œí„°** | ì›¹ ìŠ¤í¬ë˜í•‘ | âœ… | selector: `article`, 23ê°œ ìš”ì†Œ (RSS í”¼ë“œ ë¹„ì–´ìˆìŒ) |
| 8 | **ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸** | ì›¹ ìŠ¤í¬ë˜í•‘ | âœ… | selector: `h2 a`, 2ê°œ ìš”ì†Œ |
| 9 | **í”Œë˜í…€** | RSS | âœ… | 10ê°œ ê¸°ì‚¬ |
| 10 | **AIíƒ€ì„ìŠ¤** | ì›¹ ìŠ¤í¬ë˜í•‘ | âœ… | selector: `article`, 7ê°œ ìš”ì†Œ |

**ê²°ê³¼:**
- âœ… RSS: 3ê°œ (ë²¤ì²˜ìŠ¤í€˜ì–´, ì•„ì›ƒìŠ¤íƒ ë”©, í”Œë˜í…€)
- âœ… ì›¹ ìŠ¤í¬ë˜í•‘: 5ê°œ (ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´, ìŠ¤íƒ€íŠ¸ì—…ì—”, ë¸”ë¡œí„°, ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸, AIíƒ€ì„ìŠ¤)
- âŒ ì œì™¸: 2ê°œ (ë”ë²¨, ë”ë¸Œì´ì”¨)

---

## êµ¬í˜„ ë‹¨ê³„

### Phase 1: RSS í”¼ë“œ (3ê°œ ì‚¬ì´íŠ¸) â­â­â­

**ìš°ì„ ìˆœìœ„: ìµœìš°ì„ **
- ë²¤ì²˜ìŠ¤í€˜ì–´
- ì•„ì›ƒìŠ¤íƒ ë”©
- í”Œë˜í…€

**ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰:** í•˜ë£¨ 30-50ê±´

**êµ¬í˜„ ì½”ë“œ:**
```python
import feedparser

def collect_rss(feed_url):
    feed = feedparser.parse(feed_url)
    articles = []

    for entry in feed.entries:
        articles.append({
            'title': entry.title,
            'url': entry.link,
            'published': entry.published,
            'summary': entry.get('summary', '')
        })

    return articles

# ë²¤ì²˜ìŠ¤í€˜ì–´
venturesquare = collect_rss('https://www.venturesquare.net/feed')

# ì•„ì›ƒìŠ¤íƒ ë”©
outstanding = collect_rss('https://outstanding.kr/feed')

# í”Œë˜í…€
platum = collect_rss('https://platum.kr/feed')
```

---

### Phase 2: ì›¹ ìŠ¤í¬ë˜í•‘ (5ê°œ ì‚¬ì´íŠ¸) â­â­

**ìš°ì„ ìˆœìœ„: ì¤‘ìš”**
- ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´
- ìŠ¤íƒ€íŠ¸ì—…ì—”
- ë¸”ë¡œí„°
- ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸
- AIíƒ€ì„ìŠ¤

**ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰:** í•˜ë£¨ 20-40ê±´

---

## ì‚¬ì´íŠ¸ë³„ ìƒì„¸ êµ¬í˜„ ë°©ë²•

### 1. ë²¤ì²˜ìŠ¤í€˜ì–´ (RSS) â­â­â­

```python
URL = 'https://www.venturesquare.net/feed'

def collect_venturesquare():
    feed = feedparser.parse(URL)
    articles = []

    for entry in feed.entries:
        # íˆ¬ì í‚¤ì›Œë“œ í•„í„°ë§
        if any(kw in entry.title for kw in ['íˆ¬ì', 'ìœ ì¹˜', 'ì‹œë¦¬ì¦ˆ']):
            articles.append({
                'site_number': 9,
                'site_name': 'ë²¤ì²˜ìŠ¤í€˜ì–´',
                'site_url': 'https://www.venturesquare.net',
                'article_title': entry.title,
                'article_url': entry.link,
                'published_date': entry.published,
                'content_snippet': entry.get('summary', '')[:500]
            })

    return articles
```

**ì˜ˆìƒ:** í•˜ë£¨ 10-20ê±´

---

### 2. ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´ (ì›¹ ìŠ¤í¬ë˜í•‘) â­â­

```python
URL = 'https://startuptoday.kr'
SELECTOR = 'article'

def collect_startuptoday():
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    response = requests.get(URL, headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

    articles = []
    for article in soup.select(SELECTOR):
        title_elem = article.select_one('h2, h3, .title')
        link_elem = article.select_one('a')

        if title_elem and link_elem:
            title = title_elem.text.strip()
            link = link_elem['href']

            # ì ˆëŒ€ URL ë³€í™˜
            if not link.startswith('http'):
                link = URL + link

            # íˆ¬ì í‚¤ì›Œë“œ í•„í„°ë§
            if any(kw in title for kw in ['íˆ¬ì', 'ìœ ì¹˜', 'ì‹œë¦¬ì¦ˆ']):
                articles.append({
                    'site_number': 11,
                    'site_name': 'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´',
                    'site_url': URL,
                    'article_title': title,
                    'article_url': link,
                    'published_date': datetime.now().strftime('%Y-%m-%d'),
                    'content_snippet': None
                })

    return articles
```

**ì˜ˆìƒ:** í•˜ë£¨ 3-7ê±´

---

### 3. ì•„ì›ƒìŠ¤íƒ ë”© (RSS) â­â­

```python
URL = 'https://outstanding.kr/feed'

def collect_outstanding():
    feed = feedparser.parse(URL)
    articles = []

    for entry in feed.entries:
        # íˆ¬ì í‚¤ì›Œë“œ í•„í„°ë§
        if any(kw in entry.title for kw in ['íˆ¬ì', 'ìœ ì¹˜', 'ìŠ¤íƒ€íŠ¸ì—…']):
            articles.append({
                'site_number': 13,
                'site_name': 'ì•„ì›ƒìŠ¤íƒ ë”©',
                'site_url': 'https://outstanding.kr',
                'article_title': entry.title,
                'article_url': entry.link,
                'published_date': entry.published,
                'content_snippet': entry.get('summary', '')[:500]
            })

    return articles
```

**ì˜ˆìƒ:** í•˜ë£¨ 5-10ê±´

---

### 4. ë”ë²¨ âŒ ì œì™¸

**ì´ìœ :**
- ìœ ë£Œ ì‚¬ì´íŠ¸ (ë¡œê·¸ì¸ í•„ìš”)
- ì›¹ ìŠ¤í¬ë˜í•‘ ë¶ˆê°€ëŠ¥
- Naver ê²€ìƒ‰ APIë¡œë„ ì œí•œì 

**ëŒ€ì•ˆ:**
- ì œëª©ë§Œ Naver ê²€ìƒ‰ APIë¡œ ìˆ˜ì§‘ (ì œí•œì )
- ë˜ëŠ” ì™„ì „ ì œì™¸

---

### 5. ë”ë¸Œì´ì”¨ âŒ ì œì™¸

**ì´ìœ :**
- íˆ¬ìì‚¬ ëª©ë¡ í˜ì´ì§€ (íˆ¬ì ë‰´ìŠ¤ ì•„ë‹˜)
- ì´ì „ì— ì˜ëª»ëœ ë°ì´í„° ìˆ˜ì§‘ë¨

**ê²°ì •:** ì™„ì „ ì œì™¸

---

### 6. ìŠ¤íƒ€íŠ¸ì—…ì—” (ì›¹ ìŠ¤í¬ë˜í•‘) â­â­

```python
URL = 'https://startupn.kr'
SELECTOR = 'article'

def collect_startupn():
    # ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´ì™€ ìœ ì‚¬í•œ ë¡œì§
    # selector: article
    pass
```

**ì˜ˆìƒ:** í•˜ë£¨ 3-5ê±´

---

### 7. ë¸”ë¡œí„° (ì›¹ ìŠ¤í¬ë˜í•‘) â­â­

```python
URL = 'https://www.bloter.net'
SELECTOR = 'article'

def collect_bloter():
    # RSS í”¼ë“œê°€ ë¹„ì–´ìˆìœ¼ë¯€ë¡œ ì›¹ ìŠ¤í¬ë˜í•‘ ì‚¬ìš©
    # selector: article
    pass
```

**ì˜ˆìƒ:** í•˜ë£¨ 3-7ê±´

---

### 8. ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸ (ì›¹ ìŠ¤í¬ë˜í•‘) â­

```python
URL = 'https://www.economist.co.kr'
SELECTOR = 'h2 a'

def collect_economist():
    # selector: h2 a
    # ë§í¬ê°€ ì ì„ ìˆ˜ ìˆìŒ (2ê°œ)
    pass
```

**ì˜ˆìƒ:** í•˜ë£¨ 2-4ê±´

---

### 9. í”Œë˜í…€ (RSS) â­â­â­

```python
URL = 'https://platum.kr/feed'

def collect_platum():
    feed = feedparser.parse(URL)
    articles = []

    for entry in feed.entries:
        # íˆ¬ì í‚¤ì›Œë“œ í•„í„°ë§
        if any(kw in entry.title for kw in ['íˆ¬ì', 'ìœ ì¹˜', 'ì‹œë¦¬ì¦ˆ']):
            articles.append({
                'site_number': 10,
                'site_name': 'í”Œë˜í…€',
                'site_url': 'https://platum.kr',
                'article_title': entry.title,
                'article_url': entry.link,
                'published_date': entry.published,
                'content_snippet': entry.get('summary', '')[:500]
            })

    return articles
```

**ì˜ˆìƒ:** í•˜ë£¨ 5-10ê±´

---

### 10. AIíƒ€ì„ìŠ¤ (ì›¹ ìŠ¤í¬ë˜í•‘) â­â­

```python
URL = 'https://www.aitimes.com'
SELECTOR = 'article'

def collect_aitimes():
    # selector: article
    pass
```

**ì˜ˆìƒ:** í•˜ë£¨ 3-5ê±´

---

## í†µí•© ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ êµ¬ì¡°

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í†µí•© ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# RSS ìˆ˜ì§‘ í•¨ìˆ˜ë“¤
def collect_rss_sites():
    all_articles = []

    # ë²¤ì²˜ìŠ¤í€˜ì–´
    all_articles.extend(collect_venturesquare())

    # ì•„ì›ƒìŠ¤íƒ ë”©
    all_articles.extend(collect_outstanding())

    # í”Œë˜í…€
    all_articles.extend(collect_platum())

    return all_articles


# ì›¹ ìŠ¤í¬ë˜í•‘ ìˆ˜ì§‘ í•¨ìˆ˜ë“¤
def collect_web_scraping_sites():
    all_articles = []

    # ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´
    all_articles.extend(collect_startuptoday())

    # ìŠ¤íƒ€íŠ¸ì—…ì—”
    all_articles.extend(collect_startupn())

    # ë¸”ë¡œí„°
    all_articles.extend(collect_bloter())

    # ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸
    all_articles.extend(collect_economist())

    # AIíƒ€ì„ìŠ¤
    all_articles.extend(collect_aitimes())

    return all_articles


# ë©”ì¸ ì‹¤í–‰
def main():
    print("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")

    # RSS ìˆ˜ì§‘
    rss_articles = collect_rss_sites()
    print(f"RSS: {len(rss_articles)}ê±´")

    # ì›¹ ìŠ¤í¬ë˜í•‘ ìˆ˜ì§‘
    web_articles = collect_web_scraping_sites()
    print(f"Web: {len(web_articles)}ê±´")

    # í•©ê³„
    all_articles = rss_articles + web_articles
    print(f"Total: {len(all_articles)}ê±´")

    # í•„í„°ë§ (í•œêµ­ ê¸°ì—…, íˆ¬ì ë‰´ìŠ¤)
    filtered = filter_korean_investment_news(all_articles)
    print(f"Filtered: {len(filtered)}ê±´")

    # Supabase ì €ì¥
    save_to_supabase(filtered)

    return filtered


if __name__ == '__main__':
    main()
```

---

## ì˜ˆìƒ ìˆ˜ì§‘ëŸ‰ (í•˜ë£¨ ê¸°ì¤€)

| ë°©ë²• | ì‚¬ì´íŠ¸ ìˆ˜ | ê¸°ì‚¬ ìˆ˜ | í•œêµ­ ê¸°ì—… (75%) |
|------|----------|---------|----------------|
| RSS | 3ê°œ | 30-50ê±´ | 22-37ê±´ |
| ì›¹ ìŠ¤í¬ë˜í•‘ | 5ê°œ | 20-40ê±´ | 15-30ê±´ |
| **í•©ê³„** | **8ê°œ** | **50-90ê±´** | **37-67ê±´** |

**ìµœì¢… ëª©í‘œ:** í•˜ë£¨ **40-70ê±´** í•œêµ­ ê¸°ì—… íˆ¬ì ë‰´ìŠ¤

---

## êµ¬í˜„ ì¼ì •

| Phase | ì‘ì—… | ì˜ˆìƒ ì‹œê°„ |
|-------|------|----------|
| Phase 1 | RSS 3ê°œ êµ¬í˜„ | 2ì‹œê°„ |
| Phase 2 | ì›¹ ìŠ¤í¬ë˜í•‘ 5ê°œ êµ¬í˜„ | 4ì‹œê°„ |
| Phase 3 | í†µí•© & í…ŒìŠ¤íŠ¸ | 2ì‹œê°„ |
| **ì´í•©** | | **8ì‹œê°„** |

---

## ë‹¤ìŒ ë‹¨ê³„

### 1. Phase 1 êµ¬í˜„ (RSS 3ê°œ)
```bash
python collect_rss_news.py
```

### 2. Phase 2 êµ¬í˜„ (ì›¹ ìŠ¤í¬ë˜í•‘ 5ê°œ)
```bash
python collect_web_news.py
```

### 3. í†µí•© ì‹¤í–‰
```bash
python collect_all_news.py
```

---

## ì£¼ì˜ì‚¬í•­

### Rate Limiting
```python
import time

# ê° ì‚¬ì´íŠ¸ ìˆ˜ì§‘ í›„ 1ì´ˆ ëŒ€ê¸°
time.sleep(1)
```

### ì—ëŸ¬ í•¸ë“¤ë§
```python
try:
    articles = collect_site()
except Exception as e:
    print(f"Error: {e}")
    articles = []
```

### ì¤‘ë³µ ì œê±°
```python
# URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
seen_urls = set()
unique_articles = []

for article in all_articles:
    if article['article_url'] not in seen_urls:
        seen_urls.add(article['article_url'])
        unique_articles.append(article)
```

---

## ìš”ì•½

âœ… **ìˆ˜ì§‘ ê°€ëŠ¥:** 8ê°œ ì‚¬ì´íŠ¸
- RSS: 3ê°œ (ë²¤ì²˜ìŠ¤í€˜ì–´, ì•„ì›ƒìŠ¤íƒ ë”©, í”Œë˜í…€)
- ì›¹ ìŠ¤í¬ë˜í•‘: 5ê°œ (ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´, ìŠ¤íƒ€íŠ¸ì—…ì—”, ë¸”ë¡œí„°, ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸, AIíƒ€ì„ìŠ¤)

âŒ **ì œì™¸:** 2ê°œ ì‚¬ì´íŠ¸
- ë”ë²¨ (ìœ ë£Œ)
- ë”ë¸Œì´ì”¨ (íˆ¬ìì‚¬ ëª©ë¡)

ğŸ¯ **ëª©í‘œ:** í•˜ë£¨ 40-70ê±´ í•œêµ­ ê¸°ì—… íˆ¬ì ë‰´ìŠ¤ ìˆ˜ì§‘
