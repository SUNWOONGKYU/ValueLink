#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WOWTALE ì‚¬ì´íŠ¸ ë‚´ ê²€ìƒ‰ - 126ê°œ ê¸°ì—…ëª…ìœ¼ë¡œ ì§ì ‘ ê²€ìƒ‰
"""

import os
import sys
import csv
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time
from urllib.parse import quote

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}


def search_wowtale(company_name):
    """WOWTALE ì‚¬ì´íŠ¸ ë‚´ ê²€ìƒ‰"""

    # WOWTALE ê²€ìƒ‰ URL (ì¼ë°˜ì ì¸ WordPress ê²€ìƒ‰ íŒ¨í„´)
    search_url = f"https://wowtale.net/?s={quote(company_name)}"

    try:
        response = requests.get(search_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
        # WordPress ì¼ë°˜ íŒ¨í„´: article, post, entry ë“±ì˜ í´ë˜ìŠ¤
        articles = []

        # ë°©ë²• 1: article íƒœê·¸
        for article in soup.find_all('article'):
            title_tag = article.find('h2')
            if not title_tag:
                title_tag = article.find('h3')
            if not title_tag:
                continue

            link_tag = title_tag.find('a', href=True)
            if not link_tag:
                link_tag = article.find('a', href=True)
            if not link_tag:
                continue

            title = title_tag.get_text().strip()
            url = link_tag.get('href')

            # íˆ¬ì í‚¤ì›Œë“œ í™•ì¸
            investment_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ']
            if any(kw in title for kw in investment_keywords):
                articles.append({'title': title, 'url': url})

        # ë°©ë²• 2: ì¼ë°˜ ë§í¬ì—ì„œ 2026/01 í¬í•¨ + ê¸°ì—…ëª… í¬í•¨
        if not articles:
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                text = link.get_text().strip()

                if company_name in text and '/202' in href and 'wowtale.net' in href:
                    investment_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ']
                    if any(kw in text for kw in investment_keywords):
                        articles.append({'title': text, 'url': href})

        # ì²« ë²ˆì§¸ ê²°ê³¼ ë°˜í™˜
        if articles:
            return articles[0]

        return None

    except Exception as e:
        return None


def extract_article_date(url):
    """ê¸°ì‚¬ ë‚ ì§œ ì¶”ì¶œ"""

    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # ë‚ ì§œ ì¶”ì¶œ
        date_tag = soup.find('time', class_='entry-date')
        if not date_tag:
            date_tag = soup.find('time')

        published_date = datetime.now().strftime('%Y-%m-%d')
        if date_tag:
            try:
                datetime_attr = date_tag.get('datetime')
                if datetime_attr:
                    dt = datetime.strptime(datetime_attr.split('T')[0], '%Y-%m-%d')
                    published_date = dt.strftime('%Y-%m-%d')
            except:
                pass

        return published_date

    except:
        return datetime.now().strftime('%Y-%m-%d')


def main():
    print("=" * 80)
    print("WOWTALE ì‚¬ì´íŠ¸ ë‚´ ê²€ìƒ‰ - 126ê°œ ê¸°ì—…ëª…")
    print("=" * 80)

    # CSV ì½ê¸°
    csv_file = 'sensible_companies_2026_01_COMPLETE.csv'

    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        companies = list(reader)

    print(f"\nì´ {len(companies)}ê°œ ê¸°ì—…\n")

    found = 0
    duplicate = 0
    not_found = []

    for idx, row in enumerate(companies, 1):
        company = row['ê¸°ì—…ëª…']

        print(f"[{idx:3d}/{len(companies)}] {company:20s}...", end=' ')

        # WOWTALE ê²€ìƒ‰
        result = search_wowtale(company)

        if result:
            url = result['url']
            title = result['title']

            # ì¤‘ë³µ í™•ì¸
            existing = supabase.table("investment_news_articles")\
                .select("id")\
                .eq("article_url", url)\
                .execute()

            if not existing.data:
                # ë‚ ì§œ ì¶”ì¶œ
                published_date = extract_article_date(url)

                article = {
                    'site_number': 1,
                    'site_name': 'WOWTALE',
                    'site_url': "",
                    'article_title': title,
                    'article_url': url,
                    'published_date': published_date
                }

                try:
                    supabase.table("investment_news_articles").insert(article).execute()
                    print(f"âœ… {title[:40]}...")
                    found += 1
                except Exception as e:
                    print(f"âŒ DB ì˜¤ë¥˜: {e}")
            else:
                print(f"âš ï¸ ì¤‘ë³µ")
                duplicate += 1
        else:
            print("âŒ ëª» ì°¾ìŒ")
            not_found.append(company)

        time.sleep(1)  # ìš”ì²­ ê°„ê²©

    print(f"\n{'='*80}")
    print("WOWTALE ê²€ìƒ‰ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"âœ… ìƒˆë¡œ ë°œê²¬: {found}ê°œ")
    print(f"âš ï¸ ì¤‘ë³µ: {duplicate}ê°œ")
    print(f"âŒ ëª» ì°¾ìŒ: {len(not_found)}ê°œ")
    print(f"{'='*80}")

    # ìµœì¢… í†µê³„
    result = supabase.table("investment_news_articles").select("article_title").execute()

    final_collected = set()
    for article in result.data:
        for row in companies:
            if row['ê¸°ì—…ëª…'] in article['article_title']:
                final_collected.add(row['ê¸°ì—…ëª…'])

    print(f"\nğŸ“Š 126ê°œ ê¸°ì—… ìµœì¢…:")
    print(f"  âœ… ë‰´ìŠ¤ ìˆìŒ: {len(final_collected)}ê°œ ({len(final_collected)*100//126}%)")
    print(f"  âŒ ë‰´ìŠ¤ ì—†ìŒ: {126-len(final_collected)}ê°œ")

    if not_found:
        with open('wowtale_not_found.txt', 'w', encoding='utf-8') as f:
            for company in not_found:
                f.write(f"{company}\n")
        print(f"\nâš ï¸ WOWTALEì—ì„œ ëª» ì°¾ì€ ê¸°ì—…: wowtale_not_found.txt ({len(not_found)}ê°œ)")

    print(f"\ninvestment_news_articles í…Œì´ë¸” ì´ ë ˆì½”ë“œ:")
    count_result = supabase.table("investment_news_articles").select("id", count="exact").execute()
    print(f"  {count_result.count}ê°œ")


if __name__ == '__main__':
    main()
