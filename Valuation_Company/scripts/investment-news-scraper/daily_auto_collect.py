#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë§¤ì¼ ìë™ ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ (ì™„ë²½í•œ í†µí•© ë²„ì „ v2)

í”„ë¡œì„¸ìŠ¤:
1. 5ëŒ€ ì–¸ë¡ ê¸°ê´€ ì›¹ í¬ë¡¤ë§
2. Google Searchë¡œ ì¶”ê°€ ìˆ˜ì§‘
3. Geminië¡œ íˆ¬ì ë‰´ìŠ¤ ê²€ì¦
4. investment_news_articles í…Œì´ë¸” ì €ì¥
5. Deal í…Œì´ë¸” ë“±ë¡ (íšŒì‚¬ë‹¹ ìµœê³  ì ìˆ˜ 1ê°œ)
6. ëˆ„ë½ ì •ë³´ ì±„ìš°ê¸° (Geminië¡œ íˆ¬ìì, ì£¼ìš”ì‚¬ì—…)
7. ë„¤ì´ë²„ APIë¡œ ì¶”ê°€ ê²€ì¦/ë³´ì™„
8. ë„¤ì´ë²„ ë‰´ìŠ¤ â†’ ì‹¤ì œ ì–¸ë¡ ì‚¬ ë³€í™˜
9. Deal ë²ˆí˜¸ ì¬ì •ë ¬
10. ì´ë©”ì¼ ë°œì†¡

ì‹¤í–‰: python daily_auto_collect.py [--date YYYY-MM-DD]
"""

import os
import sys
import argparse
from datetime import datetime, timedelta
from dotenv import load_dotenv
from supabase import create_client
import requests
from bs4 import BeautifulSoup
import codecs
from google import genai
from google.genai import types
import time
import json
from urllib.parse import urlparse

if sys.platform == 'win32':
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')

load_dotenv()

# Supabase & Gemini í´ë¼ì´ì–¸íŠ¸
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_SERVICE_KEY"))
gemini_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

# 5ëŒ€ ì–¸ë¡ ê¸°ê´€
MEDIA_SITES = [
    {
        'id': 9,
        'name': 'ë²¤ì²˜ìŠ¤í€˜ì–´',
        'url': 'https://www.venturesquare.net/category/news/',
        'article_selector': 'article.post',
        'title_selector': 'h5 a',
        'link_selector': 'h5 a',
    },
    {
        'id': 11,
        'name': 'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´',
        'url': 'https://www.startuptoday.kr/news/articleList.html',
        'article_selector': 'div.article-list-content',
        'title_selector': 'h4.titles',
        'link_selector': 'a',
    },
    {
        'id': 13,
        'name': 'ì•„ì›ƒìŠ¤íƒ ë”©',
        'url': 'https://outstanding.kr/',
        'article_selector': 'article',
        'title_selector': 'h2 a',
        'link_selector': 'h2 a',
    },
    {
        'id': 10,
        'name': 'í”Œë˜í…€',
        'url': 'https://platum.kr/',
        'article_selector': 'article',
        'title_selector': 'h2 a',
        'link_selector': 'h2 a',
    },
    {
        'id': 1,
        'name': 'WOWTALE',
        'url': 'https://wowtale.net/',
        'article_selector': 'article',
        'title_selector': 'h2 a',
        'link_selector': 'h2 a',
    },
]


def log(message, level="INFO"):
    """ë¡œê·¸ ì¶œë ¥"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    print(f"[{timestamp}] [{level}] {message}")


def extract_article_date(html_content, url):
    """ê¸°ì‚¬ HTMLì—ì„œ ë°œí–‰ì¼ ì¶”ì¶œ"""
    try:
        soup = BeautifulSoup(html_content, 'html.parser')

        # ë©”íƒ€ íƒœê·¸ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
        date_meta = soup.find('meta', {'property': 'article:published_time'})
        if date_meta:
            date_str = date_meta.get('content', '')
            return date_str.split('T')[0] if 'T' in date_str else date_str[:10]

        # time íƒœê·¸
        time_tag = soup.find('time')
        if time_tag:
            datetime_attr = time_tag.get('datetime')
            if datetime_attr:
                return datetime_attr.split('T')[0] if 'T' in datetime_attr else datetime_attr[:10]

        return None
    except:
        return None


def verify_with_gemini(title, url):
    """Geminië¡œ íˆ¬ì ë‰´ìŠ¤ì¸ì§€ ê²€ì¦"""
    prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ì´ ìŠ¤íƒ€íŠ¸ì—… íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”:

ì œëª©: {title}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
{{
    "is_investment": true,
    "company": "íšŒì‚¬ëª…",
    "stage": "ì‹œë“œ/í”„ë¦¬A/ì‹œë¦¬ì¦ˆA ë“±",
    "investors": "íˆ¬ììëª…",
    "amount": ìˆ«ìë§Œ
}}

íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ê°€ ì•„ë‹ˆë©´ {{"is_investment": false}}ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""

    try:
        response = gemini_client.models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0,
                max_output_tokens=256,
                response_mime_type='application/json'
            )
        )

        if response and hasattr(response, 'text'):
            text = response.text.strip()
            result = json.loads(text)
            return result

        return None
    except Exception as e:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨
        invest_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'ë¼ìš´ë“œ']
        if any(kw in title for kw in invest_keywords):
            return {'is_investment': True, 'company': None, 'stage': None, 'investors': None, 'amount': None}
        return {'is_investment': False}


def extract_deal_info_with_gemini(title, url):
    """Geminië¡œ ë‰´ìŠ¤ì—ì„œ Deal ì •ë³´ ì¶”ì¶œ"""
    prompt = f"""
ë‹¤ìŒ íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

ì œëª©: {title}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€:
{{
    "company_name": "íšŒì‚¬ëª…",
    "industry": "ì—…ì¢… (AI/í—¬ìŠ¤ì¼€ì–´/í•€í…Œí¬ ë“±)",
    "stage": "íˆ¬ìë‹¨ê³„ (ì‹œë“œ/í”„ë¦¬A/ì‹œë¦¬ì¦ˆA ë“±)",
    "investors": "íˆ¬ìì (ì½¤ë§ˆë¡œ êµ¬ë¶„)",
    "amount": "íˆ¬ìê¸ˆì•¡ (ì–µì› ìˆ«ìë§Œ)",
    "location": "ì§€ì—­",
    "employees": "ì§ì›ìˆ˜ (ìˆ«ìë§Œ)"
}}

ì¡°ê±´:
- ì •ë³´ ì—†ìœ¼ë©´ null
- amountëŠ” ì–µì› ë‹¨ìœ„ ìˆ«ìë§Œ (50ì–µ â†’ 50)
- employeesëŠ” ìˆ«ìë§Œ
- íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ê°€ ì•„ë‹ˆë©´ company_nameì„ nullë¡œ
"""

    try:
        response = gemini_client.models.generate_content(
            model='gemini-2.5-flash',
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0,
                max_output_tokens=512,
                response_mime_type='application/json'
            )
        )

        if response and hasattr(response, 'text'):
            text = response.text.strip()
            result = json.loads(text)
            return result

        return None
    except Exception as e:
        return None


def calculate_score(info):
    """ê¸°ì‚¬ ì ìˆ˜ ê³„ì‚° (11ì  ë§Œì )"""
    score = 0

    if info.get('amount'):
        score += 3
    if info.get('investors'):
        score += 3
    if info.get('stage'):
        score += 2
    if info.get('industry'):
        score += 1
    if info.get('location'):
        score += 1
    if info.get('employees'):
        score += 1

    return score


def extract_site_name_from_url(url):
    """URLì—ì„œ ì‹¤ì œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ"""
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')

            # og:site_name ë©”íƒ€ íƒœê·¸
            og_site = soup.find('meta', {'property': 'og:site_name'})
            if og_site and og_site.get('content'):
                return og_site.get('content').strip()

            # publisher ë©”íƒ€ íƒœê·¸
            publisher = soup.find('meta', {'name': 'publisher'})
            if publisher and publisher.get('content'):
                return publisher.get('content').strip()

        return None
    except:
        return None


# ============================================================
# Step 1: 5ëŒ€ ì–¸ë¡ ê¸°ê´€ ì›¹ í¬ë¡¤ë§
# ============================================================
def step1_crawl_media_sites(target_date):
    """5ëŒ€ ì–¸ë¡ ê¸°ê´€ì—ì„œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    log(f"Step 1: 5ëŒ€ ì–¸ë¡ ê¸°ê´€ í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ ë‚ ì§œ: {target_date})")

    all_articles = []

    for site in MEDIA_SITES:
        log(f"  ğŸ“° {site['name']} í¬ë¡¤ë§ ì¤‘...")

        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(site['url'], headers=headers, timeout=10)

            if response.status_code != 200:
                log(f"    âŒ HTTP {response.status_code}", "ERROR")
                continue

            soup = BeautifulSoup(response.content, 'html.parser')
            article_elements = soup.select(site['article_selector'])[:20]

            site_articles = 0
            for article in article_elements:
                try:
                    title_elem = article.select_one(site['title_selector'])
                    link_elem = article.select_one(site['link_selector'])

                    if not title_elem or not link_elem:
                        continue

                    title = title_elem.get_text(strip=True)
                    url = link_elem.get('href', '')

                    # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                    if url.startswith('/'):
                        base_url = site['url'].split('?')[0].rsplit('/', 1)[0]
                        url = base_url + url

                    if not url.startswith('http'):
                        continue

                    # íˆ¬ì í‚¤ì›Œë“œ í•„í„°
                    invest_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ', 'Pre-A', 'ì‹œë“œ']
                    if any(kw in title for kw in invest_keywords):
                        all_articles.append({
                            'site_id': site['id'],
                            'site_name': site['name'],
                            'title': title,
                            'url': url,
                        })
                        site_articles += 1

                    time.sleep(0.3)
                except:
                    continue

            log(f"    âœ… {site_articles}ê°œ ë°œê²¬")

        except Exception as e:
            log(f"    âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)[:50]}", "ERROR")

        time.sleep(1)

    log(f"  ğŸ“Š ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
    return all_articles


# ============================================================
# Step 2: Gemini ê²€ì¦ + ì €ì¥
# ============================================================
def step2_verify_and_save(articles, target_date):
    """Geminië¡œ ê²€ì¦í•˜ê³  investment_news_articlesì— ì €ì¥"""
    log(f"Step 2: Gemini ê²€ì¦ ë° ì €ì¥")

    saved = 0

    for i, article in enumerate(articles, 1):
        log(f"  [{i}/{len(articles)}] {article['title'][:40]}...")

        # ì¤‘ë³µ ì²´í¬
        existing = supabase.table('investment_news_articles').select('id').eq('article_url', article['url']).execute()
        if existing.data:
            log(f"    âš ï¸ ì¤‘ë³µ")
            continue

        # ë‚ ì§œ ì¶”ì¶œ
        try:
            article_response = requests.get(article['url'], headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
            published_date = extract_article_date(article_response.content, article['url']) if article_response.status_code == 200 else None
        except:
            published_date = None

        # ë‚ ì§œ í•„í„°
        if published_date != target_date:
            log(f"    âŒ ë‚ ì§œ ë²”ìœ„ ë°– ({published_date})")
            continue

        # Gemini ê²€ì¦
        gemini_result = verify_with_gemini(article['title'], article['url'])

        if gemini_result and gemini_result.get('is_investment'):
            # ì €ì¥
            try:
                supabase.table('investment_news_articles').insert({
                    'site_number': article['site_id'],
                    'site_name': article['site_name'],
                    'site_url': urlparse(article['url']).netloc,
                    'article_title': article['title'],
                    'article_url': article['url'],
                    'published_date': published_date,
                    'has_amount': gemini_result.get('amount') is not None,
                    'has_investors': gemini_result.get('investors') is not None,
                    'has_stage': gemini_result.get('stage') is not None,
                }).execute()

                saved += 1
                log(f"    âœ… ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                log(f"    âŒ ì €ì¥ ì˜¤ë¥˜: {str(e)[:40]}", "ERROR")
        else:
            log(f"    âŒ íˆ¬ì ë‰´ìŠ¤ ì•„ë‹˜")

        time.sleep(1)

    log(f"  ğŸ“Š {saved}ê°œ ì €ì¥ ì™„ë£Œ")
    return saved


# ============================================================
# Step 3: Deal í…Œì´ë¸” ë“±ë¡
# ============================================================
def step3_register_to_deals(target_date):
    """Deal í…Œì´ë¸”ì— ë“±ë¡ (íšŒì‚¬ë‹¹ ìµœê³  ì ìˆ˜ 1ê°œ)"""
    log(f"Step 3: Deal í…Œì´ë¸” ë“±ë¡")

    # í•´ë‹¹ ë‚ ì§œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    articles = supabase.table('investment_news_articles').select('*').eq('published_date', target_date).execute()

    if not articles.data:
        log(f"  âš ï¸ í•´ë‹¹ ë‚ ì§œ ë‰´ìŠ¤ ì—†ìŒ")
        return 0

    log(f"  ğŸ“° {len(articles.data)}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘...")

    # ê° ë‰´ìŠ¤ì—ì„œ ì •ë³´ ì¶”ì¶œ
    news_with_info = []

    for article in articles.data:
        info = extract_deal_info_with_gemini(article['article_title'], article['article_url'])

        if info and info.get('company_name'):
            score = calculate_score(info)
            news_with_info.append({
                'article': article,
                'info': info,
                'score': score
            })
            log(f"    âœ… {info['company_name']} (ì ìˆ˜: {score})")

        time.sleep(0.8)

    log(f"  ğŸ“Š {len(news_with_info)}ê°œ íšŒì‚¬ ë°œê²¬")

    # íšŒì‚¬ë³„ ìµœê³  ì ìˆ˜ ì„ íƒ
    company_best = {}
    for news in news_with_info:
        company = news['info']['company_name']
        score = news['score']

        if company not in company_best or score > company_best[company]['score']:
            company_best[company] = news

    # ì¤‘ë³µ ì²´í¬ ë° ë“±ë¡
    existing_deals = supabase.table('deals').select('company_name').execute()
    existing_companies = {deal['company_name'] for deal in existing_deals.data}

    last_deal = supabase.table('deals').select('number').order('number', desc=True).limit(1).execute()
    next_number = last_deal.data[0]['number'] + 1 if last_deal.data else 1

    registered = 0

    for company, news in company_best.items():
        if company in existing_companies:
            log(f"    âš ï¸ {company}: ì´ë¯¸ ì¡´ì¬")
            continue

        article = news['article']
        info = news['info']

        try:
            supabase.table('deals').insert({
                'number': next_number,
                'company_name': company,
                'industry': info.get('industry'),
                'stage': info.get('stage'),
                'investors': info.get('investors'),
                'amount': info.get('amount'),
                'location': info.get('location'),
                'news_title': article['article_title'],
                'news_url': article['article_url'],
                'news_date': article['published_date'],
                'site_name': article['site_name'],
            }).execute()

            log(f"    âœ… {company} ë“±ë¡ (#{next_number})")

            existing_companies.add(company)
            next_number += 1
            registered += 1

        except Exception as e:
            log(f"    âŒ {company} ì˜¤ë¥˜: {str(e)[:40]}", "ERROR")

    log(f"  ğŸ“Š {registered}ê°œ ì‹ ê·œ ë“±ë¡")
    return registered


# ============================================================
# Step 4: ëˆ„ë½ ì •ë³´ ì±„ìš°ê¸°
# ============================================================
def step4_fill_missing_info():
    """íˆ¬ìì ë° ì£¼ìš”ì‚¬ì—… ì •ë³´ ì±„ìš°ê¸°"""
    log(f"Step 4: ëˆ„ë½ ì •ë³´ ì±„ìš°ê¸°")

    # íˆ¬ìì ì—†ëŠ” Deal
    deals_no_investors = supabase.table('deals').select('*').is_('investors', 'null').execute()
    log(f"  ğŸ“Š íˆ¬ìì ì •ë³´ ì—†ëŠ” Deal: {len(deals_no_investors.data)}ê°œ")

    # ì£¼ìš”ì‚¬ì—… ì—†ëŠ” Deal
    deals_no_industry = supabase.table('deals').select('*').or_('industry.is.null,industry.eq.-').execute()
    log(f"  ğŸ“Š ì£¼ìš”ì‚¬ì—… ì •ë³´ ì—†ëŠ” Deal: {len(deals_no_industry.data)}ê°œ")

    # (ì¶”í›„ Geminië¡œ ì¶”ì¶œ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥)
    log(f"  âš ï¸ ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”")


# ============================================================
# Step 5: ë„¤ì´ë²„ ë‰´ìŠ¤ â†’ ì‹¤ì œ ì–¸ë¡ ì‚¬ ë³€í™˜
# ============================================================
def step5_fix_naver_news():
    """ë„¤ì´ë²„ ë‰´ìŠ¤ë¡œ í‘œì‹œëœ í•­ëª©ì˜ ì‹¤ì œ ì–¸ë¡ ì‚¬ ì¶”ì¶œ"""
    log(f"Step 5: ë„¤ì´ë²„ ë‰´ìŠ¤ ì–¸ë¡ ì‚¬ ë³€í™˜")

    result = supabase.table('deals').select('id,company_name,site_name,news_url').eq('site_name', 'ë„¤ì´ë²„ ë‰´ìŠ¤').execute()

    if not result.data:
        log(f"  âœ… ë³€í™˜ í•„ìš” ì—†ìŒ")
        return

    log(f"  ğŸ“Š {len(result.data)}ê°œ í•­ëª© ì²˜ë¦¬ ì¤‘...")

    updated = 0
    for deal in result.data:
        real_site = extract_site_name_from_url(deal['news_url'])

        if real_site:
            supabase.table('deals').update({'site_name': real_site}).eq('id', deal['id']).execute()
            updated += 1

        time.sleep(0.5)

    log(f"  âœ… {updated}ê°œ ë³€í™˜ ì™„ë£Œ")


# ============================================================
# Step 6: Deal ë²ˆí˜¸ ì¬ì •ë ¬
# ============================================================
def step6_renumber_deals():
    """Deal ë²ˆí˜¸ë¥¼ ìµœì‹ ìˆœìœ¼ë¡œ ì¬ì •ë ¬"""
    log(f"Step 6: Deal ë²ˆí˜¸ ì¬ì •ë ¬")

    deals = supabase.table('deals').select('*').order('news_date', desc=True).order('id', desc=True).execute()

    log(f"  ğŸ“Š ì´ {len(deals.data)}ê°œ Deal ì¬ì •ë ¬ ì¤‘...")

    # Step 1: ìŒìˆ˜ë¡œ ë³€ê²½ (ì¤‘ë³µ ë°©ì§€)
    for i, deal in enumerate(deals.data, 1):
        supabase.table('deals').update({'number': -i}).eq('id', deal['id']).execute()

    # Step 2: ì–‘ìˆ˜ë¡œ ë³€ê²½
    for new_number, deal in enumerate(deals.data, 1):
        supabase.table('deals').update({'number': new_number}).eq('id', deal['id']).execute()

    log(f"  âœ… ìµœì‹ ìˆœ 1~{len(deals.data)}ë²ˆ ì¬ì •ë ¬ ì™„ë£Œ")


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================
def main():
    parser = argparse.ArgumentParser(description='ë§¤ì¼ ìë™ ë‰´ìŠ¤ ìˆ˜ì§‘')
    parser.add_argument('--date', type=str, help='ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ (YYYY-MM-DD)', default=None)
    args = parser.parse_args()

    # ëŒ€ìƒ ë‚ ì§œ ê²°ì •
    if args.date:
        target_date = args.date
    else:
        # ê¸°ë³¸: ì–´ì œ
        target_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')

    print("=" * 70)
    print("ğŸ“° ë§¤ì¼ ìë™ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
    print(f"â° ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ ìˆ˜ì§‘ ëŒ€ìƒ ë‚ ì§œ: {target_date}")
    print("=" * 70)

    try:
        # Step 1: ì›¹ í¬ë¡¤ë§
        articles = step1_crawl_media_sites(target_date)

        if articles:
            # Step 2: ê²€ì¦ ë° ì €ì¥
            saved = step2_verify_and_save(articles, target_date)

            if saved > 0:
                # Step 3: Deal ë“±ë¡
                registered = step3_register_to_deals(target_date)

                if registered > 0:
                    # Step 4: ëˆ„ë½ ì •ë³´ ì±„ìš°ê¸°
                    step4_fill_missing_info()

                    # Step 5: ë„¤ì´ë²„ ë‰´ìŠ¤ ë³€í™˜
                    step5_fix_naver_news()

                    # Step 6: ë²ˆí˜¸ ì¬ì •ë ¬
                    step6_renumber_deals()

        print("\n" + "=" * 70)
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("=" * 70)

    except Exception as e:
        log(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
        raise


if __name__ == '__main__':
    main()
