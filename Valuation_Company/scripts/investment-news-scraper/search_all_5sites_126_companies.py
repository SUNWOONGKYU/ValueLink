#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
5ê°œ ì–¸ë¡ ì‚¬ ì‚¬ì´íŠ¸ ë‚´ ê²€ìƒ‰ - 126ê°œ ê¸°ì—…ëª…
WOWTALE, ë²¤ì²˜ìŠ¤í€˜ì–´, ì•„ì›ƒìŠ¤íƒ ë”©, í”Œë˜í…€, ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´
"""

import os
import sys
import csv
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time
from urllib.parse import quote

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# 5ê°œ ì–¸ë¡ ì‚¬ ì •ë³´
SITES = {
    'WOWTALE': {
        'number': 1,
        'search_url': 'https://wowtale.net/?s={}',
        'domain': 'wowtale.net'
    },
    'ë²¤ì²˜ìŠ¤í€˜ì–´': {
        'number': 9,
        'search_url': 'https://www.venturesquare.net/?s={}',
        'domain': 'venturesquare.net'
    },
    'ì•„ì›ƒìŠ¤íƒ ë”©': {
        'number': 13,
        'search_url': 'https://outstanding.kr/?s={}',
        'domain': 'outstanding.kr'
    },
    'í”Œë˜í…€': {
        'number': 10,
        'search_url': 'https://platum.kr/?s={}',
        'domain': 'platum.kr'
    },
    'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´': {
        'number': 11,
        'search_url': 'https://www.startuptoday.kr/?s={}',
        'domain': 'startuptoday.kr'
    }
}


def search_site(site_name, site_info, company_name):
    """íŠ¹ì • ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì—…ëª… ê²€ìƒ‰"""

    search_url = site_info['search_url'].format(quote(company_name))

    try:
        response = requests.get(search_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
        articles = []

        # WordPress ì¼ë°˜ íŒ¨í„´: article íƒœê·¸
        for article in soup.find_all('article'):
            title_tag = article.find('h2')
            if not title_tag:
                title_tag = article.find('h3')
            if not title_tag:
                title_tag = article.find('h1')
            if not title_tag:
                continue

            link_tag = title_tag.find('a', href=True)
            if not link_tag:
                link_tag = article.find('a', href=True)
            if not link_tag:
                continue

            title = title_tag.get_text().strip()
            url = link_tag.get('href')

            # ê³µì§€ì‚¬í•­ ì œì™¸
            if '[ê³µì§€]' in title or 'ê³µì§€ì‚¬í•­' in title or 'ì´ë²¤íŠ¸' in title:
                continue

            # íˆ¬ì í‚¤ì›Œë“œ í™•ì¸
            investment_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ']
            if any(kw in title for kw in investment_keywords):
                # ê¸°ì—…ëª… í™•ì¸
                if company_name in title:
                    articles.append({'title': title, 'url': url})

        # ì¼ë°˜ ë§í¬ì—ì„œë„ ê²€ìƒ‰ (article íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš°)
        if not articles:
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                text = link.get_text().strip()

                # ê³µì§€ì‚¬í•­ ì œì™¸
                if '[ê³µì§€]' in text or 'ê³µì§€ì‚¬í•­' in text or 'ì´ë²¤íŠ¸' in text:
                    continue

                if company_name in text and site_info['domain'] in href:
                    investment_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ']
                    if any(kw in text for kw in investment_keywords):
                        articles.append({'title': text, 'url': href})

        # ì²« ë²ˆì§¸ ê²°ê³¼ ë°˜í™˜
        if articles:
            return articles[0]

        return None

    except Exception as e:
        return None


def extract_article_date(url):
    """ê¸°ì‚¬ ë‚ ì§œ ì¶”ì¶œ"""

    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # ë‚ ì§œ ì¶”ì¶œ
        date_tag = soup.find('time')
        if not date_tag:
            date_tag = soup.find('span', class_=['date', 'entry-date', 'published'])

        published_date = datetime.now().strftime('%Y-%m-%d')
        if date_tag:
            try:
                datetime_attr = date_tag.get('datetime')
                if datetime_attr:
                    dt = datetime.strptime(datetime_attr.split('T')[0], '%Y-%m-%d')
                    published_date = dt.strftime('%Y-%m-%d')
                else:
                    date_text = date_tag.get_text().strip()
                    # YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD í˜•ì‹ ì¶”ì¶œ
                    import re
                    match = re.search(r'(\d{4})[.-](\d{2})[.-](\d{2})', date_text)
                    if match:
                        published_date = f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
            except:
                pass

        return published_date

    except:
        return datetime.now().strftime('%Y-%m-%d')


def main():
    print("=" * 80)
    print("5ê°œ ì–¸ë¡ ì‚¬ ì‚¬ì´íŠ¸ ë‚´ ê²€ìƒ‰ - 126ê°œ ê¸°ì—…ëª…")
    print("=" * 80)

    # CSV ì½ê¸°
    csv_file = 'sensible_companies_2026_01_COMPLETE.csv'

    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        companies = list(reader)

    print(f"\nì´ {len(companies)}ê°œ ê¸°ì—…\n")

    found_by_site = {site: 0 for site in SITES.keys()}
    total_found = 0
    total_duplicate = 0
    not_found = []

    for idx, row in enumerate(companies, 1):
        company = row['ê¸°ì—…ëª…']

        print(f"[{idx:3d}/{len(companies)}] {company:20s}...", end=' ')

        found_this_company = False

        # 5ê°œ ì‚¬ì´íŠ¸ì—ì„œ ìˆœì„œëŒ€ë¡œ ê²€ìƒ‰
        for site_name, site_info in SITES.items():
            result = search_site(site_name, site_info, company)

            if result:
                url = result['url']
                title = result['title']

                # ì¤‘ë³µ í™•ì¸
                existing = supabase.table("investment_news_articles")\
                    .select("id")\
                    .eq("article_url", url)\
                    .execute()

                if not existing.data:
                    # ë‚ ì§œ ì¶”ì¶œ
                    published_date = extract_article_date(url)

                    article = {
                        'site_number': site_info['number'],
                        'site_name': site_name,
                        'site_url': "",
                        'article_title': title,
                        'article_url': url,
                        'published_date': published_date
                    }

                    try:
                        supabase.table("investment_news_articles").insert(article).execute()
                        print(f"âœ… [{site_name}] {title[:30]}...")
                        found_by_site[site_name] += 1
                        total_found += 1
                        found_this_company = True
                        break  # í•˜ë‚˜ ì°¾ìœ¼ë©´ ë‹¤ìŒ ê¸°ì—…ìœ¼ë¡œ
                    except Exception as e:
                        pass
                else:
                    print(f"âš ï¸ [{site_name}] ì¤‘ë³µ")
                    total_duplicate += 1
                    found_this_company = True
                    break

            time.sleep(0.5)  # ì‚¬ì´íŠ¸ë‹¹ ëŒ€ê¸°

        if not found_this_company:
            print("âŒ ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ ëª» ì°¾ìŒ")
            not_found.append(company)

        time.sleep(1)  # ê¸°ì—…ë‹¹ ëŒ€ê¸°

    print(f"\n{'='*80}")
    print("5ê°œ ì–¸ë¡ ì‚¬ ê²€ìƒ‰ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"âœ… ìƒˆë¡œ ë°œê²¬: {total_found}ê°œ")
    for site, count in found_by_site.items():
        if count > 0:
            print(f"   - {site}: {count}ê°œ")
    print(f"âš ï¸ ì¤‘ë³µ: {total_duplicate}ê°œ")
    print(f"âŒ ëª» ì°¾ìŒ: {len(not_found)}ê°œ")
    print(f"{'='*80}")

    # ìµœì¢… í†µê³„
    result = supabase.table("investment_news_articles").select("article_title").execute()

    final_collected = set()
    for article in result.data:
        for comp in companies:
            if comp['ê¸°ì—…ëª…'] in article['article_title']:
                final_collected.add(comp['ê¸°ì—…ëª…'])

    print(f"\nğŸ“Š 126ê°œ ê¸°ì—… ìµœì¢…:")
    print(f"  âœ… ë‰´ìŠ¤ ìˆìŒ: {len(final_collected)}ê°œ ({len(final_collected)*100//126}%)")
    print(f"  âŒ ë‰´ìŠ¤ ì—†ìŒ: {126-len(final_collected)}ê°œ")

    if not_found:
        with open('all_5sites_not_found.txt', 'w', encoding='utf-8') as f:
            for company in not_found:
                f.write(f"{company}\n")
        print(f"\nâš ï¸ 5ê°œ ì‚¬ì´íŠ¸ì—ì„œ ëª» ì°¾ì€ ê¸°ì—…: all_5sites_not_found.txt ({len(not_found)}ê°œ)")

    print(f"\ninvestment_news_articles í…Œì´ë¸” ì´ ë ˆì½”ë“œ:")
    count_result = supabase.table("investment_news_articles").select("id", count="exact").execute()
    print(f"  {count_result.count}ê°œ")


if __name__ == '__main__':
    main()
