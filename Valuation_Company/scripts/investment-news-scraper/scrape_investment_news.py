#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íˆ¬ì ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸
ì‘ì„±ì¼: 2026-01-25
ìš©ë„: êµ­ë‚´ 19ê°œ íˆ¬ììœ ì¹˜ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ë° Supabase ì €ì¥
"""

import os
import time
import logging
from datetime import datetime, date
from typing import List, Dict, Optional
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
# from supabase import create_client, Client

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('scraping_log.txt', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


# ================================================================
# ì„¤ì •
# ================================================================

# Supabase ì—°ê²°
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError("í™˜ê²½ë³€ìˆ˜ SUPABASE_URLê³¼ SUPABASE_KEYë¥¼ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")

# supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ëŒ€ìƒ ì‚¬ì´íŠ¸ ëª©ë¡
SITES = [
    {'number': 8, 'name': 'ë”ë¸Œì´ì”¨', 'url': 'https://thevc.kr'},
    {'number': 9, 'name': 'ë²¤ì²˜ìŠ¤í€˜ì–´', 'url': 'https://www.venturesquare.net'},
    {'number': 10, 'name': 'í”Œë˜í…€', 'url': 'https://platum.kr'},
    {'number': 11, 'name': 'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´', 'url': 'https://startuptoday.kr'},
    {'number': 12, 'name': 'ìŠ¤íƒ€íŠ¸ì—…ì—”', 'url': 'https://startupn.kr'},
    {'number': 13, 'name': 'ì•„ì›ƒìŠ¤íƒ ë”©', 'url': 'https://outstanding.kr'},
    {'number': 14, 'name': 'ëª¨ë¹„ì¸ì‚¬ì´ë“œ', 'url': 'https://mobiinside.co.kr'},
    {'number': 15, 'name': 'ì§€ë””ë„·ì½”ë¦¬ì•„', 'url': 'https://www.zdnet.co.kr'},
    {'number': 16, 'name': 'ë”ë²¨', 'url': 'https://www.thebell.co.kr'},
    {'number': 17, 'name': 'ë„¥ìŠ¤íŠ¸ìœ ë‹ˆì½˜', 'url': 'https://nextunicorn.kr'},
    {'number': 18, 'name': 'í…Œí¬ì›”ë“œë‰´ìŠ¤', 'url': 'https://www.epnc.co.kr'},
    {'number': 19, 'name': 'AIíƒ€ì„ìŠ¤', 'url': 'https://www.aitimes.com'},
    {'number': 20, 'name': 'ë²¤ì²˜ê²½ì˜ì‹ ë¬¸', 'url': 'https://www.vmnews.co.kr'},
    {'number': 21, 'name': 'ë‰´ìŠ¤í†±', 'url': 'https://www.newstopkorea.com'},
    {'number': 22, 'name': 'ë¸”ë¡œí„°', 'url': 'https://www.bloter.net'},
    {'number': 23, 'name': 'ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸', 'url': 'https://www.economist.co.kr'},
    {'number': 24, 'name': 'ë§¤ì¼ê²½ì œ MKí…Œí¬ë¦¬ë·°', 'url': 'https://www.mk.co.kr/news/it'},
    {'number': 25, 'name': 'ë‹¤ìŒë‰´ìŠ¤ ë²¤ì²˜/ìŠ¤íƒ€íŠ¸ì—…', 'url': 'https://news.daum.net/section/2/venture'},
    {'number': 26, 'name': 'ëŒ€í•œë¯¼êµ­ ì •ì±…ë¸Œë¦¬í•‘', 'url': 'https://www.korea.kr'},
]

# ê²€ìƒ‰ í‚¤ì›Œë“œ (íˆ¬ììœ ì¹˜ ê´€ë ¨)
KEYWORDS = ['íˆ¬ì', 'íˆ¬ììœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'ë²¤ì²˜ìºí”¼í„¸', 'VC', 'ì—”ì ¤íˆ¬ì', 'í”„ë¦¬ì‹œë¦¬ì¦ˆ', 'ë¸Œë¦¿ì§€']

# ê¸°ê°„ ì„¤ì •
START_DATE = date(2026, 1, 1)
END_DATE = date.today()

# ìš”ì²­ ì„¤ì •
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
}

REQUEST_DELAY = 2  # ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)


# ================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ================================================================

def contains_keyword(text: str) -> bool:
    """í…ìŠ¤íŠ¸ì— íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    if not text:
        return False
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in KEYWORDS)


def parse_date(date_str: str) -> Optional[date]:
    """ë‚ ì§œ ë¬¸ìì—´ì„ date ê°ì²´ë¡œ ë³€í™˜ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)"""
    date_formats = [
        '%Y-%m-%d',
        '%Y.%m.%d',
        '%Y/%m/%d',
        '%Yë…„ %mì›” %dì¼',
    ]

    for fmt in date_formats:
        try:
            return datetime.strptime(date_str.strip(), fmt).date()
        except ValueError:
            continue

    logger.warning(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}")
    return None


def is_valid_date(article_date: date) -> bool:
    """ê¸°ì‚¬ ë‚ ì§œê°€ ìˆ˜ì§‘ ê¸°ê°„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
    return START_DATE <= article_date <= END_DATE


# ================================================================
# ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ (ì‚¬ì´íŠ¸ë³„ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•„ìš”)
# ================================================================

def _scrape_thevc_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ thevc.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_venturesquare_net(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles = []
    site_number = site_info['number']
    site_name = site_info['name']
    site_url_base = site_info['url'] # Base URL

    # ë²¤ì²˜ìŠ¤í€˜ì–´ í™ˆí˜ì´ì§€ì˜ "ìŠ¤íƒ€íŠ¸ì—… í† í”½" ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ
    # ì´ ì„¹ì…˜ì€ ul#topic-news-list ë‚´ë¶€ì— ê¸°ì‚¬ ëª©ë¡ì´ ìˆìŠµë‹ˆë‹¤.
    # HTML ë¤í”„ ë¶„ì„ ê²°ê³¼, ì´ ë¶€ë¶„ì´ ì œëª©ê³¼ ë‚ ì§œ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ê³  ìˆì–´ ê°€ì¥ ì í•©í•©ë‹ˆë‹¤.

    # target_urlì€ scrape_site_dispatchì—ì„œ ì´ë¯¸ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” response.textë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©

    try:
        # article_elem ëŒ€ì‹  topic-news-listì˜ li ìš”ì†Œë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.
        article_elements = soup.select('ul#topic-news-list > li')
        logger.info(f"[{site_name}] Found {len(article_elements)} potential article elements in 'ul#topic-news-list > li'.")

        for i, article_elem in enumerate(article_elements):
            logger.debug(f"[{site_name}] Processing article element {i+1}: {article_elem.prettify()[:500]}...") # Print first 500 chars for brevity
            try:
                # ì œëª© ì¶”ì¶œ: h4.bold ì•ˆì— ìˆëŠ” a íƒœê·¸
                title_elem = article_elem.select_one('h4.bold a')
                if not title_elem:
                    continue
                title = title_elem.get_text(strip=True)

                # í‚¤ì›Œë“œ í•„í„°ë§
                if not contains_keyword(title):
                    continue

                # URL ì¶”ì¶œ: h4.bold ì•ˆì— ìˆëŠ” a íƒœê·¸ì˜ href ì†ì„±
                article_url = title_elem['href']
                if not article_url.startswith('http'):
                    article_url = site_url_base.rstrip('/') + article_url

                # ë‚ ì§œ ì¶”ì¶œ: time íƒœê·¸ì˜ datetime ì†ì„±
                date_elem = article_elem.select_one('time[datetime]')
                published_date = None
                if date_elem and date_elem.has_attr('datetime'):
                    # datetime ì†ì„±ê°’ì€ 'YYYY-MM-DD HH:MM:SS' í˜•ì‹ì¼ ìˆ˜ ìˆìŒ
                    # parse_date í•¨ìˆ˜ëŠ” ë‹¤ì–‘í•œ í˜•ì‹ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                    date_text = date_elem['datetime'].split('T')[0] # 'YYYY-MM-DD' ë¶€ë¶„ë§Œ ì‚¬ìš©
                    published_date = parse_date(date_text)

                # ë‚ ì§œê°€ ì—†ê±°ë‚˜ ê¸°ê°„ ì™¸ë©´ ìŠ¤í‚µ
                if not published_date or not is_valid_date(published_date):
                    continue

                # ë‚´ìš© ë°œì·Œ (ì„ íƒ ì‚¬í•­): Venturesquareì˜ ì´ ì„¹ì…˜ì—ì„œëŠ” ë°œì·Œê°€ ëª…í™•í•˜ì§€ ì•ŠìŒ
                snippet = None # ì´ ì„¹ì…˜ì—ì„œëŠ” ë°œì·Œê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ Noneìœ¼ë¡œ ì„¤ì •

                articles.append({
                    'site_number': site_number,
                    'site_name': site_name,
                    'site_url': site_url_base, # Use the base site URL
                    'article_title': title,
                    'article_url': article_url,
                    'published_date': published_date.isoformat(),
                    'content_snippet': snippet,
                })

            except Exception as e:
                logger.error(f"[{site_name}] ê¸°ì‚¬ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e} (URL: {article_url if 'article_url' in locals() else 'N/A'})")
                continue

    except Exception as e:
        logger.error(f"âŒ [{site_name}] ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

    logger.info(f"âœ… [{site_number}] {site_name}: {len(articles)}ê±´ ìˆ˜ì§‘")
    return articles

def _scrape_platum_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ platum.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_startuptoday_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ startuptoday.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_startupn_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ startupn.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_outstanding_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ outstanding.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_mobiinside_co_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ mobiinside.co.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_zdnet_co_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ www.zdnet.co.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_thebell_co_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ www.thebell.co.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_nextunicorn_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ nextunicorn.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_epnc_co_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ www.epnc.co.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_aitimes_com(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ www.aitimes.com ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_vmnews_co_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ www.vmnews.co.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_newstopkorea_com(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ www.newstopkorea.com ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_bloter_net(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ www.bloter.net ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_economist_co_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ www.economist.co.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_mk_co_kr_news_it(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ www.mk.co.kr/news/it ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_news_daum_net_section_2_venture(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ news.daum.net/section/2/venture ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data

def _scrape_korea_kr(soup: BeautifulSoup, site_info: Dict) -> List[Dict]:
    articles_data = []
    # âš ï¸ www.korea.kr ì‚¬ì´íŠ¸ HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° ìˆ˜ì • í•„ìš”
    logger.warning(f"âš ï¸ {site_info['name']} ({site_info['url']}) ìŠ¤í¬ë˜í•‘ ë¡œì§: ìˆ˜ë™ êµ¬í˜„ í•„ìš”")
    return articles_data


# ì‚¬ì´íŠ¸ë³„ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ ë§¤í•‘
SITE_SCRAPERS = {
    'https://thevc.kr': _scrape_thevc_kr,
    'https://www.venturesquare.net': _scrape_venturesquare_net,
    'https://platum.kr': _scrape_platum_kr,
    'https://startuptoday.kr': _scrape_startuptoday_kr,
    'https://startupn.kr': _scrape_startupn_kr,
    'https://outstanding.kr': _scrape_outstanding_kr,
    'https://mobiinside.co.kr': _scrape_mobiinside_co_kr,
    'https://www.zdnet.co.kr': _scrape_zdnet_co_kr,
    'https://www.thebell.co.kr': _scrape_thebell_co_kr,
    'https://nextunicorn.kr': _scrape_nextunicorn_kr,
    'https://www.epnc.co.kr': _scrape_epnc_co_kr,
    'https://www.aitimes.com': _scrape_aitimes_com,
    'https://www.vmnews.co.kr': _scrape_vmnews_co_kr,
    'https://www.newstopkorea.com': _scrape_newstopkorea_com,
    'https://www.bloter.net': _scrape_bloter_net,
    'https://www.economist.co.kr': _scrape_economist_co_kr,
    'https://www.mk.co.kr/news/it': _scrape_mk_co_kr_news_it,
    'https://news.daum.net/section/2/venture': _scrape_news_daum_net_section_2_venture,
    'https://www.korea.kr': _scrape_korea_kr,
}


def scrape_site_dispatch(site: Dict) -> List[Dict]:
    """
    ê° ì‚¬ì´íŠ¸ì˜ URLì— ë”°ë¼ ì ì ˆí•œ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ë””ìŠ¤íŒ¨ì²˜.
    ê° ì‚¬ì´íŠ¸ë³„ HTML êµ¬ì¡°ì— ë§ê²Œ `_scrape_ì‚¬ì´íŠ¸ëª…` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.
    """
    articles = []
    site_number = site['number']
    site_name = site['name']
    site_url = site['url']

    logger.info(f"ğŸ” [{site_number}] {site_name} ìŠ¤í¬ë˜í•‘ ì‹œì‘...")

    try:
        # ì‚¬ì´íŠ¸ ë©”ì¸ í˜ì´ì§€ ìš”ì²­
        # ë²¤ì²˜ê²½ì˜ì‹ ë¬¸ (www.vmnews.co.kr)ì˜ SSL ì˜¤ë¥˜ë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•´ verify=False ì¶”ê°€
        if site_url == 'https://www.vmnews.co.kr':
            response = requests.get(site_url, headers=HEADERS, timeout=10, verify=False)
        else:
            response = requests.get(site_url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        # TODO: ëŒ€ë¶€ë¶„ì˜ í•œêµ­ì–´ ì‚¬ì´íŠ¸ëŠ” EUC-KR ë˜ëŠ” CP949ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
        #       response.encodingì„ 'utf-8'ë¡œ ê°•ì œí•˜ê¸° ì „ì— requestsê°€ ìë™ìœ¼ë¡œ ê°ì§€í•˜ë„ë¡ í•˜ê±°ë‚˜,
        #       chardet ë¼ì´ë¸ŒëŸ¬ë¦¬ ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì¸ì½”ë”©ì„ ëª…ì‹œì ìœ¼ë¡œ ê°ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
        #       í˜„ì¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ utf-8ë¡œ ì‹œë„í•˜ê³ , ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        response.encoding = 'utf-8' # ê¸°ë³¸ì ìœ¼ë¡œ utf-8ë¡œ ì‹œë„

        soup = BeautifulSoup(response.text, 'lxml')

        # í•´ë‹¹ ì‚¬ì´íŠ¸ì— ë§ëŠ” ìŠ¤í¬ë˜í¼ í•¨ìˆ˜ í˜¸ì¶œ
        scraper_func = SITE_SCRAPERS.get(site_url)
        if scraper_func:
            articles = scraper_func(soup, site)
        else:
            logger.error(f"âŒ [{site_number}] {site_name}ì— ëŒ€í•œ ìŠ¤í¬ë˜í¼ í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        logger.info(f"âœ… [{site_number}] {site_name}: {len(articles)}ê±´ ìˆ˜ì§‘")

    except requests.RequestException as e:
        logger.error(f"âŒ [{site_number}] {site_name} ìš”ì²­ ì‹¤íŒ¨: {e}")
    except Exception as e:
        logger.error(f"âŒ [{site_number}] {site_name} ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")

    return articles



# ================================================================
# Supabase ì €ì¥
# ================================================================

def save_to_supabase(articles: List[Dict]) -> int:
    """
    ìˆ˜ì§‘ëœ ê¸°ì‚¬ë¥¼ Supabaseì— ì €ì¥ (REST API ì§ì ‘ í˜¸ì¶œ)
    """
    if not articles:
        return 0

    saved_count = 0
    batch_size = 100

    # REST API ì—”ë“œí¬ì¸íŠ¸
    api_url = f"{SUPABASE_URL}/rest/v1/investment_news_articles"
    headers = {
        'apikey': SUPABASE_KEY,
        'Authorization': f'Bearer {SUPABASE_KEY}',
        'Content-Type': 'application/json',
        'Prefer': 'return=representation'
    }

    for i in range(0, len(articles), batch_size):
        batch = articles[i:i + batch_size]

        try:
            # REST API POST ìš”ì²­
            response = requests.post(api_url, json=batch, headers=headers, timeout=30)

            if response.status_code == 201:
                saved_count += len(batch)
                logger.info(f"ğŸ’¾ Supabase ì €ì¥: {len(batch)}ê±´ (ëˆ„ì : {saved_count}ê±´)")
            elif response.status_code == 409:
                # ì¤‘ë³µ URL
                logger.warning(f"âš ï¸  ì¤‘ë³µ URL ê°ì§€, ìŠ¤í‚µ: {len(batch)}ê±´")
            else:
                logger.error(f"âŒ Supabase ì €ì¥ ì‹¤íŒ¨ (HTTP {response.status_code}): {response.text}")

        except requests.RequestException as e:
            logger.error(f"âŒ Supabase ì €ì¥ ìš”ì²­ ì‹¤íŒ¨: {e}")
        except Exception as e:
            logger.error(f"âŒ Supabase ì €ì¥ ì˜¤ë¥˜: {e}")

    return saved_count


# ================================================================
# ë©”ì¸ í•¨ìˆ˜
# ================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=" * 60)
    logger.info("ğŸ“° íˆ¬ì ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    logger.info(f"ğŸ“… ê¸°ê°„: {START_DATE} ~ {END_DATE}")
    logger.info(f"ğŸŒ ëŒ€ìƒ ì‚¬ì´íŠ¸: {len(SITES)}ê°œ")
    logger.info("=" * 60)

    start_time = time.time()
    total_articles = []

    # ì‚¬ì´íŠ¸ë³„ ìŠ¤í¬ë˜í•‘
    for idx, site in enumerate(SITES, 1):
        logger.info(f"\n[{idx}/{len(SITES)}] {site['name']} ì²˜ë¦¬ ì¤‘...")

        articles = scrape_site_dispatch(site)
        total_articles.extend(articles)

        # ìš”ì²­ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        if idx < len(SITES):
            logger.info(f"â³ {REQUEST_DELAY}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(REQUEST_DELAY)

    # Supabase ì €ì¥
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ’¾ Supabase ì €ì¥ ì‹œì‘...")
    saved_count = save_to_supabase(total_articles)

    # ê²°ê³¼ ìš”ì•½
    elapsed_time = time.time() - start_time
    logger.info("\n" + "=" * 60)
    logger.info("âœ… ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
    logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê±´ìˆ˜: {len(total_articles)}ê±´")
    logger.info(f"ğŸ’¾ ì €ì¥ ê±´ìˆ˜: {saved_count}ê±´")
    logger.info(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    logger.info("=" * 60)

    # ì‚¬ì´íŠ¸ë³„ í†µê³„
    logger.info("\nğŸ“ˆ ì‚¬ì´íŠ¸ë³„ ìˆ˜ì§‘ ê±´ìˆ˜:")
    site_stats = {}
    for article in total_articles:
        site_name = article['site_name']
        site_stats[site_name] = site_stats.get(site_name, 0) + 1

    for site_name, count in sorted(site_stats.items(), key=lambda x: x[1], reverse=True):
        logger.info(f"  - {site_name}: {count}ê±´")

    # ë­í‚¹ ì—…ë°ì´íŠ¸ ì•ˆë‚´
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Œ ë‹¤ìŒ ë‹¨ê³„:")
    logger.info("1. Supabaseì—ì„œ ë‹¤ìŒ SQL ì‹¤í–‰:")
    logger.info("   SELECT update_news_ranking();")
    logger.info("2. ë­í‚¹ í™•ì¸:")
    logger.info("   SELECT * FROM v_latest_ranking;")
    logger.info("=" * 60)


# ================================================================
# ì‹¤í–‰
# ================================================================

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)
