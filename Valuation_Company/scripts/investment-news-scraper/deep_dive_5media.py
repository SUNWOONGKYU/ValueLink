#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
5ê°œ ì–¸ë¡ ì‚¬ ê¹Šì´ íƒìƒ‰ - ì•„ì¹´ì´ë¸Œ, ì¹´í…Œê³ ë¦¬, íƒœê·¸ë³„ ìˆ˜ì§‘
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time
from urllib.parse import urljoin

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# ëª» ì°¾ì€ 3ê°œ ê¸°ì—…
target_companies = ["ë¶€ìŠ¤í‹°ìŠ¤", "ì• í”Œì—ì´ì•„ì´", "ì†Œì…œë¦­ìŠ¤ì½”ë¦¬ì•„"]

# 5ê°œ ì–¸ë¡ ì‚¬ ì‹¬ì¸µ íƒìƒ‰ URL
MEDIA_SOURCES = {
    "WOWTALE": {
        "name": "WOWTALE",
        "number": 1,
        "urls": [
            "https://wowtale.net/category/investment/",
            "https://wowtale.net/2026/01/",
            "https://wowtale.net/2025/12/",
            "https://wowtale.net/2025/11/",
        ]
    },
    "ë²¤ì²˜ìŠ¤í€˜ì–´": {
        "name": "ë²¤ì²˜ìŠ¤í€˜ì–´",
        "number": 9,
        "urls": [
            "https://www.venturesquare.net/category/news/investment",
            "https://www.venturesquare.net/category/startup",
            "https://www.venturesquare.net/page/2/",
            "https://www.venturesquare.net/page/3/",
            "https://www.venturesquare.net/page/4/",
        ]
    },
    "ë”ë²¨": {
        "name": "ë”ë²¨",
        "number": 16,
        "urls": [
            "https://www.thebell.co.kr/free/content/NewsList.asp?svccode=00&trustkey=00",
            "https://www.thebell.co.kr/free/content/NewsList.asp?page=2",
            "https://www.thebell.co.kr/free/content/NewsList.asp?page=3",
        ]
    },
    "í”Œë˜í…€": {
        "name": "í”Œë˜í…€",
        "number": 10,
        "urls": [
            "https://platum.kr/archives/category/startup-story/investment",
            "https://platum.kr/archives/category/startup-story",
            "https://platum.kr/page/2/",
            "https://platum.kr/page/3/",
        ]
    },
    "ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´": {
        "name": "ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´",
        "number": 11,
        "urls": [
            "https://www.startuptoday.kr/news/articleList.html?sc_section_code=S1N1",
            "https://www.startuptoday.kr/news/articleList.html?sc_section_code=S1N2",
            "https://www.startuptoday.kr/news/articleList.html?page=2",
            "https://www.startuptoday.kr/news/articleList.html?page=3",
        ]
    }
}


def extract_articles_from_page(url, media_name):
    """í˜ì´ì§€ì—ì„œ ëª¨ë“  ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ"""

    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        articles = []

        # ëª¨ë“  ë§í¬ ì¶”ì¶œ
        links = soup.find_all('a', href=True)

        for link in links:
            href = link.get('href', '')
            text = link.get_text().strip()

            # ì ˆëŒ€ URL ë³€í™˜
            if href.startswith('/'):
                base_url = '/'.join(url.split('/')[:3])
                href = urljoin(base_url, href)

            # ê¸°ì‚¬ URL íŒ¨í„´ í™•ì¸
            is_article = False

            if media_name == "WOWTALE" and '/2025/' in href or '/2026/' in href:
                is_article = True
            elif media_name == "ë²¤ì²˜ìŠ¤í€˜ì–´" and 'venturesquare.net' in href and len(href.split('/')) > 3:
                is_article = True
            elif media_name == "ì•„ì›ƒìŠ¤íƒ ë”©" and 'outstanding.kr' in href and href.count('/') >= 3:
                is_article = True
            elif media_name == "í”Œë˜í…€" and 'platum.kr/archives/' in href:
                is_article = True
            elif media_name == "ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´" and 'articleView.html' in href:
                is_article = True

            if is_article and text:
                # 3ê°œ ê¸°ì—… ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨?
                for company in target_companies:
                    if company in text:
                        # íˆ¬ì í‚¤ì›Œë“œ í™•ì¸
                        investment_keywords = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ', 'M&A', 'ì¸ìˆ˜']
                        if any(kw in text for kw in investment_keywords):
                            articles.append({
                                'title': text,
                                'url': href,
                                'company': company
                            })
                            break

        return articles

    except Exception as e:
        print(f"    ì˜¤ë¥˜: {e}")
        return []


def main():
    print("=" * 80)
    print("5ê°œ ì–¸ë¡ ì‚¬ ì‹¬ì¸µ íƒìƒ‰ - ì•„ì¹´ì´ë¸Œ, ì¹´í…Œê³ ë¦¬, í˜ì´ì§€ë³„")
    print("=" * 80)
    print(f"\nğŸ¯ íƒ€ê²Ÿ: {', '.join(target_companies)}\n")

    total_found = 0
    total_duplicate = 0
    all_found_companies = set()

    for media_name, media_info in MEDIA_SOURCES.items():
        print(f"\n{'='*80}")
        print(f"ğŸ“° {media_name} íƒìƒ‰ ì¤‘...")
        print(f"{'='*80}")

        media_found = 0
        media_duplicate = 0

        for idx, url in enumerate(media_info['urls'], 1):
            print(f"\n[{idx}/{len(media_info['urls'])}] {url}")
            print(f"  ğŸ” í¬ë¡¤ë§ ì¤‘...", end=' ')

            articles = extract_articles_from_page(url, media_name)

            print(f"{len(articles)}ê°œ ê¸°ì‚¬ ë°œê²¬")

            for article in articles:
                print(f"  âœ… [{article['company']}] {article['title'][:50]}...")

                # DB ì €ì¥
                article_data = {
                    'site_number': media_info['number'],
                    'site_name': media_info['name'],
                    'site_url': "",
                    'article_title': article['title'],
                    'article_url': article['url'],
                    'published_date': datetime.now().strftime('%Y-%m-%d')
                }

                # ì¤‘ë³µ í™•ì¸
                existing = supabase.table("investment_news_articles")\
                    .select("id")\
                    .eq("article_url", article_data['article_url'])\
                    .execute()

                if not existing.data:
                    try:
                        supabase.table("investment_news_articles").insert(article_data).execute()
                        print(f"     ğŸ’¾ ì €ì¥ ì™„ë£Œ")
                        media_found += 1
                        total_found += 1
                        all_found_companies.add(article['company'])
                    except:
                        print(f"     âŒ DB ì˜¤ë¥˜")
                else:
                    print(f"     âš ï¸  ì¤‘ë³µ")
                    media_duplicate += 1
                    total_duplicate += 1

            time.sleep(1)

        print(f"\n{media_name} ê²°ê³¼:")
        print(f"  âœ… ìƒˆë¡œ ë°œê²¬: {media_found}ê°œ")
        print(f"  âš ï¸  ì¤‘ë³µ: {media_duplicate}ê°œ")

    print(f"\n{'='*80}")
    print("5ê°œ ì–¸ë¡ ì‚¬ ì‹¬ì¸µ íƒìƒ‰ ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"âœ… ì´ ìƒˆë¡œ ë°œê²¬: {total_found}ê°œ")
    print(f"âš ï¸  ì´ ì¤‘ë³µ: {total_duplicate}ê°œ")
    print(f"{'='*80}")

    if all_found_companies:
        print(f"\nğŸ‰ ë°œê²¬ëœ ê¸°ì—…:")
        for company in all_found_companies:
            print(f"  âœ… {company}")

    not_found = set(target_companies) - all_found_companies
    if not_found:
        print(f"\nâŒ ì—¬ì „íˆ ëª» ì°¾ì€ ê¸°ì—…:")
        for company in not_found:
            print(f"  - {company}")

    # ìµœì¢… í†µê³„
    count_result = supabase.table("investment_news_articles").select("id", count="exact").execute()
    print(f"\ninvestment_news_articles í…Œì´ë¸” ì´ ë ˆì½”ë“œ: {count_result.count}ê°œ")


if __name__ == '__main__':
    main()
