#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
5ê°œ ì–¸ë¡ ì‚¬ ì™„ì „ ìˆ˜ì§‘ (ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§)
- WOWTALE, ë²¤ì²˜ìŠ¤í€˜ì–´, ì•„ì›ƒìŠ¤íƒ ë”©, í”Œë˜í…€, ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´
"""

import os
import sys
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client, Client
import time
import re

# UTF-8 ì¶œë ¥ ì„¤ì •
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

load_dotenv()

supabase: Client = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")
)

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

INVESTMENT_KEYWORDS = ['íˆ¬ì', 'ìœ ì¹˜', 'í€ë”©', 'ì‹œë¦¬ì¦ˆ', 'Series', 'ë¼ìš´ë“œ', 'VC']

# 5ê°œ ì–¸ë¡ ì‚¬ ì„¤ì •
SITES = [
    {
        'number': 1,
        'name': 'WOWTALE',
        'url': 'https://wowtale.net',
        'main_url': 'https://wowtale.net',
        'url_pattern': '/2026/01/'
    },
    {
        'number': 9,
        'name': 'ë²¤ì²˜ìŠ¤í€˜ì–´',
        'url': 'https://www.venturesquare.net',
        'main_url': 'https://www.venturesquare.net',
        'url_pattern': 'venturesquare.net'
    },
    {
        'number': 13,
        'name': 'ì•„ì›ƒìŠ¤íƒ ë”©',
        'url': 'https://outstanding.kr',
        'main_url': 'https://outstanding.kr',
        'url_pattern': 'outstanding.kr'
    },
    {
        'number': 10,
        'name': 'í”Œë˜í…€',
        'url': 'https://platum.kr',
        'main_url': 'https://platum.kr',
        'url_pattern': 'platum.kr'
    },
    {
        'number': 11,
        'name': 'ìŠ¤íƒ€íŠ¸ì—…íˆ¬ë°ì´',
        'url': 'https://www.startuptoday.kr',
        'main_url': 'https://www.startuptoday.kr/news/articleList.html',
        'url_pattern': 'startuptoday.kr'
    }
]


def get_recent_urls(site):
    """ë©”ì¸ í˜ì´ì§€ì—ì„œ ìµœì‹  URL ìˆ˜ì§‘"""

    try:
        response = requests.get(site['main_url'], headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        links = soup.find_all('a', href=True)

        recent_urls = set()
        for link in links:
            href = link.get('href', '')

            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            if href.startswith('/'):
                href = site['url'] + href
            elif not href.startswith('http'):
                href = site['url'] + '/' + href

            # ì‚¬ì´íŠ¸ URL íŒ¨í„´ í™•ì¸
            if site['url_pattern'] in href:
                # WOWTALEì€ 2026ë…„ 1ì›”ë§Œ
                if site['name'] == 'WOWTALE':
                    if '/2026/01/' in href:
                        recent_urls.add(href)
                else:
                    recent_urls.add(href)

        return list(recent_urls)[:50]  # ìµœëŒ€ 50ê°œ

    except Exception as e:
        return []


def crawl_article(url, site):
    """ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§"""

    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')

        # ì œëª© ì¶”ì¶œ
        title_elem = soup.find('h1') or soup.find('title')
        title = title_elem.get_text(strip=True) if title_elem else ""

        # ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸
        if len(title) < 10:
            return None

        # ê³µì§€ì‚¬í•­ ì œì™¸
        if '[ê³µì§€]' in title or 'ê³µì§€' in title[:10]:
            return None

        # íˆ¬ì í‚¤ì›Œë“œ í™•ì¸
        if not any(kw in title for kw in INVESTMENT_KEYWORDS):
            return None

        # ë‚ ì§œ ì¶”ì¶œ
        date_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', url)
        if date_match:
            year, month, day = date_match.groups()
            published_date = f"{year}-{month}-{day}"
        else:
            published_date = datetime.now().strftime('%Y-%m-%d')

        return {
            'site_number': site['number'],
            'site_name': site['name'],
            'site_url': site['url'],
            'article_title': title,
            'article_url': url,
            'published_date': published_date
        }

    except Exception as e:
        return None


def scrape_site(site):
    """ì‚¬ì´íŠ¸ë³„ ìˆ˜ì§‘"""

    print(f"\n{'='*60}")
    print(f"ğŸ“° {site['name']} ìˆ˜ì§‘ ì¤‘...")
    print(f"{'='*60}")

    # URL ìˆ˜ì§‘
    urls = get_recent_urls(site)
    print(f"  â†’ {len(urls)}ê°œ URL ë°œê²¬")

    if not urls:
        print(f"  âŒ URL ìˆ˜ì§‘ ì‹¤íŒ¨")
        return 0, 0, 0

    saved = 0
    duplicate = 0
    skip = 0

    for idx, url in enumerate(urls, 1):
        article = crawl_article(url, site)

        if not article:
            skip += 1
            continue

        # ì¤‘ë³µ í™•ì¸
        existing = supabase.table("investment_news_articles")\
            .select("id")\
            .eq("article_url", article['article_url'])\
            .execute()

        if existing.data:
            duplicate += 1
        else:
            # DB ì €ì¥
            try:
                supabase.table("investment_news_articles").insert(article).execute()
                print(f"  [{idx}/{len(urls)}] âœ… {article['article_title'][:50]}...")
                saved += 1
            except:
                pass

        time.sleep(0.1)

    print(f"\n  ê²°ê³¼: âœ… {saved}ê°œ ì €ì¥, âš ï¸ {duplicate}ê°œ ì¤‘ë³µ, âŒ {skip}ê°œ ì œì™¸")

    return saved, duplicate, skip


def main():
    print("=" * 60)
    print("5ê°œ ì–¸ë¡ ì‚¬ ì™„ì „ ìˆ˜ì§‘")
    print("=" * 60)

    total_saved = 0
    total_duplicate = 0
    total_skip = 0

    for site in SITES:
        saved, duplicate, skip = scrape_site(site)
        total_saved += saved
        total_duplicate += duplicate
        total_skip += skip

    print(f"\n{'='*60}")
    print("ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ")
    print(f"{'='*60}")
    print(f"âœ… ì´ ì €ì¥: {total_saved}ê°œ")
    print(f"âš ï¸ ì´ ì¤‘ë³µ: {total_duplicate}ê°œ")
    print(f"âŒ ì´ ì œì™¸: {total_skip}ê°œ")
    print(f"{'='*60}")

    # ìµœì¢… ë ˆì½”ë“œ ìˆ˜ í™•ì¸
    result = supabase.table("investment_news_articles")\
        .select("id", count="exact")\
        .execute()

    print(f"\nğŸ“Š investment_news_articles í…Œì´ë¸”: {result.count}ê°œ ë ˆì½”ë“œ")


if __name__ == '__main__':
    main()
