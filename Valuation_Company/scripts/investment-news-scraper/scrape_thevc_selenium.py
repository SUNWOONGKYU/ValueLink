#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
THE VC íˆ¬ì ë°ì´í„° ìŠ¤í¬ë˜í•‘ (Selenium)
"""

import os
import time
import json
import logging
from datetime import datetime, date
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def setup_driver():
    """Chrome WebDriver ì„¤ì •"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver


def scrape_thevc():
    """THE VC íˆ¬ì ë°ì´í„° ìˆ˜ì§‘"""
    driver = setup_driver()
    articles = []

    try:
        url = "https://thevc.kr/browse/investments"
        logger.info(f"ğŸ“ ì ‘ì† ì¤‘: {url}")

        driver.get(url)
        time.sleep(5)  # JavaScript ë Œë”ë§ ëŒ€ê¸°

        # ìŠ¤í¬ë¡¤ ë‹¤ìš´ (lazy loading)
        for _ in range(3):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)

        logger.info(f"âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {driver.title}")

        # HTML ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')

        # í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥ (ë””ë²„ê¹…ìš©)
        with open('thevc_page_source.html', 'w', encoding='utf-8') as f:
            f.write(soup.prettify())
        logger.info("ğŸ“„ í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥: thevc_page_source.html")

        # íˆ¬ì ë°ì´í„° ì¶”ì¶œ ì‹œë„
        # THE VCëŠ” íˆ¬ì DBì´ë¯€ë¡œ í…Œì´ë¸” ë˜ëŠ” ì¹´ë“œ í˜•ì‹ìœ¼ë¡œ ë˜ì–´ ìˆì„ ê²ƒ

        # ë°©ë²• 1: í…Œì´ë¸” í–‰ ì°¾ê¸°
        rows = soup.find_all('tr')
        logger.info(f"ğŸ” í…Œì´ë¸” í–‰ ìˆ˜: {len(rows)}")

        # ë°©ë²• 2: íˆ¬ì ì¹´ë“œ ì°¾ê¸°
        cards = soup.find_all('div', class_=lambda x: x and ('card' in x.lower() or 'item' in x.lower()))
        logger.info(f"ğŸ” ì¹´ë“œ ìˆ˜: {len(cards)}")

        # ë°©ë²• 3: ë§í¬ì—ì„œ "íˆ¬ì" í‚¤ì›Œë“œ í¬í•¨ëœ ê²ƒ ì°¾ê¸°
        links = soup.find_all('a', href=True)
        investment_links = [link for link in links if 'íˆ¬ì' in link.get_text()]
        logger.info(f"ğŸ” 'íˆ¬ì' í¬í•¨ ë§í¬: {len(investment_links)}")

        # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
        for i, link in enumerate(investment_links[:5]):
            logger.info(f"  {i+1}. {link.get_text()[:50]} -> {link['href']}")

    except Exception as e:
        logger.error(f"âŒ ì—ëŸ¬: {e}", exc_info=True)

    finally:
        driver.quit()

    return articles


if __name__ == '__main__':
    logger.info("=" * 60)
    logger.info("ğŸ” THE VC í˜ì´ì§€ ë¶„ì„ ì‹œì‘")
    logger.info("=" * 60)

    articles = scrape_thevc()

    logger.info("=" * 60)
    logger.info(f"âœ… ì™„ë£Œ: {len(articles)}ê±´ ìˆ˜ì§‘")
    logger.info("=" * 60)
